\chapter{Implementation}
\label{cha:implementation}

\epigraph{Ideas are easy, Implementation is hard.}{Guy Kawasaki}

Most of the implementation details and general decisions made for developing a
real-world functional project, based on the design architecture presented in chapter
\ref{cha:architecture}, are discussed in this chapter. It is organized in such a
way that it starts with the foundation and then progresses to the autoscaling of
worker nodes before moving on to example deployments of various applications in the
cluster. \\ %
The technology and programming languages utilized in the implementation are heterogeneous.
Nevertheless, according to a shared set of APIs and pre-defined structures, they
interact as a distinct and homogenous entity that maintains the entire cluster
in an active and healthy state. \\ %
Furthermore, several of the components and/or technologies discussed in this chapter
are interchangeable with other solutions. The latter is highly valuable for
organizations since it provides more configuration freedom and final control
over the cluster while continuously reflecting its ultimate goal. \\ %
The main implementation of the cluster and its related applications currently
supports the \texttt{Linux} platform (see section
\ref{subsec:architecture_components_node}) together with the architectures \texttt{amd64}\footnote{\url{https://wikipedia.org/wiki/X86-64}}
and \texttt{arm64}\footnote{\url{https://wikipedia.org/wiki/AArch64}}.

\section{Distributions}
\label{sec:implementation_distributions}

% TODO Reference introduction dependencies

Almost all software relies on the necessity for a stable and consistent operating
system (OS). \\ %
The OS, as specified in section \ref{subsec:architecture_components_node}, is installed
on every node in the cluster and must be based on the \texttt{Linux Kernel}. The
latter is a critical requirement because most of the technologies employed in the
implementation rely on Linux primitives and core functionality to operate properly.
To enforce resource management for pods and containers, for example, all
Kubernetes distributions require the \texttt{cgroup v2}\footnote{\url{https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html}}
kernel module\footnote{\url{https://kubernetes.io/docs/concepts/architecture/cgroups}}.
Furthermore, the Linux Kernel is an Open Source project to which everyone can contribute
and get access, representing one fundamental objective of the entire project. \\ %
To represent the potential of the implementation architecture being compatible
with several Linux distributions, the section is titled Distributions rather than
Operating System. A Linux distribution, often known as a Linux distro, is an
operating system that is built from components developed by various open-source
projects and programmers. The Linux kernel (the operating system's core), the
GNU shell utilities (terminal interface and commands), the X server (for a
graphical desktop), the desktop environment, a package management system, an
installer, and other services are all included in each distribution. Many
components are created independently and released in source code format. Thousands
of software packages, utilities, and applications can be found in a single Linux
distribution. Linux distributions combine code from open-source projects into a
single operating system that can be installed and booted. Linux distributions are
available for a wide range of specialized use cases, including desktop computers,
servers without a graphical interface, supercomputers, mobile devices, and more\cite{linux_distro}.
\\ %
To be completely compatible with the implementation architecture, a Linux
distribution must meet three major requirements. They are described in the following
list:
\begin{enumerate}
  \item The Linux Kernel version must be compatible with the Kubernetes version.
    \newline
    Newer versions of K8s may require newer versions of the Linux Kernel to
    provide newer functions and/or improve overall security.

  \item All necessary packages, as explained and documented in the section
    \ref{subsec:implementation_distributions_packages}, must be installed and globally
    accessible in the system.
    \newline
    A check on all essential packages is performed before the execution of the
    installation program, as detailed in section \ref{sec:implementation_installer}.
    If even one package is missing from the system, the installation fails with an
    error message identifying the missing packages.

  \item An Init System that is supported and compatible, as defined in the
    section \ref{subsec:implementation_distributions_init_system}.
    \newline
    OpenRC and Systemd are supported by the implementation. They are currently
    the two most used init systems, and the majority of available distributions are
    based on one of them.
\end{enumerate}
Finally, two custom Linux distributions are already configured, built, and packaged
as ISO files, as shown in section
\ref{subsec:implementation_distributions_iso_image}. They are based on a simple command
line with no graphical interface and contain only what is required to start a node.
This is done to simplify the overall setup by eliminating unnecessary software
and technologies, addressing any incompatibilities, and increasing the cluster's
implementation and testing efficiency. Both are derived from pre-existing Linux distributions
that are known to be stable, easy to configure, and lightweight, requiring just
the bare minimum of resources to operate. Furthermore, they are built on
separate init systems, requiring the overall implementation to be compatible with
and tested with both.

\subsection{Packages}
\label{subsec:implementation_distributions_packages}

The packages used in the implementation that must be available in a Linux
distribution are specified in this section. A package is a collection of files that
have been bundled together and may be installed and uninstalled as a group\cite{package_manager}.
\\ %
All packages may be installed automatically using the package manager of the
respective Linux distribution. A package manager's job is to provide an interface
that assists the user in managing the collection of packages installed on the system.
A package manager maintains track of what software is installed on the system
and makes it simple to install new software, update software to newer versions,
or delete previously installed software. A package is usually a single piece of software.
However, it is typical for applications to be made up of numerous dependent packages.
For example, the \texttt{jq} package includes not just the \texttt{jq} command-line
JSON processor, but also the \texttt{oniguruma}\footnote{\url{https://github.com/kkos/oniguruma}}
package, a regular expressions library. Furthermore, various optional add-on
packages including extra data and documentation are available\cite{package_manager}.
\\ %
All packages are already included by default in the two custom Linux distributions.
As a result, there is no need for any changes or the installation of extra packages.
\\ %
Each package in table \ref{tbl:packages} contains a brief description extracted from
its official documentation, as well as some notes on why it was utilized. Furthermore,
the required column indicates with symbols if the package is required (\textcolor{bulmaGreen}{\faicon{check}}),
optional (\textcolor{bulmaBlue}{\faicon{question}}), or not required (\textcolor{bulmaRed}{\faicon{times}}).
It should be noted that the name of some packages may differ based on the
package manager employed by the Linux distribution.

\begin{xltabular}
  {\textwidth} { >{\ttfamily}l | c | X | X }

  \multicolumn{1}{ c |}{\large{\textbf{Name}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Required}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Description}}} &
  \multicolumn{1}{ c }{\large{\textbf{Notes}}} \\ \hhline{====}

  bc & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{bc}, also known as basic
  calculator, is a language that allows for arbitrary precision numbers with
  interactive statement execution\cite{bc}. & The Linux terminal does not
  support complicated math operations or number comparisons. As a result, the
  \texttt{bc} program is used throughout the installation procedure to guarantee
  that computed data, such as the node's mean power consumption and Simple
  Standard Deviation\footnote{\url{https://wikipedia.org/wiki/Standard_deviation}},
  is neither less than nor more than a threshold value. \\ \hline

  coreutils & \textcolor{bulmaGreen}{\faicon{check}} & The GNU Core Utilities
  are the GNU\footnote{\url{https://www.gnu.org}} operating system's essential
  file, shell, and text manipulation utilities. These are the core utilities that
  are required to be included in any operating system\cite{coreutils}. & By default,
  minimal Linux distributions only include a portion of the massive list of
  utilities provided in the standard \texttt{coreutils} package. As a result, several
  of the applications utilized in the implementation are missing. Therefore, the
  whole \texttt{coreutils} package is required. \\ \hline

  docker & \textcolor{bulmaBlue}{\faicon{question}} & Docker Engine is a container
  engine that allows creating, managing, and operating OCI\footnote{\url{https://opencontainers.org}}
  (Open Container Initiative) Containers on a Linux system\cite{docker}. & During
  the cluster initialization procedure, \texttt{docker} is widely utilized to tag\footnote{\url{https://docs.docker.com/engine/reference/commandline/tag}}
  and push\footnote{\url{https://docs.docker.com/engine/reference/commandline/push}}
  various container images corresponding to specific implementation dependencies
  (see section \ref{sec:implementation_dependencies}) to the private registry.
  It should be noted that \texttt{docker} is not the only tool for working with OCI
  containers, hence the optional symbol, but it is the most popular with well-known
  and simple CLI commands. As an example, \texttt{podman}\footnote{\url{https://podman.io}}
  is a Docker-compatible command line front end that can simply alias the Docker
  CLI (\texttt{alias docker=podman}), requiring no modifications to the
  implementation code\cite{podman}. \\ \hline

  ethtool & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{ethtool} is a standard
  Linux software for managing network drivers and hardware by modifying Network Interface
  Controller (NIC) settings\cite{ethtool}. & If the node's NIC interface supports
  Wake-on-Lan (WoL), \texttt{ethtool} is used to activate it.
  \newline
  It's worth noting that the NIC interface may reset to factory settings
  following a reboot. As a result, \texttt{ethtool} is always run before the NIC
  interface is activated. \\ \hline

  grep & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{grep} prints lines from
  input files or standard input that match one or more patterns\cite{grep}. & \texttt{grep}
  is an essential utility in all Linux systems. It is widely utilized in all scripts
  for matching predetermined patterns and executing arbitrary actions based on the
  latter's results. \\ \hline

  inotify-tools & \textcolor{bulmaGreen}{\faicon{check}} & Monitor one or more
  files or directories for a predefined set of events, such as open, close, move/rename,
  delete or create. Under the hood, to be more efficient, it makes use of Linux's
  \texttt{inotify}\footnote{\url{https://docs.kernel.org/filesystems/inotify.html}}
  interface\cite{inotify_tools}. & Obtaining access information to the
  underlying Kubernetes cluster is required during the cluster initialization
  phase. This information is only available in an automatically generated configuration
  file saved in a specific and well-known directory. As a result, before proceeding
  to the next phase, the installation script uses the \texttt{inotifywait} program
  to wait for the file to be created. \\ \hline

  iproute2 & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{iproute2} is a collection
  of utilities for managing TCP/IP\footnote{\url{https://wikipedia.org/wiki/Internet_protocol_suite}}
  networking and traffic control in Linux\cite{iproute2}.
  \newline
  The most essential tools are \texttt{ip} and \texttt{tc}. IPv4 and IPv6
  configuration is handled by \texttt{ip}, whereas \texttt{tc} stands for traffic
  control. & During the installation phase, the command \texttt{ip} is used to
  gather information about physical network interfaces that are not \texttt{loopback}\footnote{\url{https://wikipedia.org/wiki/Loopback}},
  such as name and MAC address. \\ \hline

  jq & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{jq} is a lightweight and
  flexible command-line JSON\footnote{\url{https://www.json.org}} processor\cite{jq}.
  & To manipulate JSON data, all scripts make extensive use of \texttt{jq}. \\ \hline

  ncurses & \textcolor{bulmaRed}{\faicon{times}} & The \texttt{ncurses} (new curses)
  library is a free software emulation of curses in System V\footnote{\url{https://wikipedia.org/wiki/UNIX_System_V}}.
  It supports pads, color, multiple highlights, forms characters, and function-key
  mapping, as well as all of the other SVr4-curses improvements over BSD curses\cite{ncurses}.
  & The \texttt{ncurses} package includes the \texttt{tput} software, which is a
  utility for retrieving terminal capabilities in shell scripts.
  \newline
  When the spinner process is spawned in a script, \texttt{tput} is used to
  update its symbols at regular intervals, replacing the old one with the new one
  and updating the cursor position. Because a spinner is only a decorative
  element, the package is not required and can be omitted. \\ \hline

  nodejs & \textcolor{bulmaBlue}{\faicon{question}} & Node.js is an open-source,
  cross-platform JavaScript\footnote{\url{https://developer.mozilla.org/docs/Web/JavaScript}}
  runtime environment\cite{nodejs}. & This package is optional since the Server component,
  see section \ref{subsec:architecture_components_server}, may be bundled as a single
  binary or in a container, removing the need for the \texttt{nodejs} package to
  be installed. Because the present implementation does not provide any of the
  aforementioned alternatives, \texttt{nodejs} is included in both of the two
  custom Linux distributions. \\ \hline

  npm & \textcolor{bulmaBlue}{\faicon{question}} & The package manager for the Node.js
  JavaScript platform. It installs modules so that Node.js can locate them and
  intelligently resolves dependency conflicts. It is highly adaptable to serve a
  wide range of use cases. It is often used to publish, find, install, and build
  node applications\cite{npm}. & This package is optional since the Server component
  may be bundled as a single binary or in a container, removing the need for the
  \texttt{npm} package to be installed. Because the present implementation does
  not provide any of the aforementioned alternatives, \texttt{npm} is included
  in both of the two custom Linux distributions. \\ \hline

  openssh & \textcolor{bulmaGreen}{\faicon{check}} & The OpenSSH software suite includes
  tools for managing secure tunneling capabilities, various authentication
  methods, and advanced configuration options.
  \newline
  OpenSSH is the most popular remote login tool for the SSH protocol\footnote{\url{https://wikipedia.org/wiki/Secure_Shell}}.
  It encrypts all traffic to prevent eavesdropping, connection hijacking, and other
  types of attacks\cite{openssh}. & OpenSSH is critical in the cluster for protecting
  remote operations, key management, and other tasks. All operations are
  inherently unsafe without its software and can pose serious security issues, especially
  if the cluster is accessible from external networks. \\ \hline

  postgresql & \textcolor{bulmaBlue}{\faicon{question}} & \texttt{PostgreSQL} is
  a powerful, open-source object-relational database system with a high
  reputation for reliability, feature robustness, and performance\cite{postgresql}.
  & \texttt{PostgreSQL} is the database implementation used for the Database component
  (see section \ref{subsec:architecture_components_database}).
  \newline
  This package is optional since the database may be implemented outside of the
  cluster using other solutions. It is up to the organization in charge of the cluster
  to decide whether or not the database package should be included in the Linux distribution.
  \\ \hline

  procps-ng & \textcolor{bulmaRed}{\faicon{times}} & \texttt{procps} is a collection
  of command-line and full-screen programs that offer information from the \texttt{/proc}\footnote{\url{https://docs.kernel.org/filesystems/proc.html}}
  pseudo-filesystem. \texttt{procps} programs often concentrate on the structures
  that describe the processes operating on the system\cite{procps_ng}. & The \texttt{procps-ng}
  package includes the \texttt{ps} software, which displays information about
  the system's active processes.
  \newline
  When the spinner process is spawned in a script, \texttt{ps} is used to obtain
  the PID\footnote{\url{https://wikipedia.org/wiki/Process_identifier}} of the parent
  process. Because a spinner is only a decorative element, the package is not required
  and can be omitted. \\ \hline

  sed & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{sed} is a stream editor.
  A stream editor is used on an input stream (file or standard input) to perform
  simple filtering and text transformations. While comparable to an editor that allows
  scripted edits, \texttt{sed} operates by making only one pass through the input(s)
  and is thus more efficient\cite{sed}. & \texttt{sed} is an essential utility in
  all Linux systems. It is widely utilized in all scripts for extracting well-defined
  portions and/or filtering data from its input(s). \\ \hline

  sudo & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{sudo} (su\footnote{\url{https://wikipedia.org/wiki/Su_(Unix)}}
  "do") enables a system administrator to delegate authority to allow specific users
  (or groups of users) to run certain (or all) commands as root or another user while
  providing an audit record of the commands and their arguments. \texttt{sudo} works
  on a per-command basis; it is not a shell replacement\cite{sudo}. & The
  installation phase involves actions that need administrative authorization. Before
  performing any processing, the script checks to see whether the current user
  already has root\footnote{\url{https://wikipedia.org/wiki/Superuser}}
  privileges; if not, it requests them with \texttt{sudo}. \\ \hline

  sysbench & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{sysbench} is a multithreaded
  scriptable benchmark tool for measuring CPU, memory, file I/O, mutex
  performance, and database performance\cite{sysbench}. & During the installation
  phase, \texttt{sysbench} is extensively used to analyze a cluster node's performance
  and, in conjunction with other tools, its power consumption. \\ \hline

  tar & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{tar} is an archiving application
  that can store and manipulate numerous files in a single file (an archive).
  The archive can be either a standard file or a device, which can be placed on a
  local or remote system\cite{tar}. & \texttt{tar} is primarily used in
  production during the installation process to extract and manipulate various
  archive files.
  \newline
  \texttt{tar} is used in development to create the bundle archive for
  distribution, which contains all files and applications. \\ \hline

  tzdata & \textcolor{bulmaGreen}{\faicon{check}} & The Time Zone Database is a
  collection of code and data that represents the history of local time for
  several representative locations all over the world. It is updated regularly to
  reflect changes in time zone boundaries, UTC offsets, and daylight-saving laws
  enacted by legislative entities\cite{tzdata}. & To avoid date and time inconsistencies,
  all nodes in the cluster must be configured to the same timezone. By default,
  all nodes are configured to the Etc/UTC timezone. \\ \hline

  util-linux & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{util-linux} is a
  random collection of Linux utilities\cite{util_linux}. & The installation
  script makes use of two \texttt{util-linux} utility programs: \texttt{lscpu} to
  display information about the CPU architecture and \texttt{lsblk} to list
  block devices. \\ \hline

  yq & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{yq} is a portable and
  lightweight command-line YAML\footnote{\url{https://yaml.org}}, JSON, and XML\footnote{\url{https://www.w3.org/XML}}
  processor.
  \newline
  \texttt{yq} has a syntax similar to \texttt{jq}. It does not cover all that
  \texttt{jq} supports, but it does support the majority of the most common
  operations and functions\cite{yq}. & To manipulate YAML data, all scripts make
  extensive use of \texttt{yq}.
  \newline
  \texttt{yq} is primarily used to read and save YAML files. However, because of
  its more advanced functions and solutions, \texttt{jq} is used for the
  majority of complex data processing, at the expense of \texttt{yq}. As a result,
  YAML structures are first transformed to/from JSON, then processed with \texttt{jq},
  and then converted back to/from YAML. The double conversion should be
  eliminated and/or simplified in future implementations. \\ \hline

  \caption{Packages list}
  \label{tbl:packages}
\end{xltabular}

\subsection{Init System}
\label{subsec:implementation_distributions_init_system}

The Init System, short for Initialization System, is the first process that runs
when a Unix-based computer operating system is bootstrapped. Init is a daemon process
that is normally given Process IDentifier 1 (PID 1) and runs until the system is
shut down. It is the direct or indirect ancestor of all other processes and
adopts all orphaned processes automatically. The kernel starts Init during the boot
process; if the kernel is unable to start it, a kernel panic occurs\cite{init_system}.
\\ %
The Init System is logically positioned above the Kernel and is an essential
component of every modern system. It frequently integrates sophisticated features
over simple ones and is very customizable. Furthermore, it offers active control
and monitoring over the running processes, keeping the entire system healthy and
active. The latter is critical because it enables automated and continuous monitoring
of cluster implementation architecture-related processes operating in the node without
the need for a custom solution. For example, if a K8s instance executing in a worker
node crashes (i.e. exits with a code different than 0), the Init System detects it
and restarts the process automatically, making the node active and able to
receive workload again. \\ %
When the node is bootstrapped, the Init System is set to start all cluster-related
processes (i.e., K8s instance and Metric Server) and run a script that sends a message
to the server component notifying it that the node is active and ready. Before
the node is shut down, it automatically terminates all cluster-related processes
and sends a message to the server component informing that the node is shutting down
and will be inactive. As previously stated, the cluster performs these two operations
automatically: bootstrap for upscaling and shutdown for downscaling. However,
there are scenarios in which a node is manually bootstrapped and/or shut down by
human action without involving the server component directly. Despite the possibility
of the latter, the overall cluster state remains consistent, and all node statuses
are correctly updated, thanks to the Init System and its monitoring and
configuration capabilities. \\ %
The implementation currently supports two init systems: OpenRC (described in
section \ref{subsec:implementation_distributions_init_system_openrc}) and Systemd
(described in section
\ref{subsec:implementation_distributions_init_system_systemd}). They are the two
most popular and widely used Init Systems, and the vast majority of Linux distributions
support one or both of them. Their capabilities are nearly identical. However,
configuration files and available application(s) that interact with the bare-metal
Init System API, are different, necessitating double effort to achieve the same objective.
\\ %
It should be noted that if the Init System daemon process crashes due to an error,
the entire system crashes and becomes inactive, necessitating a manual restart.
Even though the likelihood of the latter is extremely low, it is never zero. As
a result, as stated in section \ref{subsec:architecture_components_server}, a
heartbeat mechanism has been established in the cluster to constantly monitor
and identify the status and availability of all active nodes. If there are no configuration
or networking issues and a node's heartbeat is not received within the predefined
interval, it indicates that the heartbeat daemon executing in the node has
crashed and has not yet been restarted, or that the entire node has crashed and is
not automatically recoverable. \\ %
In each of the following sections, where an Init System distribution is
described, an example of the related service configuration file is presented to illustrate
its differences. A service configuration file contains information about a
process that the Init System controls and monitors. The application used as a reference
in this example is K3s, a lightweight Kubernetes distribution explained in
further detail in section \ref{subsec:implementation_dependencies_k3s}. Before starting,
it must clear any temporary files in the \texttt{tmp} directory and load environment
variables from two distinct directories. Furthermore, anytime the application process
crashes, the Init System must restart it within 5 seconds after detection. Even though
each Init System has a unique service configuration file, the result is nearly
identical. The list below highlights some of the most significant configuration requirements.
To aid comprehension, the identification number for each requirement is also provided
in the Init System's service configuration file.
\begin{enumerate}[label=\protect\circled{\arabic{*}}]
  \item General information about the service.
    \newline
    Useful for users and/or administrators.

  \item Services on which the service is dependent.
    \newline
    The listed services must be activated before the current service may start.

  \item Remove any temporary files that begin with the name \texttt{k3s} and are
    placed in the temporary (\texttt{tmp}) directory before launching the application.

  \item Type of service. Different types correspond to various service behaviors
    as well as how the application is monitored and regulated.
    \newline
    In this case, monitor the application process and restart it if there is a
    crash or an anomaly.

  \item Absolute path to the application's binary file.

  \item Application arguments.

  \item Waiting time in seconds before restarting the crashed application.

  \item The maximum number of restarts permitted. When the threshold is met, the
    program is no longer restarted.
    \newline
    A value of 0 indicates that no threshold exists and that the application has
    no restart limit.

  \item Load \texttt{/etc/environment} and \texttt{/etc/rancher/k3s/k3s.env} environment
    variables files before starting the application.
\end{enumerate}

\subsubsection{OpenRC}
\label{subsec:implementation_distributions_init_system_openrc}

\begin{wrapfigure}
  {r}{.25\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/logos/gentoo.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://www.gentoo.org/inside-gentoo/artwork/gentoo-logo.html}} }
  \caption{Gentoo Linux logo}
\end{wrapfigure}

OpenRC is a dependency-based init system for Unix-like systems that maintain compatibility
with the system's init system, which is generally found in \texttt{/sbin/init}. \\ %
At boot, OpenRC starts the appropriate system services in the right sequence,
manages them while the system is running, and stops them at shutdown. It can
manage installed daemons, optionally monitor the processes it launches, and
start processes in parallel when possible to reduce boot time. \\ %
OpenRC was developed for Gentoo\footnote{\url{https://www.gentoo.org}}, but it may
be used in other Linux distributions and BSD\footnote{\url{https://www.bsd.org}}
systems as well\cite{openrc}. \\ %
Listing \ref{lst:openrc} illustrates the example of the OpenRC service
configuration file for K3s saved in \texttt{/etc/init.d/k3s}. Manual control of the
service is possible using the \texttt{rc-service} command, which locates and runs
the OpenRC service with the specified arguments: \texttt{rc-service k3s <CMD> [...]}.
\texttt{<CMD>} can be substituted by \texttt{start} to start the service, \texttt{stop}
to stop the service, \texttt{restart} to restart the service, and \texttt{status}
to acquire the service status. Furthermore, with the command \texttt{rc-update
add k3s default}, the service may be set to start automatically when the system
boots up. The latter examples are merely a very small subset of all the available
commands/capabilities that OpenRC is capable of.

\begin{lstlisting}[language=shell, escapeinside={(*@}{@*)}, morekeywords={[2]{depend, start_pre, want, after, sourcex}}, morekeywords={[4]{name, description, supervisor, command, command_args, output_log, error_log, pidfile, respawn_delay, respawn_max}}, xleftmargin=\parindent, label={lst:openrc}, caption=\texttt{OpenRC} service configuration file for K3s]
  #!/sbin/openrc-run

  name="k3s"                                                                                      (*@ \circled{\texttt{\tiny{1}}} @*)
  description="Lightweight Kubernetes"                                                            (*@ \circled{\texttt{\tiny{1}}} @*)

  depend() {
    want cgroups                                                                                  (*@ \circled{\texttt{\tiny{2}}} @*)
    after network-online                                                                          (*@ \circled{\texttt{\tiny{2}}} @*)
  }

  start_pre() {
    rm -f "/tmp/k3s.*"                                                                            (*@ \circled{\texttt{\tiny{3}}} @*)
  }

  supervisor="supervise-daemon"                                                                   (*@ \circled{\texttt{\tiny{4}}} @*)
  command="/usr/local/bin/k3s"                                                                    (*@ \circled{\texttt{\tiny{5}}} @*)
  command_args="server"                                                                           (*@ \circled{\texttt{\tiny{6}}} @*)
  output_log="/var/log/k3s.log"
  error_log="/var/log/k3s.log"
  pidfile="/var/run/k3s.pid"
  respawn_delay=5                                                                                 (*@ \circled{\texttt{\tiny{7}}} @*)
  respawn_max=0                                                                                   (*@ \circled{\texttt{\tiny{8}}} @*)

  set -o allexport
  if [ -f "/etc/environment" ]; then sourcex "/etc/environment"; fi                               (*@ \circled{\texttt{\tiny{9}}} @*)
  if [ -f "/etc/rancher/k3s/k3s.env" ]; then sourcex "/etc/rancher/k3s/k3s.env"; fi               (*@ \circled{\texttt{\tiny{9}}} @*)
  set +o allexport
\end{lstlisting}

\subsubsection{systemd}
\label{subsec:implementation_distributions_init_system_systemd}

\begin{wrapfigure}
  {r}{.25\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/logos/systemd.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://brand.systemd.io}} }
  \caption{systemd logo}
\end{wrapfigure}

systemd is a collection of building blocks for a Linux system. It provides a system
and service manager that runs as PID 1 and starts the rest of the system. \\ %
systemd supports aggressive parallelization, on-demand daemon startup, process tracking
through Linux control groups, mount and automount point management, and a complex
transactional dependency-based service control logic. \\ %
Other components include a logging daemon, tools to handle basic system
configuration such as the hostname, date, and locale, a list of logged-in users and
running containers and virtual machines, system accounts, runtime directories and
settings, and daemons to manage simple network configuration, network time synchronization,
log forwarding, and name resolution\cite{systemd}. \\ %
Listing \ref{lst:systemd} illustrates the example of the systemd service
configuration file for K3s saved in \texttt{/etc/systemd/system/k3s.service}. The
\texttt{systemctl} command, which manages the systemd system and the service manager,
may be used to manually control the service: \texttt{systemd <CMD> k3s [...]}. \texttt{<CMD>}
can be replaced with \texttt{start} to start the service, \texttt{stop} to stop it,
\texttt{restart} to restart it, and \texttt{status} to obtain the service status.
Furthermore, the service may be enabled to start automatically when the system boots
up by using the commands \texttt{systemctl enable k3s} followed by \texttt{systemctl
daemon-reload} to reload the systemd management configuration. The latter
examples are only a small portion of all the commands and capabilities provided
by systemd.

\begin{lstlisting}[language=ini, escapeinside={(*@}{@*)}, morekeywords={Description, Documentation, Wants, After, StartLimitBurst, WantedBy, Type, ExecStartPre, ExecStart, EnvironmentFile, KillMode, Delegate, TimeoutStartSec, Restart, RestartSec}, xleftmargin=\parindent, label={lst:systemd}, caption=\texttt{systemd} service configuration file for K3s]
  [Unit]
  Description=Lightweight Kubernetes                                                              (*@ \circled{\texttt{\tiny{1}}} @*)
  Documentation=https://k3s.io                                                                    (*@ \circled{\texttt{\tiny{1}}} @*)
  Wants=network-online.target                                                                     (*@ \circled{\texttt{\tiny{2}}} @*)
  After=network-online.target                                                                     (*@ \circled{\texttt{\tiny{2}}} @*)
  StartLimitBurst=0                                                                               (*@ \circled{\texttt{\tiny{8}}} @*)

  [Install]
  WantedBy=multi-user.target

  [Service]
  Type=notify                                                                                     (*@ \circled{\texttt{\tiny{4}}} @*)
  ExecStartPre=/usr/bin/env sh -c 'rm -f "/tmp/k3s.*"'                                            (*@ \circled{\texttt{\tiny{3}}} @*)
  ExecStart=/usr/local/bin/k3s server                                                         (*@ \hspace*{1.375pt} \circled{\texttt{\tiny{5}}} \circled{\texttt{\tiny{6}}} @*)
  EnvironmentFile=-/etc/environment                                                               (*@ \circled{\texttt{\tiny{9}}} @*)
  EnvironmentFile=-/etc/rancher/k3s/k3s.env                                                       (*@ \circled{\texttt{\tiny{9}}} @*)
  KillMode=process
  Delegate=yes
  TimeoutStartSec=0
  Restart=always                                                                                  (*@ \circled{\texttt{\tiny{4}}} @*)
  RestartSec=5s                                                                                   (*@ \circled{\texttt{\tiny{7}}} @*)
\end{lstlisting}

\subsection{ISO Image}
\label{subsec:implementation_distributions_iso_image}

This section briefly describes the two original Linux distributions, Alpine Linux
and Arch Linux, which served as the foundation for the two custom Linux
distributions. They were both utilized and tested together in the implementation
cluster. Furthermore, it is shown how the two custom ISO image files are customized
and generated by the use of a simple script. \\ %
Each customized Linux distribution comes in the form of a single ISO image file.
A disk image that contains everything that would be written on an optical disc, including
the optical disc file system, is known as an optical disc image (or ISO image). ISO
images are intended to include the binary image of an optical media file system,
as well as the data in its binary files, replicated exactly as they were saved on
the disc. The data included within the ISO image will be structured in
accordance with the file system used on the optical disc from which it was
produced\cite{iso}. \\ %
The ISO image can be burned into physical media such as a CD, or a USB flash
drive or can be mounted as an ISO file. The latter shows the high level of flexibility
and overall compatibility that an ISO file has. Moreover, it is extremely portable
since it is intrinsically only a single file making it the perfect format for a release
bundle and very useful in an Air-Gap environment. \\ %
After an ISO image of a Linux distribution has been burned to media, it can be
easily installed on multiple systems by attaching the media to the system and
selecting it as the default boot. It starts a live (volatile) OS that is not currently
installed on the primary disk. The live Linux distribution may then be installed
directly into the primary disk, making it permanently available on the system without
the need for removable media. The latter step may be performed manually, but it is
more complex than it appears since it necessitates an understanding of disk formatting,
partitioning, as well as other topics. Fortunately, most Linux distributions,
including the two custom ones, have simple tools that automate the installation procedure
by asking only a few simple questions to the user/administrator.

\subsubsection{Alpine Linux}
\label{subsubsec:implementation_distributions_iso_alpine_linux}

\begin{wrapfigure}
  {r}{.25\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/logos/alpine.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://alpinelinux.org}} }
  \caption{Alpine Linux logo}
\end{wrapfigure}

Alpine Linux\footnote{\url{https://www.alpinelinux.org}} is a lightweight,
security-focused, and resource-efficient distribution. In recent years, has grown
in popularity as the foundation for the majority of Docker images. It is based
on musl\footnote{\url{https://musl.libc.org}} (an implementation of the C standard
library, an alternative to glibc\footnote{\url{https://www.gnu.org/software/libc}}),
BusyBox\footnote{\url{https://busybox.net}} (a single, small executable that
combines tiny versions of many common UNIX utilities), a custom package manager
called APK\footnote{\url{https://wiki.alpinelinux.org/wiki/Alpine_Package_Keeper}}
and the OpenRC Init System (see section \ref{subsec:implementation_distributions_init_system_openrc})\cite{alpine_linux}.
\\ %
Alpine Linux is the preferred reCluster distribution since it has been widely used
and tested throughout the development process.

The official documentation\footnote{\url{https://wiki.alpinelinux.org/wiki/How_to_make_a_custom_ISO_image_with_mkimage}}
provides a brief guideline on how to build a custom Alpine Linux ISO image.
Unfortunately, it is not exhaustive, and it assumes a lot of information and
configurations. A simpler, easy-to-use, and portable solution is required for cluster
implementation. This resulted in the creation of a POSIX script that handles all
of the burdens, as explained below. \\ %
Building a custom Alpine Linux ISO image involves three prerequisites: an Alpine
Linux system, specific tools/packages to configure and build the image, and a
general configuration with a special administrator user and the generation of
signing keys. These requirements remain constant over time and can be regarded as
essentially static. Furthermore, having a physical Alpine Linux system dedicated
just to generating custom images is a waste of resources. As a result, a custom Dockerfile
based on Alpine Linux has been created to allow building the image in different environments
without the need for a physical machine and with all of the previous requirements
already fulfilled\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/distributions/alpine/Dockerfile}}.
A Dockerfile is a text document that contains all of the commands needed to
compile a container image\cite{dockerfile}. \\ %
The ISO image is customized using the \texttt{mkimg.recluster.sh}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/distributions/alpine/mkimg.recluster.sh}}
file, which defines and integrates a custom recluster profile, based on the
basic Alpine Linux profile. This file, whose content is shown in listing
\ref{lst:alpine}, is not included in the Dockerfile because it is considered dynamic,
in the sense that it could be updated and customized over time to meet potential
new cluster specifications. \\ %
Finally, to create the ISO image, the container image must be launched in
detached mode, a local volume mounted to the container, the \texttt{mkimg.recluster.sh}
file loaded, and the official \texttt{mkimage.sh} script executed with extra configuration
parameters. When performed manually, the latter tasks are extremely difficult and
prone to human error. As a result, all operations have been included in a POSIX script,
simply named \texttt{build.sh}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/distributions/alpine/build.sh}},
that does everything automatically, including cleanup procedures when the image(s)
has been generated or an error occurs. Moreover, all ISO images created are
saved in the current working directory under the directory \texttt{iso}. As an
example, a custom image based on Alpine Linux version 3.17 and generated with the
recluster profile for architecture \texttt{x86\_64} is saved as \texttt{alpine-recluster-v3.17-x86\_64.iso}
and is just \texttt{190MiB} in size. \\ %
To configure and install Alpine Linux permanently on the primary disk, simply run
the \texttt{setup-alpine}\footnote{\url{https://docs.alpinelinux.org/user-handbook/0.1a/Installing/setup_alpine.html}}
program and answer a few easy questions.

\begin{lstlisting}[language=shell, morekeywords={[2]{profile_recluster, profile_base}}, morekeywords={[4]{title, desc, profile_abbrev, image_ext, arch, output_format, apks, apkovl}}, xleftmargin=\parindent, label={lst:alpine}, caption=Contents of \texttt{mkimg.recluster.sh} file which shows the reCluster profile definition]
  profile_recluster() {
    profile_base
    title="reCluster"
    desc="reCluster Alpine Linux"
    profile_abbrev="recluster"
    image_ext="iso"
    arch="x86_64 aarch64"
    output_format="iso"
    apks="$apks coreutils docker ethtool inotify-tools iproute2 jq ncurses nodejs npm postgresql procps sudo sysbench tzdata util-linux yq"
    apkovl="genapkovl-recluster.sh"
  }
\end{lstlisting}

\subsubsection{Arch Linux}
\label{subsubsec:implementation_distributions_iso_arch_linux}

\begin{wrapfigure}
  {r}{.25\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/logos/arch.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://archlinux.org/art}} }
  \caption{Arch Linux logo}
\end{wrapfigure}

Arch Linux\footnote{\url{https://archlinux.org}} is a general-purpose distribution
that focuses on simplicity, minimalism, and code elegance. It is based on
Systemd Init System (see section \ref{subsec:implementation_distributions_init_system_systemd})
and strives to stay bleeding edge, offering the latest stable versions of most software
through Pacman\footnote{\url{https://wiki.archlinux.org/title/Pacman}} package manager.
Uses a rolling release system that allows one-time installation and perpetual
software upgrades, allowing to not reinstall or upgrade the system from one version
to the next. Notably, Arch Linux is the foundation for several many popular enterprise-grade
distributions, such as Manjaro\footnote{\url{https://manjaro.org}} and SteamOS\footnote{\url{https://store.steampowered.com/steamos}},
demonstrating its customizability, power, and stability\cite{arch_linux}.

The official documentation\footnote{\url{https://wiki.archlinux.org/title/archiso}}
includes precise guidelines for building a custom Arch Linux ISO image. For cluster
implementation, there is a need for only a basic/minimal image with a simple and
easy-to-use procedure. As a result, a POSIX script has been written to handle
all of the difficulties and configurations (described below). Some techniques are
quite similar to those outlined in Alpine Linux. However, the files and overall configurations
differ significantly. \\ %
Building a custom Arch Linux ISO image requires three prerequisites: an Arch
Linux system, specific tools/packages to configure and build the image, and a
general configuration with a baseline profile. As with Alpine Linux, these
requirements remain constant over time, resulting in the same possible problems.
As a result, a custom Dockerfile based on Arch Linux has been developed\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/distributions/arch/Dockerfile}}.
\\ %
The ISO image is customized with the \texttt{profiledef.sh}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/distributions/arch/profiledef.sh}}
file, the content of which is displayed in listing \ref{lst:arch}, that defines
and integrates a custom profile based on the baseline Arch Linux profile already
included in the container image. For the same reasons as in Alpine Linux, this
file is not available in the Dockerfile container image. \\ %
Finally, similar to the Alpine Linux approach, the container image must be
launched, a local volume mounted, the \texttt{profiledef.sh} file loaded, and
the official \texttt{mkarchiso} program executed with extra configuration parameters
to generate the ISO image. Because of its inherent difficulties, the latter exposes
the same issues as in Alpine Linux. Therefore, a similar POSIX script, also
named \texttt{build.sh}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/distributions/arch/build.sh}},
has been created with similar features but adapted to Arch Linux. As an example,
a custom Arch Linux image based on the reCluster profile and built on March 23rd
2023 for architecture \texttt{x86\_64} is saved as \texttt{reCluster-2023.03.23-x86\_64.iso}
and is \texttt{731MiB} in size. \\ %
To configure and install Arch Linux permanently on the primary disk, simply run
the \texttt{archinstall}\footnote{\url{https://wiki.archlinux.org/title/archinstall}}
program and answer a few easy questions.

\begin{lstlisting}[language=shell, morekeywords={[4]{iso_name, iso_label, iso_publisher, iso_application, iso_version, install_dir, buildmodes, bootmodes, arch, pacman_conf, airootfs_image_type, airootfs_image_tool_options, file_permissions}}, xleftmargin=\parindent, label={lst:arch}, caption=Contents of \texttt{profiledef.sh} file which shows the reCluster profile definition]
  iso_name="reCluster"
  iso_label="ARCH_$(date +%Y%m)"
  iso_publisher="reCluster"
  iso_application="reCluster Arch Linux"
  iso_version="$(date +%Y.%m.%d)"
  install_dir="arch"
  buildmodes=("iso")
  bootmodes=("bios.syslinux.mbr" "bios.syslinux.eltorito"
             "uefi-ia32.grub.esp" "uefi-x64.grub.esp"
             "uefi-ia32.grub.eltorito" "uefi-x64.grub.eltorito")
  arch="x86_64"
  pacman_conf="pacman.conf"
  airootfs_image_type="erofs"
  airootfs_image_tool_options=("-zlz4hc,12 -E ztailpacking")
  file_permissions=(
    ["/etc/shadow"]="0:0:400"
  )
\end{lstlisting}

\section{Dependencies}
\label{sec:implementation_dependencies}

This section is split into three segments, each of which is dependent on the
preceding one's knowledge. \\ %
The first segment defines an Air-Gap environment and why it is critical for
overall implementation, and more broadly for specific cluster use cases, to
allow scenarios where internet access is highly limited or even non-existent. \\ %
The second segment illustrates and explains the most important third-party
applications on which the overall architectural implementation is based. Each one
is tailored to a distinct function in the cluster and has a direct logical mapping
to a component in the architecture depicted in chapter \ref{cha:architecture}.
The majority, but not all, of the specified applications are interchangeable
with other implementations/distributions. As a result, any application-specific
configuration and/or capability must be adjusted and/or reconfigured to be
compatible with the new one. \\ %
The Server and Autoscaler components are missing because, due to their importance
and implementation, they have their dedicated sections, section \ref{sec:implementation_server}
and section \ref{sec:implementation_autoscaling}, where they are explained in
greater detail. Nevertheless, the name of the related component's implementation
appears in various portions of this section. It should be noted that the Server implementation
is cloud/cluster provider-specific, and therefore it is completely built from
scratch, whereas Autoscaler has an official Kubernetes implementation that is only
compatible with a limited number of particular and large cloud providers. As a
result, a custom solution has been created that is derived from the original and
has been appropriately changed to support the cluster implementation, Server API,
and overall use case. \\ %
Finally, the last segment discusses how to manage all dependencies and how they
are downloaded for a cluster release and to be functional in an Air-Gap scenario,
among other things. The latter relies on an intuitive configuration file and a
user-friendly POSIX script.

\subsection{Air-Gap Environment}
\label{subsec:implementation_dependencies_air_gap_environment}

All devices in an Air-Gap environment have no physical connectivity to the public
internet or any other Local Area Network (LAN) that is not itself in an Air-Gap environment.
Email clients, browsers, SSH, and other communication applications are all physically
and logically isolated from the outside world. \\ %
Systems in an Air-Gap environment are permanently separated from the outside world
by default. However, if possible, the network can communicate with other
physically isolated devices. Any data transfer outside of the network must take place
via external hardware that is temporarily connected to the network. USB flash
drives, Hotspot devices and other removable media are examples of such hardware.
Importantly, these external devices must be physically connected to and disconnected
from the Air-Gap environment network by human intervention. \\ %
Figure \ref{fig:airgap} depicts an example overview of an Air-Gap environment consisting
of two distinct Air-Gap networks that can connect with one another. Because they
are on separate networks, each network resembles a hypothetical cluster implementation
where communication between the two does not directly involve cluster operations
and administration (see section
\ref{subsec:architecture_network_internal_network}). Every machine in the
environment is unable to exchange data, connect to any external network, or
connect to the internet in general. The only mechanism to move data between the Air-Gap
environment and the outside world is through the use of a USB flash drive, and
the only way to connect the Air-Gap networks to an external network and/or the
internet is through the use of a personal hotspot. Because no automated mechanisms
are involved, both of the latter actions need human interaction.

\begin{wrapfigure}
  {r}{.5\textwidth}
  \centering
  \includegraphics[width=.5\textwidth]{images/implementation/airgap.pdf}
  \caption{An
  Air-Gap
  Environment
  composed
  of
  two
  Air-Gap
  Networks
  that
  are
  physically
  and
  logically
  isolated
  by
  any
  external
  data
  exchange
  and
  network.}
  \label{fig:airgap}
\end{wrapfigure}

Air-Gap environments are frequently employed in Network Security scenarios where
security is a top priority. In an Air-Gap scenario, a correctly designed network
mandates that devices within the network be invisible to and successfully isolated
from remote threat actors, who often scan the public internet for vulnerable
devices. Similarly, an attacker outside of the Air-Gap environment network
cannot directly execute Remote Code Execution\footnote{\url{https://tuxcare.com/remote-code-execution-attack-what-it-is-how-to-protect-your-systems}}
(RCE) attacks on potential software vulnerabilities within the environment\cite{airgap}.
\\ %
Although Air-Gap environments are extensively employed in the security branch,
high-security scenarios are not the only ones for which the cluster
implementation is designed. In reality, there are situations in which the
internet connection, and thus the connectivity to the external global network is
either too expensive to maintain or not available at all in the location where the
cluster is deployed. \\ %
To be fully compatible with an Air-Gap environment in various possible scenarios,
the cluster implementation is designed to have all external dependencies, including
custom Linux Distributions, pre-packaged with each cluster release and without
the need, by default, for possible external requests during its utilization. As a
result, all that is required is the unique archive file, which is automatically created
with each new release and contains all of the files required to bootstrap a functioning
cluster instance. Furthermore, even if not in an Air-Gap environment, during the
installation procedure, the organization in charge of the cluster's administration
can choose to either download all external dependencies from the internet (every
time for every node) or to directly use the already available pre-packaged dependencies,
reducing network usage as well as overall installation time.

\subsection{PostgreSQL}
\label{subsec:implementation_dependencies_postgresql}

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/postgresql.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://wiki.postgresql.org/wiki/Logo}} }
  \caption{PostgreSQL logo}
\end{wrapfigure}

Premise: This section varies from the other following sections in that it does not
intend to go into detail about the PostgreSQL dependency and what and how its core
features have been employed in the cluster implementation. PostgreSQL has only
been used in the most basic ways for storing the server's data in a persistent
and reliable system. Furthermore, there are numerous database systems to choose
from, both free and paid, each with unique characteristics that might meet the
different requirements of the organization managing the cluster. \\ %

PostgreSQL\footnote{\url{https://www.postgresql.org}} is a powerful, open-source
object-relational database system that uses and extends the SQL\footnote{\url{https://wikipedia.org/wiki/SQL}}
language with several capabilities for properly storing and scaling the most
complex data workloads. It is designed to assist developers in building applications,
and administrators in preserving data integrity and building fault-tolerant
systems\cite{postgresql}. \\ %
PostgreSQL has a direct mapping with the cluster architecture's Database component.
\\ %
PostgreSQL was chosen over all other possible alternatives for three key reasons,
which are detailed below:
\begin{enumerate}
  \item Is one of four databases officially supported by the K3s dependency (see
    section \ref{subsec:implementation_dependencies_k3s}) that may be utilized as
    an external data source for persistently storing the Kubernetes cluster's state
    (see section \ref{subsubsec:implementation_dependencies_k3s_enhancements}).

  \item The Prisma ORM (see section
    \ref{subsubsec:implementation_server_database_object_relational_mapping}) used
    in the server component implementation officially supports it (see section
    \ref{sec:implementation_server}).

  \item The server implementation mostly employs simple SQL code, with no
    complex or proprietary functionalities. As a result, it may be easily
    substituted with MySQL\footnote{\url{https://www.mysql.com}} (another database
    that fulfills the preceding two criteria) without any difficult migration
    procedures. The latter is particularly valuable since it allows organizations
    to select from a variety of database systems while being compatible with cluster
    implementation.
\end{enumerate}

\subsection{K3s}
\label{subsec:implementation_dependencies_k3s}

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/k3s.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://k3s.io}} }
  \caption{K3s logo}
\end{wrapfigure}

K3s is a lightweight Kubernetes distribution designed for production workloads
in resource-constrained, high-availability, unattended, and remote environments\cite{k3s}.
K3s is the cluster implementation's core, where all Kubernetes-related
operations are managed and processed. Because K3s is an element of both a Worker
and a Controller Node has no clear mapping to an architectural component. \\ %
K3s is distributed as a single small binary (less than 60MiB) that reduces the requirements
and procedures required to install, run, and automatically update a production Kubernetes
cluster\cite{k3s}. It is completely compatible with both the \texttt{OpenRC} and
\texttt{systemd} Init Systems (see section
\ref{subsec:implementation_distributions_init_system}) and can run on any Linux distribution
with a Kernel that satisfies the basic Kubernetes requirements\footnote{\url{https://github.com/k3s-io/k3s/blob/master/install.sh}}.

\subsubsection{Enhancements}
\label{subsubsec:implementation_dependencies_k3s_enhancements}

K3s is a fully compliant Kubernetes distribution that has removed the majority
of legacy, alpha, and cloud-provider-specific code to minimize overall
application size and hardware requirements. Furthermore, K3s provides the
following enhancements over a conventional Kubernetes distribution\cite{k3s_enhancements}:
\begin{itemize}
  \item All required dependencies are pre-packaged within a single binary file,
    necessitating only a modern Kernel and \texttt{cgroup} mounts.

  \item Lightweight storage backend with \texttt{SQlite}\footnote{\url{https://www.sqlite.org}}
    as the default storage mechanism. Additional solutions based on \texttt{etcd}\footnote{\url{https://etcd.io}},
    \texttt{MySQL}, or \texttt{PostgreSQL} are also available. Both embedded and
    external storage mechanisms are supported.
    \newline
    The cluster architecture design, as stated in chapter \ref{cha:architecture},
    is based on a high availability model rather than a fault tolerance strategy.
    As a result, it is recommended to use \texttt{etcd} (embedded or external) or
    the already available cluster's database as an external storage mechanism,
    eliminating needless data replications and relying on a single storage area
    that can be controlled autonomously.

  \item Secure by default, with appropriate default parameters for lightweight
    environments. K3s manages automatically the complexity of TLS\footnote{\url{https://wikipedia.org/wiki/Transport_Layer_Security}}
    (Transport Layer Security) like certificate distributions.
    \newline
    If security is not a priority, or if the cluster is in an Air-Gap environment,
    the security settings can be relaxed to increase overall cluster's
    performance while simultaneously decreasing resource utilization.
\end{itemize}

\subsubsection{Architecture}
\label{subsubsec:implementation_dependencies_k3s_architecture}

K3s architecture clearly distinguishes between two types of Nodes, Server nodes and
Agent nodes, which are described below\cite{k3s_architecture}. Furthermore, figure
\ref{fig:k3s} depicts the distinction between the two node types by displaying the
various components provided on each node.
\begin{enumerate}
  \item \texttt{Server} node
    \newline
    A server node is defined as a host running the K3s server command (\lstinline[language=shell,
    basicstyle=\ttfamily, morekeywords={[2]{k3s}}, morekeywords={[3]{server}}]{k3s server}),
    with K3s managing the control-plane and datastore components.
    \newline
    The \texttt{Controller} Node component of the cluster's architecture, depicted
    in the section \ref{subsubsec:architecture_components_node_controller}, is a
    logical mapping to the K3s Server Node.

  \item \texttt{Agent} node
    \newline
    An agent node is defined as a host running the K3s agent command (\lstinline[language=shell,
    basicstyle=\ttfamily, morekeywords={[2]{k3s}}, morekeywords={[3]{agent}}]{k3s agent}),
    that does not have any datastore or control-plane components running.
    \newline
    The \texttt{Worker} Node component of the cluster's architecture, depicted in
    the section \ref{subsubsec:architecture_components_node_worker}, is a
    logical mapping to the K3s Agent Node.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=.9\linewidth]{images/implementation/k3s.png} } %
  {\scriptsize Source: \url{https://k3s.io} }
  \caption{Difference between K3s Server and K3s Agent nodes}
  \label{fig:k3s}
\end{figure}

Additionally, the K3s architecture is completely adaptable and may accommodate a
variety of different setups, as shown in the list below, depending on the cluster's
usage and ultimate goal\cite{k3s_architecture}.
\begin{enumerate}
  \item \texttt{Single Server with an Embedded Database}
    \newline
    A single-node K3s server with a \texttt{SQLite} database embedded. Each
    agent node is associated with the same K3s Server node. By utilizing the K3s
    API on the Server node, an administrator can directly control the Kubernetes
    resources.
    \newline
    This configuration is only recommended for testing purposes, not for real-world
    production deployments or cluster implementation.

  \item \texttt{High-Availability Server with an Embedded Database}
    \newline
    An High-Availability (HA) K3s cluster is composed of two or more Server
    nodes that serve the Kubernetes API as well as additional control plane
    services. The database is embedded and operated on the same Server node by the
    K3s instance.

  \item \texttt{High-Availability Server with an External Database}
    \newline
    Similar to the previous setup, however, the datastore is located outside of
    the Server Node. The K3s Server instance does not execute any database application
    and instead relies on an externally accessible and operating database.
\end{enumerate}

\subsubsection{The Choice}
\label{subsubsec:implementation_dependencies_k3s_the_choice}

The K3s distribution was chosen from among a multitude of various Kubernetes distributions,
including the official one, for the four main reasons, listed below.

\begin{enumerate}
  \item Minimal hardware requirements that are simple to satisfy, even on older
    systems\cite{k3s_hardware}:
    \begin{itemize}
      \item A Server node requires 1 GiB of memory and 1 CPU core.
        \newline
        As stated in the section
        \ref{subsubsec:architecture_components_node_controller}, the Server (Controller)
        node's minimal hardware requirements must be adjusted to match the cluster's
        size. A cluster with hundreds of Worker nodes managed by a single Server
        node with only 1 GiB of memory and 1 CPU core is unrealistic.

      \item An Agent node requires 512 MiB of memory and 1 CPU core.
        \newline
        A Raspberry Pi 3 Model B single-board computer, released in February 2016\footnote{\url{https://www.raspberrypi.com/products/raspberry-pi-3-model-b-plus}}
        (7 years ago at the time of writing), is fully compatible with K3s and
        can operate as a Worker node without any difficulty.
    \end{itemize}
    In comparison, the official Kubernetes distribution, \texttt{kubeadm},
    necessitates at least 2 GiB of memory and 2 CPU cores\cite{k8s_hardware}.

  \item Configurability, modularity, and usability\cite{k3s_configuration}.
    \newline
    As an example, it is extremely simple to bootstrap a small Kubernetes
    cluster using K3s in a Single Server with an Embedded Database setup:
    \begin{itemize}
      \item Start the Server node with a unique token (used to join a Server or
        Agent nodes to the cluster).
        \newline
        \lstinline[language=shell, alsoletter={-}, morekeywords={[2]{k3s}}, morekeywords={[3]{--token}},
        morekeywords={[4]{server}}, xleftmargin=\parindent]{ k3s server --token "<TOKEN>"}

      \item Start the Agent node specifying the unique token and the Server address.
        \newline
        \lstinline[language=shell, alsoletter={-}, morekeywords={[2]{k3s}}, morekeywords={[3]{--token, --server}},
        morekeywords={[4]{agent}}, xleftmargin=\parindent]{ k3s agent --token "<TOKEN>" --server "<SERVER_ADDRESS>"}
    \end{itemize}

  \item K3s can be easily installed in an Air-Gap environment. The only
    additional steps necessary to have a fully functional cluster in comparison
    to a standard installation are the deployment of a private registry (see section
    \ref{subsec:implementation_dependencies_docker_registry}) and the manual
    deployment of particular container images on each node\cite{k3s_airgap}.
    \newline
    By default, the cluster implementation installation (see section
    \ref{sec:implementation_installer}) already meets the latter two requirements.
    As a result, an organization that has to deploy a cluster in an Air-Gap environment
    already has received all of the required files and programs without the need
    for any additional data transfer involving an internet connection. As a
    result, an organization that has to deploy a cluster in an Air-Gap environment
    already has all of the necessary files and applications without the need for
    any additional data transfer involving an internet connection.

  \item On August 19th, 2020, the K3s project was approved into the Cloud Native
    Computing Foundation\footnote{\url{https://www.cncf.io}} (CNCF) with Sandbox
    maturity level\cite{cncf_k3s}. CNCF is a vendor-neutral cloud native computing,
    hosting critical components of the global technology infrastructure
    dedicated to making cloud native ubiquitous\cite{cncf}. CNCF certifies
    Kubernetes software compliance, ensuring that every vendor's version of Kubernetes,
    as well as open source community versions, supports the large set of Kubernetes
    APIs\cite{cncf_conformance}. The latter means full compatibility with all existing
    Kubernetes products and software, allowing for the interchangeability of
    various components within the cluster implementation representing diverse
    organizations' core aspects.
\end{enumerate}

\subsection{Node Exporter}
\label{subsec:implementation_dependencies_node_exporter}

Node Exporter provides hardware and OS-related metrics exposed by the \texttt{*NIX}
Kernel\cite{node_exporter}. It is widely used in both testing and production
environments thanks to its broad compatibility, simplicity of configuration, and
reliability, and it can be regarded as the reference implementation for a Metrics
Server. Because it is an element that constitutes both a Worker and a Controller
Node, Node Exporter does not have a clear mapping to an architectural component.

\subsubsection{Collectors}
\label{subsubsec:implementation_dependencies_node_exporter-collectors}

For each Operating System, such as Linux or OpenBSD\footnote{\url{https://www.openbsd.org}},
Node Exporter supports a wide range of different collectors, such as \texttt{cpu}
for providing CPU statistics or \texttt{meminfo} for exposing memory statistics\footnote{\url{https://github.com/prometheus/node_exporter\#collectors}}.
A collector is an element of an exporter, which represents a collection of metrics.
If it is part of direct instrumentation, it may be a single metric, or it may be
multiple metrics if it is pulling metrics from another system\cite{prometheus_metrics}.
Collectors can be enabled (\lstinline[language=shell, basicstyle=\ttfamily, alsoletter={_, -, ., <, >},
morekeywords={[2]{node_exporter}}, morekeywords={[3]{--collector.<NAME>}}]{node_exporter --collector.<NAME>})
or disabled (\lstinline[language=shell, basicstyle=\ttfamily, alsoletter={_, -, ., <, >},
morekeywords={[2]{node_exporter}}, morekeywords={[3]{--no-collector.<NAME>}}]{node_exporter --no-collector.<NAME>})
based on the data that the node needs to provide for monitoring. The exposed metrics
are represented in a standardized format (see section
\ref{subsubsec:implementation_dependencies_prometheus_features}), allowing
Prometheus (see section \ref{subsec:implementation_dependencies_prometheus}) to
analyze and process them. Listing \ref{lst:node_exporter} depicts an example section
of a response generated by Node Exporter running on a node with two CPU cores and
approximately 4 GiB of system memory. Three metrics are displayed, one from the \texttt{cpu}
collector and the other two from the \texttt{meminfo} collector: \texttt{node\_cpu\_seconds\_total}
is a counter that keeps track of how many seconds each CPU core (\texttt{0} and \texttt{1})
spent in each mode, \texttt{node\_memory\_MemTotal\_bytes} is the total amount of
physical memory (in bytes), and \texttt{node\_memory\_MemFree\_bytes} is the total
amount of physical memory (in bytes) that is not in use.

\begin{lstlisting}[language=prometheus, xleftmargin=\parindent, alsoletter={_}, morekeywords={node_cpu_seconds_total, node_memory_MemTotal_bytes, node_memory_MemFree_bytes}, morekeywords={[2]{cpu, mode}}, label={lst:node_exporter}, caption=Example section of a Node Exporter response with \texttt{cpu} and \texttt{meminfo} collectors enabled]
  # HELP node_cpu_seconds_total Seconds the CPUs spent in each mode.
  # TYPE node_cpu_seconds_total counter
  node_cpu_seconds_total{cpu="0",mode="idle"} 980.6
  node_cpu_seconds_total{cpu="0",mode="iowait"} 2.18
  node_cpu_seconds_total{cpu="0",mode="irq"} 2.97
  node_cpu_seconds_total{cpu="0",mode="nice"} 0
  node_cpu_seconds_total{cpu="0",mode="softirq"} 1.47
  node_cpu_seconds_total{cpu="0",mode="steal"} 0
  node_cpu_seconds_total{cpu="0",mode="system"} 0.59
  node_cpu_seconds_total{cpu="0",mode="user"} 423.57
  node_cpu_seconds_total{cpu="1",mode="idle"} 954.29
  node_cpu_seconds_total{cpu="1",mode="iowait"} 4.44
  node_cpu_seconds_total{cpu="1",mode="irq"} 0.98
  node_cpu_seconds_total{cpu="1",mode="nice"} 0
  node_cpu_seconds_total{cpu="1",mode="softirq"} 1.94
  node_cpu_seconds_total{cpu="1",mode="steal"} 0
  node_cpu_seconds_total{cpu="1",mode="system"} 1.43
  node_cpu_seconds_total{cpu="1",mode="user"} 448.79
  # HELP node_memory_MemTotal_bytes Memory information field MemTotal_bytes.
  # TYPE node_memory_MemTotal_bytes gauge
  node_memory_MemTotal_bytes 4.063023104e+09
  # HELP node_memory_MemFree_bytes Memory information field MemFree_bytes.
  # TYPE node_memory_MemFree_bytes gauge
  node_memory_MemFree_bytes 1.88204544e+09
\end{lstlisting}

\subsubsection{Installer}
\label{subsubsec:implementation_dependencies_node_exporter_installer}

To represent diverse organizations' goals and preferences, the cluster's installation
procedure on a system node, as detailed in the section
\ref{sec:implementation_installer}, requires complete automation and configuration
when installing a dependency. Despite being a very popular and widely used
application, Node Exporter lacks a fully featured installation script, such as
\texttt{install.sh}\footnote{\url{https://github.com/k3s-io/k3s/blob/master/install.sh}}
provided by K3s, which can simply automate every operation. As a solution, a
side project called Node Exporter Installer was established to address the
latter issue(s). The installation script is completely POSIX-compliant, highly customizable,
\texttt{OpenRC} and \texttt{systemd} Init Systems compatible, and requires only basic
applications as dependencies that are ubiquitous in almost all \texttt{*NIX} systems.
The project is completely Open Source, and other users may use it to easily and rapidly
configure and install Node Exporter. Attachment
\ref{sec:corollary_projects_node_exporter_installer} has a detailed description
of the Node Exporter Installer project.

\subsubsection{Graphics Processing Unit metrics}
\label{subsubsec:implementation_dependencies_node_exporter_graphics_processing_unit_metrics}

Node Exporter does not support exporting Graphics Processing Unit\footnote{\url{https://wikipedia.org/wiki/Graphics_processing_unit}}
(GPU) metrics. In general, supporting systems equipped with GPU(s) in a uniform
and easily accessible manner is a fairly hard undertaking that necessitates a
significant amount of work in terms of initial setup and overall administration.
Furthermore, GPU-equipped systems require availability and compatibility with a
per GPU model-specific Kernel driver to be efficient without scarifying precious
performance and wasting resources. Nevertheless, exporter implementations such
as \texttt{DCGM-Exporter}\footnote{\url{https://github.com/NVIDIA/dcgm-exporter}}
and \texttt{nvidia\_gpu\_exporter}\footnote{\url{https://github.com/utkuozdemir/nvidia_gpu_exporter}}
are specifically designed to provide just GPU metrics that can be used alongside
Node Exporter metrics. However, all current GPU exporters only support NVIDIA\footnote{\url{https://www.nvidia.com}}
GPUs and require extra dependencies to be deployed. Moreover, because of its
intrinsic heterogeneity, purpose, and complexity of supporting GPU systems, the cluster
implementation currently does not offer any GPU data, metrics, or statistics.
Furthermore, because the cluster is primarily composed of consumer hardware rather
than high-end enterprise solutions, there is a high possibility that some
systems would be equipped with AMD\footnote{\url{https://www.amd.com}} GPUs, which,
as previously stated, are currently not supported by any official and/or reliable
exporter program.

\subsection{Docker Registry}
\label{subsec:implementation_dependencies_docker_registry}

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/docker_registry.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://github.com/distribution/distribution}} }
  \caption{Docker Registry logo}
\end{wrapfigure}

Docker Registry is a stateless, highly scalable server-side service for storing
and distributing container-based application images\cite{docker_registry}. A
registry is a storage and content delivery system that holds container images in
different tagged versions\cite{docker_registry_about}. \\ %
The Docker Registry, thus the name, has a direct mapping to the Registry component
of the cluster architecture.

\subsubsection{Image Naming}
\label{subsubsec:implementation_dependencies_docker_registry_image_naming}

\texttt{[REGISTRY\_HOSTNAME][:REGISTRY\_PORT]IMAGE[:TAG]} \\ %
An image name is made up of slash-separated name fields that are optionally prefixed
with a registry hostname, complying with standard DNS\footnote{\url{https://wikipedia.org/wiki/Domain_Name_System}}
(Domain Name System) rules, and optionally postfixed with a tag name that identifies
different versions of the same series of images. If a hostname is specified, it may
be followed by a port number in the format \texttt{:PORT}. If the registry
hostname is not provided, Docker, Kubernetes, Podman, and practically every
other software that deals with container images by default utilizes Docker's
public registry (Docker Hub\footnote{\url{https://hub.docker.com}}, which is
hosted at \texttt{docker.io} at port \texttt{443})\cite{k8s_image_names}. If no tag
name is supplied, the value \texttt{latest} is automatically assigned,
indicating that the image is designated as the most recent available release
version\cite{docker_tag}. \\ %
For example, the image name \texttt{node:18} (short for \texttt{docker.io:443/node:18})
indicates the container image of Node.js version 18 accessible on the Docker
public registry at port \texttt{443}. Instead, the name \texttt{registry.recluster.local:5000/node:18}
also identifies the container image of Node.js version 18 but available on the
local private registry \texttt{registry.recluster.local} at port \texttt{5000}. Therefore,
if there is a requirement in the cluster to refer to container images on a
registry different from the default one, the full image name must be provided.

\subsubsection{Hostname To IP Address Mapping}
\label{subsubsec:implementation_dependencies_docker_registry_hostname_to_ip_address_mapping}

A local registry deployment requires that there be a known mapping in the
cluster between the registry hostname and the IP address where the registry
instance is executing. There are three options available, as indicated in the
list below:
\begin{enumerate}
  \item Use a custom Kubernetes distribution configuration file/entry to instruct
    the container runtime about the mapping. Moreover, if the Kubernetes distribution
    and runtime support it, the mapping may also be used to rewrite the default
    Docker Hub registry hostname to link to an alternative local registry
    hostname. An example image \texttt{docker.io/node:18} is transparently
    rewritten as \texttt{registry.recluster.local:5000/node:18}. The latter, allows
    the cluster to continue operating without any changes to the container image
    name of the deployments, enabling future less possible maintenance burden
    because the only change that has to be performed is on the configuration
    file and not on the cluster deployments.
    \newline
    This functionality is supported by default in K3s through a configuration file
    called \texttt{registries.yaml} that must be saved in the directory \texttt{/etc/rancher/k3s}
    on each node in the Kubernetes cluster\footnote{\url{https://docs.k3s.io/installation/private-registry}}.

  \item Add an entry to the \texttt{/etc/hosts}\footnote{\url{https://tldp.org/LDP/solrhe/Securing-Optimizing-Linux-RH-Edition-v1.3/chap9sec95.html}}
    file on each cluster node.

  \item Deploy a DNS component that must be operational in the cluster at all times.
    \newline
    This solution requires that all cluster nodes be configured to know about
    the local DNS, either manually by modifying \texttt{/etc/resolv.conf}\footnote{\url{https://tldp.org/LDP/nag/node84.html}}
    file or automatically via DHCP\footnote{\url{https://wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol}}
    (Dynamic Host Configuration Protocol).
\end{enumerate}
Each of the three options has pros and cons. If the cluster size is relatively
small and the IP address of the local registry does not vary over time, the first
or second (depending on whether the chosen Kubernetes distribution supports the
functionality) solutions are recommended. However, if the cluster consists of numerous
nodes, or if the IP address of the registry changes regularly, or if further hostname
to IP mappings are required, then having a DNS component is by far the best
approach. Nonetheless, the organization operating the cluster makes the final
decision on what is the best solution: nothing prevents the deployment of a DNS component
even in a very small cluster. \\ %
Because reCluster is based on K3s, has a modest number of nodes, and the IP address
of the registry never changes thanks to MetalLB (see section
\ref{subsec:implementation_dependencies_metallb}), the decision fell almost
naturally on the first alternative. Section \ref{subsec:implementation_installer_configuration_files}
goes into further detail on this.

\subsubsection{Image Storage}
\label{subsubsec:implementation_dependencies_docker_registry_image_storage}

The registry must store the available container images in persistent storage in
order to keep them if a failure occurs or the registry instance is restarted. Storage
operation and management are not handled by the registry itself but are delegated
to specialized drivers, the default driver of which is the local POSIX file
system of the node executing the registry instance\cite{docker_registry_about}.
\\ %
Saving all container images on a single node is considered bad practice since it
creates a single point of failure that might result in the loss of all images.
Therefore, the cluster should be equipped with extra and more powerful storage drivers
intended to tolerate multiple storage device failures at the same time as well as
automated data recovery. It should be noted, however, that the driver(s) chosen must
be compatible with the cluster environment, nodes hardware, and/or meet certain extra
requirements. Almost every cloud provider supplies its customized driver that is
compatible with the underlying high-performance and distributed data center
storage system. Implementing a custom storage driver for the cluster implementation
that meets all of the above requirements is extremely difficult owing to the overall
complexity. Thankfully, a storage driver, Longhorn, from the same creators of K3s,
already exists to address the latter with extra features and customization. Thankfully,
a storage driver that addresses the latter problem with extra features and
customization already exist: Longhorn, developed by the same team that created
K3s.

\begin{wrapfigure}
  {r}{.25\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/logos/longhorn.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://longhorn.io}} }
  \caption{Longhorn logo}
\end{wrapfigure}

Longhorn\footnote{\url{https://longhorn.io}} is an Open Source and CNCF Incubating
project\footnote{\url{https://www.cncf.io/projects/longhorn}} that provides a lightweight,
reliable, and simple distributed block storage system for Kubernetes\cite{longhorn}.
A block storage system stores data in fixed-size blocks, with each block
identified by a memory address\cite{block_storage}. Longhorn provides a
dedicated storage controller for each volume and replicates it synchronously
across multiple replicas stored on multiple nodes. Kubernetes is used to orchestrate
and manage the storage controller and replicas\cite{longhorn_architecture}. Figure
\ref{fig:longhorn} depicts Longhorn's architecture overview and Read/Write Data Flow
between the Volume, Longhorn Engine, Replica Instances, and Disks.

\begin{figure}[htbp]
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=.9\linewidth]{images/implementation/longhorn.png} } %
  {\scriptsize Source: \url{https://longhorn.io} }
  \caption{Longhorn's
  architecture
  overview
  and
  Read/Write
  Data
  Flow
  between
  the
  Volume,
  Longhorn
  Engine,
  Replica
  Instances,
  and
  Disks}
  \label{fig:longhorn}
\end{figure}

It should be mentioned that having an underlying reliable persistent storage for
the whole cluster benefits not just the registry component but also every other Kubernetes
application deployment that requires any type of persistent storage, such as
database and media services.

\subsection{MetalLB}
\label{subsec:implementation_dependencies_metallb}

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/metallb.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://metallb.universe.tf}} }
  \caption{MetalLB logo}
\end{wrapfigure}

MetalLB is a load-balancer implementation for bare-metal Kubernetes clusters
that leverages standard networking and routing protocols\cite{metallb}. \\ %
MetalLB has a direct mapping to the cluster architecture's Load Balancer component,
and because it is operated directly on and managed by Kubernetes, it is also regarded
as an internal Load Balancer that does not require any external system to work.

\subsubsection{LoadBalancer Service Type}
\label{subsubsec:implementation_dependencies_metallb__loadbalancer_service_type}

By default, Kubernetes does not provide a fully functional implementation of a network
load balancer (Service of type \texttt{LoadBalancer}\footnote{\url{https://kubernetes.io/docs/concepts/services-networking/service/\#loadbalancer}})
for bare-metal clusters\footnote{\url{https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer}}.
The native Kubernetes implementation of network load balancers is just a collection
of predefined interfaces that are explicitly designed/mapped to be compatible with
the external and sophisticated load balancers running within the different cloud
providers' large and energy-hungry data centers. These interfaces are not a real
network load balancer implementations, but rather proxies that translate
different API/function calls from the Kubernetes ecosystem to the individual
cloud provider's load balancer and vice versa. As a result, all Kubernetes
Services of type \texttt{LoadBalancer} on bare-metal clusters remain in a \texttt{pending}
state indefinitely, making any deployments that rely on the latter services unavailable
from outside the Kubernetes network\cite{metallb}. \\ %
Without the capability of using Kubernetes services of LoadBalancer type, the only
alternatives for enabling external traffic to establish connections with internal
deployments are primarily two; they are briefly described below.
\begin{itemize}
  \item \texttt{NodePort}\footnote{\url{https://kubernetes.io/docs/concepts/services-networking/service/\#type-nodeport}}
    \newline
    The Kubernetes Control Plane\footnote{\url{https://kubernetes.io/docs/concepts/overview/components/\#control-plane-components}}
    allocates a port from a predetermined range, by default from port \texttt{30000}
    to port \texttt{32767}, and every node in the cluster proxies the same port
    into a Service instance. \\ %
    Every node in the cluster is configured to listen on the same allocated port
    and forward traffic to one of the Service's ready endpoints. \\ %
    Any external traffic that connects to any node in the cluster using the
    proper protocol (i.e., \texttt{TCP} or \texttt{UDP}) and port (i.e., \texttt{31234}
    or \texttt{32000}) is forwarded to a matching internal service instance\cite{k8s_service_node_port}.

  \item \texttt{ExternalIPs}\footnote{\url{https://kubernetes.io/docs/concepts/services-networking/service/\#external-ips}}
    \newline
    External IP addresses from outside the Kubernetes network are routed to one
    or more cluster nodes. Network traffic that enters the cluster using the external
    IP address (as the destination IP) on the corresponding Service port is then
    forwarded to one of the Service endpoints. \\ %
    External IP addresses are not managed by Kubernetes and are therefore the responsibility
    of the cluster administrator\cite{k8s_service_external_ips}.
\end{itemize}

Both of these service types have considerable disadvantages, making bare-metal clusters
unsuitable for production environments. \\ %
MetalLB provides a network load balancer implementation (service of type \texttt{LoadBalancer})
that directly resides within the Kubernetes environment/network and easily
integrates with standard network devices and protocols, allowing Kubernetes deployments/services
to be accessible from external network traffic.

\subsubsection{Address Allocation}
\label{subsubsec:implementation_dependencies_metallb_address_allocation}

MetalLB is responsible for allocating and assigning IP addresses to services
based on predefined IP address pools that have been manually created by the
organization in charge of cluster administration. If the IP addresses in the pool
range are already assigned and/or reachable within the (Internal) cluster network,
an IP address conflict occurs\footnote{\url{https://www.linksys.com/support-article?articleNum=132159}}.
\\ %
After MetalLB is deployed and configured, it will automatically assign and unassign
individual addresses within the pools' range to Kubernetes services of type
\texttt{LoadBalancer}. Furthermore, a service deployment may be arbitrarily set to
be allocated with a certain IP address, ensuring that it is always externally
accessible with the same IP address. The latter is only possible if the IP address
is available within the range of a MetalLB pool and is not already assigned to another
service. \\ %
It should be noted that if all available IP addresses have already been allocated,
subsequent \texttt{LoadBalancer} services remain in the \texttt{pending} state
until an IP address from a pool becomes available for assignment or a newer pool
is created\cite{metallb_address_allocation}.

\subsubsection{External Announcement}
\label{subsubsec:implementation_dependencies_metallb__external_announcement}

Once MetalLB has allocated a pool's IP address to a \texttt{LoadBalancer} service
type, the network outside Kubernetes, i.e. the Internal Network, must be aware that
the IP address has been assigned and is accessible ("\textit{lives}" in the network)
for any kind of communication to take place. To accomplish this, MetalLB deploys
a \texttt{DaemonSet}\footnote{\url{https://kubernetes.io/docs/concepts/workloads/controllers/daemonset}}
component called \texttt{Speaker} on each cluster node, which is in charge of IP
address announcements on the network\cite{metallb_installation}. \\ %
MetalLB supports two external IP announcement modes, which are explained below and
are based on common networking or routing protocols\cite{metallb_external_announcement}.
\begin{itemize}
  \item \texttt{BGP mode}\footnote{\url{https://metallb.universe.tf/concepts/bgp}}
    \newline
    In \texttt{BGP mode}, all nodes in the cluster establish BGP\footnote{\url{https://wikipedia.org/wiki/Border_Gateway_Protocol}}
    (Border Gateway Protocol) peering sessions with adjacent routers and advise
    them on how to forward traffic to the service IPs\cite{metallb_bgp}. \\ %
    Due to the requirement of BGP-capable routers and a complex configuration,
    this mode cannot be employed in cluster implementation. Furthermore, BGP is
    utilized to link with various and external Autonomous Systems\footnote{\url{https://wikipedia.org/wiki/Autonomous_system_(Internet)}}
    (AS).

  \item \texttt{Layer 2 mode}\footnote{\url{https://metallb.universe.tf/concepts/layer2}}
    \newline
    In \texttt{Layer 2 mode}, one Kubernetes cluster node takes ownership of the
    \texttt{LoadBalancer} service and utilizes standard address discovery protocols
    (ARP\footnote{\url{https://wikipedia.org/wiki/Address_Resolution_Protocol}}
    for IPv4, NDP\footnote{\url{https://wikipedia.org/wiki/Neighbor_Discovery_Protocol}}
    for IPv6) to make the IP address visible on the local network. From the perspective
    of the network LAN, the announcing node's NIC has multiple IP addresses
    assigned to it. \\ %
    The Kubernetes \texttt{kube-proxy}\footnote{\url{https://kubernetes.io/docs/concepts/overview/components/\#kube-proxy}}
    component running on the corresponding node manages all network traffic for a
    service IP, which subsequently forwards the request to one of the service's
    pods. \\ %
    \texttt{Layer 2 mode} does not employ a true load balancer method, but
    rather a failover strategy in which a new node replaces the current leader
    node if it fails for any reason.\cite{metallb_layer2}
\end{itemize}
Due to the inability of using \texttt{BGP mode} and its inherent universality to
operate on any Ethernet network without the need for additional hardware, configurations,
or routers, the cluster implementation is completely based on \texttt{Layer 2 mode}.
Furthermore, because \texttt{Layer 2 mode}, as the name implies, operates directly
on Layer 2 of the ISO/OSI model, it is fully compatible with the cluster's
Internal Network Architecture (see section \ref{subsec:architecture_network_internal_network}).
Both MetalLB and the Internal Network's Layer 2 capabilities meet the criteria for
a fully functional Wake-on-LAN (WoL) feature (see section
\ref{subsubsec:implementation_server_database_interface}).

Figure \ref{fig:metallb} depicts an example schema of a MetalLB cluster with distinct
Kubernetes namespaces, services, and pods, as well as network connections.

\begin{figure}[htbp]
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=.9\linewidth]{images/implementation/metallb.png} } %
  {\scriptsize Source: \url{https://docs.openshift.com/container-platform/latest/networking/metallb/about-metallb.html} }
  \caption{MetalLB
  cluster
  with
  distinct
  Kubernetes
  namespaces,
  services,
  and
  pods,
  as
  well
  as
  network
  connections}
  \label{fig:metallb}
\end{figure}

\subsection{Prometheus}
\label{subsec:implementation_dependencies_prometheus}

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/prometheus.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://prometheus.io}} }
  \caption{Prometheus logo}
\end{wrapfigure}

Prometheus is an Open Source system that is specifically designed for monitoring
and alerting. It is the only system that Kubernetes natively supports and the de
facto standard across the Cloud Native ecosystem\cite{prometheus_overview_faq}.
Prometheus joined CNCF in 2016 as the second hosted project with a Graduated
maturity level, only after Kubernetes itself\footnote{\url{https://www.cncf.io/projects/prometheus}}\cite{prometheus_overview}.
Prometheus has no mapping to a cluster component, and the architecture overview
indicates no monitoring component at all. This is because having Prometheus deployed
and continually operating in the cluster is a choice of the organization operating
the cluster rather than a hard necessity; even if the Metrics Server is required
on each Node. Furthermore, Prometheus may be deployed outside of the cluster
environment and configured to access node metrics from the external network, or it
can be run exclusively for certain periods, allowing monitoring of the cluster
for testing or performance evaluation. Nonetheless, Prometheus remains an
important tool for monitoring and analyzing how the cluster is behaving, which is
why it is specified as a dependency and is also included in the final archive bundle.

As previously stated, Prometheus' primary function is to collect metrics data generated
by exporters and query/interpolate this data. Moreover, Prometheus can monitor itself
since, like Node Exporter, it provides its metrics via an HTTP endpoint. There
is a plethora of available exporters and possible integrations\footnote{\url{https://prometheus.io/docs/instrumenting/exporters}}.
In addition to the default exporter installed on each node in the cluster implementation,
an organization managing the cluster can choose to add as many exporters as the
number of components and applications running on it, resulting in a better,
clearer, and finer-grained overview of the entire cluster status both in real-time
and over a period. For example, in addition to the Database component, it is
possible to install its corresponding exporter, which allows the monitoring of Database
metrics such as health, performance and resource usage.

\subsubsection{Features}
\label{subsubsec:implementation_dependencies_prometheus_features}

Prometheus has several features\footnote{\url{https://prometheus.io/docs/introduction/overview/\#features}},
but the two most essential for the cluster implementation are described below\cite{prometheus_overview}:
\begin{enumerate}
  \item A multidimensional data model that contains time series data that is uniquely
    identified by metric name and optional key-value pairs known as labels. Time
    series data are streams of timestamped values from the same metric and set
    of labeled dimensions\cite{prometheus_data_model}.
    \begin{enumerate}
      \item \texttt{Metrics}
        \newline
        The metric name, which is sometimes complemented by a short description (\lstinline[language=prometheus]{# HELP ...}),
        defines the overall system characteristic being monitored.
        \newline
        For example, \lstinline[language=prometheus, alsoletter={_},
        morekeywords={node_memory_MemFree_bytes}]{node_memory_MemFree_bytes}
        metric defines the total amount of physical memory (in bytes) that is
        not in use.
        \newline
        Each metric has a distinct type (\lstinline[language=prometheus]{# TYPE ...}).
        There is a total of four different types of metrics, which are briefly outlined
        below\cite{prometheus_metric_types}:
        \begin{enumerate}
          \item \texttt{Counter}
            \newline
            A cumulative metric that depicts a single monotonically increasing
            counter, the value of which can only increase or be reset to zero\footnote{\url{https://prometheus.io/docs/concepts/metric_types/\#counter}}.

          \item \texttt{Gauge}
            \newline
            A metric that represents a single numerical value that can vary
            arbitrarily up and down\footnote{\url{https://prometheus.io/docs/concepts/metric_types/\#gauge}}.

          \item \texttt{Histogram}
            \newline
            Samples and counts observations in variable buckets, providing also the
            total sum of all observed values\footnote{\url{https://prometheus.io/docs/concepts/metric_types/\#histogram}}.

          \item \texttt{Summary}
            \newline
            Similar to \texttt{Histogram} type, it also calculates configurable
            quantiles over a time window\footnote{\url{https://prometheus.io/docs/concepts/metric_types/\#summary}}.
        \end{enumerate}

      \item \texttt{Labels}
        \newline
        Labels allow Prometheus' dimensional data model: each given combination
        of labels for the same metric name identifies a unique dimensional
        instantiation of that metric. These dimensions can be used to filter and
        aggregate data by the \texttt{PromQL} query language.
        \newline
        For example, \lstinline[language=prometheus, alsoletter={_},
        morekeywords={node_cpu_seconds_total}, morekeywords={[2]{cpu, mode}}]|node_cpu_seconds_total{cpu="0",mode="idle"}|,
        determines the number of seconds spent in idle mode by CPU core 0.
    \end{enumerate}

  \item Prometheus includes a functional and extensible query language known as \texttt{PromQL}\footnote{\url{https://prometheus.io/docs/prometheus/latest/querying/basics}}
    (Prometheus Query Language) that allows the selection and aggregation of
    time series data in real-time. The outcome of an expression can be shown as a
    graph, tabulated data, or consumed by external systems (such as Grafana, see
    section \ref{subsubsec:implementation_dependencies_prometheus_grafana}) via
    the HTTP API\cite{prometheus_promql}.
\end{enumerate}

Two examples of \texttt{PromQL} queries that have been widely used for cluster
monitoring are shown below. Note of how the various metrics and labels are used for
filtering and aggregation.

Listing \ref{lst:promql_1} illustrates an example of a \texttt{PromQL} query to
obtain the overall CPU usage in percent (\texttt{0} to \texttt{100}). Because the
metric \lstinline[language=prometheus, alsoletter={_}, morekeywords={node_cpu_seconds_total}]{node_cpu_seconds_total}
is of type \texttt{counter}, the considered value is limited to the last one minute
(\texttt{1m}).

\begin{lstlisting}[language=promql, numbers=none, alsoletter={_01}, morekeywords={[2]{node_cpu_seconds_total}}, morekeywords={[3]{cpu, mode}}, morekeywords={[4]{100, 1, 1m}}, xleftmargin=\parindent, label={lst:promql_1}, caption=\texttt{PromQL} query to obtain overall CPU usage in percent]
  100 * (avg without (mode, cpu) (1 - rate(node_cpu_seconds_total{mode="idle"}[1m])))
\end{lstlisting}

Listing \ref{lst:promql_2} illustrates an example of a \texttt{PromQL} query to
obtain the amount of memory usage in percent (\texttt{0} to \texttt{100}).

\begin{lstlisting}[language=promql, numbers=none, alsoletter={_, 0, 1}, morekeywords={[2]{node_memory_MemTotal_bytes, node_memory_MemFree_bytes}}, morekeywords={[4]{100}}, xleftmargin=\parindent, label={lst:promql_2}, caption=\texttt{PromQL} query to obtain the amount of memory usage in percent]
  100 * ((node_memory_MemTotal_bytes - node_memory_MemFree_bytes) / node_memory_MemTotal_bytes)
\end{lstlisting}

\subsubsection{Grafana}
\label{subsubsec:implementation_dependencies_prometheus_grafana}

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/grafana.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://grafana.com}} }
  \caption{Grafana logo}
\end{wrapfigure}

Prometheus is frequently used in conjunction with Grafana\footnote{\url{https://grafana.com/grafana}},
an interactive data visualization platform\cite{grafana}. \\ %
Prometheus collects metrics and provides the sophisticated \texttt{PromQL} query
language; Grafana then translates these metrics and/or query results into relevant
charts and graphs that may be consolidated into one or more dashboards. \\ %
It should be noted that Grafana is not considered a dependency in the cluster implementation
and therefore is not distributed in the release archive bundle.

\subsection{Management}
\label{subsec:implementation_dependencies_management}

To create a cluster release bundle, all of the dependencies must be managed and downloaded
(see section \ref{sec:good_practices_bundle}). Furthermore, as stated in previous
sections, having all of the required dependencies locally during cluster
initialization without the need for an internet connection is essential for an
Air-Gap environment but can also be beneficial for speeding up the overall
installation time because there is no need to download them from the Internet. It
should be noted that if the cluster is made up of many systems with various architectures,
such as \texttt{amd64} or \texttt{arm64}, the necessary dependencies must be downloaded
for all architectures, otherwise, the installation procedure will fail with an error.
\\ %
The management of dependencies should be as straightforward as possible, with as
few manual tasks as possible. As a result, the entire procedure has been
entirely automated using a simple configuration file called \texttt{dependencies.config.yaml}
and detailed in the section \ref{subsec:implementation_dependencies_management_configuration},
as well as a POSIX script called \texttt{dependencies.sh} and explained in section
\ref{subsec:implementation_dependencies_management_script}. \\ %
It should be mentioned that dependencies management has been designed to be as compatible
as possible, with no restrictions on what dependency and version must be
downloaded. The procedure is compatible with any publicly accessible software on
\texttt{GitHub}\footnote{\url{https://github.com}} that has at least one release\footnote{\url{docs.github.com/repositories/releasing-projects-on-github/managing-releases-in-a-repository}}.
Because compatibility with just GitHub is due to certain API calls, future
versions will also include support for custom solutions as well as both \texttt{GitLab}\footnote{\url{gitlab.com}}
and \texttt{Bitbucket}\footnote{\url{https://bitbucket.org}}.
\subsubsection{Configuration}
\label{subsec:implementation_dependencies_management_configuration}

The dependencies management configuration file, \texttt{dependencies.config.yaml},
is written in YAML\footnote{\url{https://yaml.org}} format. \\ %
The attributes for each dependency declared as a root object, are defined in
table \ref{tbl:dependency_attributes}.

\begin{xltabular}
  {\textwidth} { >{\ttfamily}l | >{\ttfamily}l | X }

  \multicolumn{1}{ c |}{\large{\textbf{Name}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Type}}} &
  \multicolumn{1}{ c }{\large{\textbf{Description}}} \\ \hhline{===}

  url & string\footnote{\url{https://yaml.org/spec/1.2.2/\#10113-generic-string}}
  & The \texttt{URL} of the dependency project's \texttt{GitHub} repository. \\ \hline

  assets & string sequence & A list of assets (files) to be downloaded.
  \newline
  Regular Expression\footnote{\url{https://wikipedia.org/wiki/Regular_expression}}
  may be used in the asset string name. This is especially useful for matching
  files that may not always have the same name across releases. \texttt{Node Exporter}
  and \texttt{Prometheus}, for example, always include the version name in each release
  asset. \\ \hline

  releases & string sequence\footnote{\url{https://yaml.org/spec/1.2.2/\#10112-generic-sequence}}
  & A list of releases (versions) to download.
  \newline
  All of the files specified in the \texttt{assets} attribute are downloaded for
  each release. As a result, the specified release values must be precisely selected,
  otherwise, the number and total size of the downloaded files might quickly become
  unmanageable. For example, if three \texttt{releases} and five \texttt{assets}
  are provided, the total number of files downloaded is 15: 5 for each release.
  \newline
  All \texttt{assets} are saved in the same directory as the corresponding release
  name.
  \newline
  A special release value named \texttt{latest} is supported, which denotes the
  most recent available release version. A particular \texttt{GitHub} API\footnote{\url{https://docs.github.com/rest}}
  request is used to obtain the corresponding version name of a \texttt{latest} release.
  \\ \hline

  files & string mapping\footnote{\url{https://yaml.org/spec/1.2.2/\#10111-generic-mapping}}
  & A list of extra files to be downloaded.
  \newline
  Each file has a \texttt{name} (key) and a string \texttt{URL} (value). A file is
  downloaded from the specified \texttt{URL} and saved with the specified
  \texttt{name}. Because these files lack a matching release value, they are saved
  in the directory with the same name as the dependency root name (the same
  where release directories are saved) \\ \hline

  \caption{Dependency attributes}
  \label{tbl:dependency_attributes}
\end{xltabular}

Listing \ref{lst:dependencies_config} displays the content of the dependencies configuration
file, \texttt{dependencies.config.yaml}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/dependencies/dependencies.config.yaml}},
which is used in the cluster implementation. It is worth noting the use of the \texttt{latest}
value in the \texttt{releases} attribute, as well as the use of Regular
Expression in the \texttt{prometheus} and \texttt{node\_exporter} \texttt{assets}
attribute. Furthermore, the \texttt{files} attribute specifies the corresponding
\texttt{LICENSE} file for each dependency.

\begin{lstlisting}[language=yaml, alsoletter={.}, morekeywords={[2]{autoscaler, k3s, node_exporter, prometheus, url, assets, releases, files, LICENSE, install.sh}}, xleftmargin=\parindent, label={lst:dependencies_config}, caption=Content of dependencies configuration file]
  ---
  autoscaler:
    url: 'https://github.com/carlocorradini/autoscaler'
    assets:
      - 'cluster-autoscaler.amd64.tar.gz'
      - 'cluster-autoscaler.arm64.tar.gz'
    releases:
      - 'latest'
    files:
      LICENSE: 'https://raw.githubusercontent.com/carlocorradini/autoscaler/master/LICENSE'

  ---
  k3s:
    url: 'https://github.com/k3s-io/k3s'
    assets:
      - 'k3s'
      - 'k3s-arm64'
      - 'k3s-airgap-images-amd64.tar.gz'
      - 'k3s-airgap-images-arm64.tar.gz'
    releases:
      - 'latest'
      - 'v1.26.1+k3s1'
      - 'v1.25.6+k3s1'
    files:
      LICENSE: 'https://raw.githubusercontent.com/k3s-io/k3s/master/LICENSE'
      install.sh: 'https://raw.githubusercontent.com/k3s-io/k3s/master/install.sh'

  ---
  node_exporter:
    url: 'https://github.com/prometheus/node_exporter'
    assets:
      - 'node_exporter-[0-9]+\.[0-9]+\.[0-9]+\.linux-amd64.tar.gz'
      - 'node_exporter-[0-9]+\.[0-9]+\.[0-9]+\.linux-arm64.tar.gz'
    releases:
      - 'latest'
      - 'v1.5.0'
    files:
      LICENSE: 'https://raw.githubusercontent.com/prometheus/node_exporter/master/LICENSE'
      install.sh: 'https://raw.githubusercontent.com/carlocorradini/node_exporter_installer/main/                  install.sh'

  ---
  prometheus:
    url: 'https://github.com/prometheus/prometheus'
    assets:
      - 'prometheus-[0-9]+\.[0-9]+\.[0-9]+\.linux-amd64.tar.gz'
      - 'prometheus-[0-9]+\.[0-9]+\.[0-9]+\.linux-arm64.tar.gz'
    releases:
      - 'latest'
      - 'v2.42.0'
    files:
      LICENSE: 'https://raw.githubusercontent.com/prometheus/prometheus/main/LICENSE'
\end{lstlisting}

\subsubsection{Script}
\label{subsec:implementation_dependencies_management_script}

A POSIX script named \texttt{dependencies.sh}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/dependencies/dependencies.sh}}
is used to automate all dependencies-related procedures. Its primary function is
to read the dependencies configuration file and synchronize (download) the dependencies
listed therein. \\ %
The script requires some \texttt{coreutils} package's utility programs and the
\texttt{yq} application to correctly handle \texttt{YAML} file and syntax. \\ %
Dependencies' script behavior is customizable by using argument flags, which
begin with a double dash followed by the property name and an optional value (\texttt{--NAME
[VALUE]}). \\ %
The accepted configuration parameters are listed in the table below.

\begin{xltabular}
  {\textwidth} { >{\ttfamily}l | X | >{\ttfamily}c }

  \multicolumn{1}{ c |}{\large{\textbf{Name}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Description}}} &
  \multicolumn{1}{ c }{\large{\textbf{Default Value}}} \\ \hhline{===}

  --config-file <FILE> & Path to the dependencies configuration file (\texttt{<FILE>}).
  \newline
  Both relative and absolute paths are supported.
  \newline
  It should be noted that the configuration file name does not have to be \texttt{dependencies.config.yaml},
  but may be any name and extension. The sole condition is that it must be in
  \texttt{YAML} format and that the attributes structure is respected. &
  dependencies.config.yaml \\ \hline

  --sync & Synchronize all dependencies specified in the configuration file.
  \newline
  Dependencies are saved in the current working directory.
  \newline
  If a file is already present in the respective directory, it is not downloaded
  (skipped). & \\ \hline

  --sync-force & The same as \texttt{--sync}, except that all dependencies are
  downloaded even if they are already present locally.
  \newline
  The downloaded file replaces the local one (if exists). & \\ \hline

  --log-level <LEVEL> & Logging level (\texttt{<LEVEL>}).
  \newline
  Attachment \ref{cha:logging} provides additional information regarding logging
  and logging levels.
  \newline
  The following logging levels are supported (listed in descending order of importance):
  \begin{itemize}[noitemsep]
    \item[\protect\icircled{\texttt{5}}] \texttt{fatal}

    \item[\protect\icircled{\texttt{4}}] \texttt{error}

    \item[\protect\icircled{\texttt{3}}] \texttt{warn}

    \item[\protect\icircled{\texttt{2}}] \texttt{info}

    \item[\protect\icircled{\texttt{1}}] \texttt{debug}
  \end{itemize}
  & info \\ \hline

  --help & Display a help message and terminate (successfully). & \\ \hline

  \caption{Dependencies script parameters}
\end{xltabular}

To synchronize (force) all of the dependencies listed in the configuration file
with the script, just execute: \lstinline[language=shell, alsoletter={.-},
morekeywords={[2]{dependencies.sh}}, morekeywords={[3]{--sync-force}}]{./dependencies.sh --sync-force}

\section{Server}
\label{sec:implementation_server}

Server\footnote{\url{https://github.com/carlocorradini/reCluster/tree/main/server}}
is a completely custom implementation that manages all low-level cluster operations,
such as turning on and off nodes, as well as user operations, such as
authentication and authorization. The server is written entirely in \texttt{TypeScript},
a strongly typed programming language that is compiled/transpiled\footnote{\url{https://wikipedia.org/wiki/Source-to-source_compiler}}
to \texttt{JavaScript}\footnote{\url{https://developer.mozilla.org/docs/Web/JavaScript}}\cite{typescript},
and executed by the \texttt{Node.js} \texttt{JavaScript} runtime\cite{nodejs}.
When dealing with particular cluster processes, it has a strong policy of resource-waste
minimization, attempting to reduce overall cluster power consumption while also
enhancing the resource utilization of active nodes. It is intended to be completely
interoperable with a Kubernetes cluster composed of heterogeneous systems. It
provides a rich, type-safe, and secure \texttt{GraphQL API} (see section
\ref{subsec:implementation_server_graphql_api}) that can be utilized manually by
administrators and basic users or automatically by scripts and other cluster components,
such as the Cluster Autoscaler (see section \ref{subsec:implementation_autoscaling_cluster_autoscaler}).
Server, as the name implies, has a direct mapping to the cluster architecture's Server
component. \\ %
To query, monitor (see section \ref{subsec:implementation_server_monitoring}), and
manipulate the state of the Kubernetes cluster, server implementation mainly
relies on the Kubernetes API. The Kubernetes API server, which is operating on a
Controller node, listens on port \texttt{6443} and is secured with TLS. TLS certificates
can be signed using a private Certificate Authority\footnote{\url{https://wikipedia.org/wiki/Certificate_authority}}
(CA) or with a Public Key Infrastructure\footnote{\url{https://wikipedia.org/wiki/Public_key_infrastructure}}
(PKI) linked to a well-known CA\cite{k8s_api_access}. Because the cluster implementation
supports deployment in an Air-Gap environment, without any connection to an
external network, and leveraging a fully-fledged certificate infrastructure for most
of the conceivable use-case scenarios might be regarded a waste of resources and
an unnecessary effort, it is recommended to utilize the solution with the
private Certificate Authority, which is also the default one. If the Kubernetes
cluster implementation is configured to work with a certificate signed by a
private CA, then any K8s client, including the one employed by the Server, must have
a local copy of the certificate, which is available within the corresponding
\texttt{kubeconfig} file generated by the Controller, to mutually trust the
connection (client-controller and controller-client) and be confident that it is
not intercepted\cite{k8s_api_access}. The overall cluster implementation is
meant to operate with a certificate signed by a private CA by default, hence the
Server requires a copy of the \texttt{kubeconfig} file (see section \ref{subsubsec:implementation_installer_cluster_initialization_kubeconfig}).
If the Kubernetes Server and the Server are deployed on the same node, as is the
case with reCluster, the latter is completely automated by the installation script
(see section \ref{sec:implementation_installer}). \\ %
The server resource-waste reduction strategy is maintained automatically and in conjunction
with the Cluster Autoscaler component implementation, without any human/physical
interaction. As stated in chapter \ref{cha:architecture}, the Server and Cluster
Autoscaler are required and operate as a single entity. Because it only has a low-level
understanding of the cluster, the Server does not know when to scale (automatically)
without the Cluster Autoscaler, which monitors the overall cluster workload and each
active node resource utilization. Vice versa, without the Server, which has
global low-level knowledge of the whole cluster, including active and inactive nodes,
the Cluster Autoscaler only knows when to scale nodes up or down but not how to
do so.

This section is split into five segments, each of which requires the knowledge provided
by the previous segment to be completely understood. The first segment depicts
the Database structure, its tables, and their relationships. The schema, which
contains the core knowledge of the cluster and users, is constantly updated and persistently
stored. The second segment depicts the GraphQL API and its queries (queries and mutations).
The API provides advanced knowledge, the foundation of which is the database and
its schema, in that certain queryable data are not available in the database but
are calculated using multiple raw data from persistent storage. The latter is
prominent throughout the autoscaling procedure. Certain queries are secured by authentication,
authorization, or both, while others are exposed to any entity making the
request (users, nodes, etc...). The third segment focuses on the autoscaling
technique, illustrating how nodes are automatically powered on (from an inactive
to active state) or powered off (from an active to inactive state). The latter
are fully automated procedures that are triggered via specific GraphQL queries
and consumed by either automatic entities like the Cluster Autoscaler or human
entities like an administrator. The fourth segment demonstrates how the server continuously
monitors the Kubernetes cluster using a specialized node informer that checks to
see whether a node resource is added, deleted, or updated. The latter aids in
keeping the cluster's knowledge consistent and up to date by using the
Kubernetes heartbeat and resource systems. The final segment is about server configuration:
how it may be configured and the many available parameters.

\subsection{Database}
\label{subsec:implementation_server_database}

This section is concerned with the Database, its structure (schema), and the interactions
between it and the Server. \\ %
To interface with the Database, the Server implementation does not directly
involve any raw SQL queries but instead depends on a kind of middleware layer
that automatically translates any Database specific \texttt{TypeScript} functions
and data structures into raw \texttt{SQL} queries and vice versa. Section \ref{subsubsec:implementation_server_database_object_relational_mapping}
explains the latter, as well as the procedures and technologies involved. \\ %
Section \ref{subsubsec:implementation_server_database_schema} depicts the
Database schema, which is a pure mapping of the cluster's low-level knowledge in
the form of raw data, its many nodes (both active and inactive), and the various
registered users. The schema not only illustrates how the various data are
defined and organized in the Database, but it also reflects the cluster's knowledge
and what data and information constitute it.

\subsubsection{Object-Relational Mapping}
\label{subsubsec:implementation_server_database_object_relational_mapping}

\begin{wrapfigure}
  {l}{.35\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/implementation/orm.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://www.educative.io/answers/what-is-object-relational-mapping}} }
  \caption{Relationship between Objects, ORM, and Relational Database}
  \label{fig:orm}
\end{wrapfigure}

Prisma ORM\footnote{\url{https://www.prisma.io}} manages all database-related operations
in the Server implementation, removing the need for raw \texttt{SQL} queries and
replacing them with more natural, easy-to-use, and maintainable \texttt{TypeScript}
functions and data structures. \\ %
Object-Relational Mapping (ORM) is a logical data bridge (middleware) that
connects a relational database to an object-oriented programming language. The ORM
converts the logical representation of the objects into an atomized form that can
be saved in the database while keeping the characteristics and relationships of the
objects so that they may be reloaded as objects/data structures\cite{orm}. Figure
\ref{fig:orm} represents this relationship and illustrates how the objects of an
object-oriented programming language, the ORM, and the relational database are related
together. \\ %
Consider a database with a table that represents the nodes in the cluster and has
a structure similar to the one shown in section \ref{subsubsec:implementation_server_database_schema}.
The \texttt{name} and \texttt{memory} properties of the node with the identifier
\texttt{e2b87848-bba0-46d5-923b-403f7141564f} should be selected. The raw
\texttt{SQL} query looks like the following code:

\begin{lstlisting}[language=typescript, xleftmargin=\parindent, morekeywords={[2]{id, result}}, morekeywords={[3]{query}}, caption=Raw SQL query to retrieve the \texttt{name} and \texttt{memory} attributes of the node with the identifier \texttt{e2b87848-bba0-46d5-923b-403f7141564f}]
  const id = "e2b87848-bba0-46d5-923b-403f7141564f";

  const result = await query("SELECT name, memory FROM node WHERE id = $1", [id]);
\end{lstlisting}

In contrast, the same query using the Prisma ORM looks like the following code:

\begin{lstlisting}[language=typescript, xleftmargin=\parindent, morekeywords={[2]{id, result, prisma, node, select, name, memory, where}}, morekeywords={[3]{findUnique}}, caption=Prisma ORM query to retrieve the \texttt{name} and \texttt{memory} attributes of the node with the identifier \texttt{e2b87848-bba0-46d5-923b-403f7141564f}]
  const id = "e2b87848-bba0-46d5-923b-403f7141564f";

  const result = await prisma.node.findUnique({
    select: {
      name: true,
      memory: true
    },
    where: {
      id: id
    }
  });
\end{lstlisting}

\begin{wrapfigure}
  {r}{.35\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/implementation/orm_productivity_control.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://www.prisma.io/docs/concepts/overview/why-prisma}} }
  \caption{\texttt{Productivity}
  vs.
  \texttt{Control}
  tradeoff
  between
  various
  database
  tools}
  \label{fig:orm_productivity_control}
\end{wrapfigure}

Take note of the differences, particularly how the ORM query is simpler, type-safe,
and does not directly involve any \texttt{SQL} code but rather pure programming language
primitives. \\ %
ORMs are not the only tool for interfacing with a Relational Database;
alternative options, such as Raw SQL and SQL query builders, are available; nonetheless,
ORMs provide the finest balance between application productivity and database
control. Raw SQL provides complete control over database operations, but
productivity suffers since interacting with raw SQL in code is time-consuming, error-prone,
and produces a lot of overhead and maintainability issues. SQL query builders maintain
a high level of control over database operations and give somewhat higher productivity,
but they do it by relying on logical thinking in SQL rather than programming
objects, functions, and primitives\cite{prisma_why}. Figure
\ref{fig:orm_productivity_control} depicts the productivity vs control tradeoff
of the various database tools, including Prisma.

\begin{wrapfigure}
  {l}{.25\textwidth} %
  \centering
  \def\stackalignment{l}\stackunder{ \includegraphics[width=\linewidth]{images/logos/prisma.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://www.prisma.io}} }
  \caption{Prisma logo}
\end{wrapfigure}

Prisma is an open-source next-generation ORM that defines the database data
model structure using a Prisma Schema file\footnote{\url{https://www.prisma.io/docs/concepts/components/prisma-schema}}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/prisma/schema.prisma}},
which is then used to construct the mapping between the database and the programming
language with a rich and user-friendly API. Furthermore, because Prisma is not designed
as a standard ORM but instead employs an intermediary Schema file, it can
improve overall productivity and control over traditional ORMs\cite{prisma}.
Figure \ref{fig:orm_productivity_control} reflects the latter.

\subsubsection{Schema}
\label{subsubsec:implementation_server_database_schema}

This section depicts and describes the database schema structure, as well as how
the various entities (tables) are connected. Figure \ref{fig:erm} displays the overall
high-level representation of the database schema as an Entity Relationship Model\footnote{\url{https://wikipedia.org/wiki/Entity-relationship_model}}
(ERM), where \fcolorbox{black}{ermGray}{\rule{0pt}{6pt}\rule{6pt}{0pt}}
represents an Entity, \fcolorbox{black}{ermBlue}{\rule{0pt}{6pt}\rule{6pt}{0pt}}
an Enum, and \textbf{---} a relation between two entities. An Entity has a
header and a body, with the header representing the entity's name and the body representing
the attributes that make it up. The body is divided into three columns: the first
column provides the attribute names, the second column is the type of the
related attributes, and the third column contains extra information and context about
the corresponding attribute (e.g. \texttt{PK} indicates that the attribute is a
Primary Key\footnote{\url{https://wikipedia.org/wiki/Primary_key}} of the entity).
An enum has the same form as an entity, with a header and a body, but the body
has a single column that represents all of the Enum's possible values. \\ %
Following the schema, each entity is extensively described, as are the
attributes that comprise it, as well as the relation(s) cardinality and why it
is important. \\ %
This part's information is crucial for gaining a better understanding of the
data that the server requires and the installation script (see section \ref{sec:implementation_installer})
collects from a node. Additionally, the latter is necessary for comprehending
the raw data provided and processed by the GraphQL API (see section \ref{subsec:implementation_server_graphql_api}).

\clearpage
\newpage


\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{images/implementation/erm.pdf}
  \caption{Database Entity Relationship Model (ERM)}
  \label{fig:erm}
\end{figure}

\clearpage
\newpage


\subsubsection{User}
\label{subsubsec:implementation_server_database_user}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_user.pdf}
\end{figure}

The user entity represents all of the cluster's registered users. \\ %
A user can logically be a real person, such as an administrator, or an automation
system, such as the Cluster Autoscaler. \\ %
The user entity is currently solely used for authentication and authorization in
the GraphQL API. Certain queries are only available to authenticated users,
while others are only available to users with specified roles and/or permissions,
or both. Yet, nothing prevents the future implementation of additional capabilities
or the extension of the entity with extra attributes and/or relationships with other
(new) entities. Because of the latter, this entity differs from the others in that
it exists independently of other entities. \\ %
The \texttt{user} entity attributes are as follows:
\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) a user record.
    \newline
    The attribute is of type Universally Unique Identifier\footnote{\url{https://wikipedia.org/wiki/Universally_unique_identifier}}
    (\texttt{UUID}) version 4, and it is a 128-bit alphanumeric value generated
    by a (secure) random number generator. Unless otherwise specified, the UUID
    value is generated automatically whenever a new user record is added to the
    entity.
    \newline
    The \texttt{UUID} type is widely used in database schema to uniquely
    identify all entity records.

  \item \texttt{username}
    \newline
    User's username.
    \newline
    A string of characters that uniquely (\texttt{UNIQUE}) identifies a user
    record.
    \newline
    The \texttt{username} and \texttt{id} attributes are logically interchangeable
    since they both uniquely identify the same user record. Yet, memorizing a personalized
    and simple sequence of characters (\texttt{username}) is significantly
    simpler than remembering a 128-bit alphanumerical (\texttt{UUID}).
    \newline
    During the authentication method in the GraphQL API, the \texttt{username}
    attribute is utilized in combination with the \texttt{password} attribute to
    guarantee that the user exists and is who it claims to be.

  \item \texttt{password}
    \newline
    User's password.
    \newline
    A (secret) string of letters, numbers, and symbols used to identify the user
    during the authentication procedure.
    \newline
    The password has rigorous criteria that can be changed in the server
    configuration (see section \ref{subsec:implementation_server_configuration}):
    A minimum of eight characters (inclusive), one capital letter, one number,
    and one symbol are required.
    \newline
    The password stored in the database is not in plain text; rather, it is derived
    from the original one provided by the user during the registration process using
    a password-hashing function\footnote{\url{https://wikipedia.org/wiki/Password-hashing_function}}.
    The password-hashing function employed in the implementation is \texttt{bcrypt}\footnote{\url{https://wikipedia.org/wiki/Bcrypt}},
    which is based on the \texttt{Blowfish}\footnote{\url{https://wikipedia.org/wiki/Blowfish_(cipher)}}
    cipher and includes a \texttt{salt}\footnote{\url{https://wikipedia.org/wiki/Salt_(cryptography)}}
    (random data used as an additional input). The \texttt{bcrypt} function is
    an industry-standard that can withstand brute-force attacks and other
    threats\cite{bcrypt}.
    \newline
    It should be noted that, for security reasons, the \texttt{password} attribute
    is never returned by the GraphQL API (there is no mapping at all in the
    GraphQL schema) or by default by the Prisma ORM unless specifically provided
    in the \texttt{select} object.

  \item \texttt{roles}
    \newline
    User's roles.
    \newline
    Roles are used to logically group users into established categories, with
    various roles indicating varying levels of privilege.
    \newline
    The \texttt{roles} attribute is of type \texttt{array} (\texttt{[]}) of \texttt{user\_role}
    enum since a user might have multiple/different responsibilities at the same
    time.
    \newline
    The \texttt{user\_role} enum has the following possible values/constants:
    \begin{itemize}
      \item \texttt{ADMIN}
        \newline
        Identifies administrators and/or automation systems.
        \newline
        It is the highest currently available role that permits the execution of
        any operation (query).

      \item \texttt{SIMPLE}
        \newline
        Identifies users who do not have any extra privileges.
        \newline
        When a new user is added to the database, the role assigned by default
        is \texttt{SIMPLE}.
    \end{itemize}
    It should be noted that a \texttt{SIMPLE} user cannot perform any operations
    that an ADMIN can't. Also, the \texttt{roles} attribute must always include at
    least one value.

  \item \texttt{permissions}
    \newline
    User's permissions.
    \newline
    Identifies the possible actions that a user is allowed to execute.
    \newline
    The \texttt{permissions} attribute is directly tied to the \texttt{roles}
    attribute and the context in which a query is executed. For example, two
    \texttt{ADMIN} users can change data on a node in the cluster (both have the
    \texttt{UPDATE} permission), but only the first user can delete a node record
    from the entity since it has the \texttt{DELETE} permission, while the second
    does not.
    \newline
    The type of the attribute is an \texttt{array} of \texttt{user\_permission}
    enum since a user might have multiple/different permissions depending on the
    role(s) and context.
    \newline
    The \texttt{user\_permission} enum is currently empty (hence the default
    value as an empty array) since no queries require rigorous authentication/authorization.
    Still, future implementations are completely supported.

  \item \texttt{created\_at}
    \newline
    Date and time of creation.
    \newline
    The attribute indicates when the user record was created.
    \newline
    The type is \texttt{timestamptz}\footnote{\url{https://www.postgresql.org/docs/current/datatype-datetime.html}}
    because it specifies a timestamp that also includes timezone information for
    the database's deployment location. It should be noted that the timezone is
    set to Coordinated Universal Time\footnote{\url{https://wikipedia.org/wiki/Coordinated_Universal_Time}}
    (UTC) by default, therefore it is not tied to a specific timezone and hence the
    value may be readily converted to a specific timezone.
    \newline
    The attribute is automatically set thanks to \texttt{DEFAULT} which gets the
    value from the \texttt{NOW()} function that returns the current timestamp
    and timezone.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the user record was updated.
    \newline
    The attribute is of type \texttt{timestamptz}.
    \newline
    The value is controlled automatically by the server rather than the database.
    The latter is owing to differing database implementations, some of which
    support the feature natively while others do not.
\end{itemize}

% TODO Change
\pagebreak


\subsubsection{Node}
\label{subsubsec:implementation_server_database_node}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_node.pdf}
\end{figure}

The node entity represents all of the cluster's physical nodes, both active and inactive.
It holds critical information about a node, and from this entity, other
information like the current status or the hardware components may be obtained. \\ %
Certain GraphQL API queries can only be performed by an authorized node. The node
lacks a \texttt{password} attribute like the user entity, both because it is bad
practice keeping the plain password in the node system and also because it is an
entirely new kind of object that must handle the various operations differently.
When the node record is created in the database for the first time, the auth token
(in the user returned when it is successfully authenticated after providing the
correct \texttt{username} and \texttt{password} attributes, see section \ref{subsubsec:implementation_server_graphql_api_mutations})
is returned only once and it cannot be obtained again. As a result, the auth
token is securely stored only in the corresponding node, and if it is lost for any
reason, indicating that the node is faulty or compromised, the only way to retrieve
a new token and perform secure operations on the specific node is to register the
node in the cluster again (removing the old record). \\ %
The \texttt{node} entity attributes are as follows:
\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) a node record.
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    The \texttt{id} attribute is related to multiple entities. A node can only
    have one status (see section \ref{subsubsec:implementation_server_database_status}),
    and the status record belongs to only one unique node, hence the relation is
    one (\texttt{1}) to one (\texttt{1}). A node can have several storages (see
    section \ref{subsubsec:implementation_server_database_storage}), but each
    storage corresponds to a single and distinct node, hence the relation is one
    (\texttt{1}) to many (\texttt{*}). Lastly, like with the previous relation,
    a node might have several interfaces (see section \ref{subsubsec:implementation_server_database_interface}),
    but each interface corresponds to a single and unique node, therefore the
    relation is one (\texttt{1}) to many (\texttt{*}).

  \item \texttt{node\_pool\_id}
    \newline
    Node pool identifier.
    \newline
    Uniquely identify a node pool record (\texttt{FK}).
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    When a new node joins the cluster, it is automatically assigned to a node
    pool (see section \ref{subsubsec:implementation_server_database_node_pool})
    based on its roles and hardware. If there is no node pool for the node, one
    is automatically created. The relation is many (\texttt{*}) to one (\texttt{1}),
    indicating that a node may only be allocated to one node pool, but a node pool
    can be assigned to several nodes.

  \item \texttt{cpu\_id}
    \newline
    Cpu identifier.
    \newline
    Uniquely identify a CPU record (\texttt{FK}).
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    During the registration phase for a new node in the cluster, it is first
    verified to determine if the same CPU type is already registered in the
    database (see section \ref{subsubsec:implementation_server_database_cpu}); if
    not, a new CPU record is created using the information supplied by the node
    and then assigned to it.
    \newline
    The relation is many (\texttt{*}) to one (\texttt{1}), indicating that a
    node may only have one CPU, although the same CPU can be present on several nodes.

  \item \texttt{name}
    \newline
    Node's name.
    \newline
    A string of characters that uniquely (\texttt{UNIQUE}) identifies a node
    record.
    \newline
    It serves the same purpose as the user entity's \texttt{username} attribute:
    it allows users/administrators to easily identify a node by using a shorter name
    rather than the 128-bit alphanumerical identifier (\texttt{id}). As a result,
    the \texttt{name} and \texttt{id} attributes are interchangeable because
    they both uniquely identify the same node entry.
    \newline
    A default value is automatically assigned by the Server during the node registration
    procedure and consists of two sections separated by a dot (\texttt{.}). The first
    section provides a high-level overview of the node's role, which can be
    either \texttt{Controller} or \texttt{Worker} (if a node is both, \texttt{Controller}
    is preferred since it has the highest logical importance). The node's identifier
    is the second element. As an example, a node with the id \texttt{e2b87848-bba0-46d5-923b-403f7141564f}
    and both roles will be given the name \texttt{controller.e2b87848-bba0-46d5-923b-403f7141564f}.
    \newline
    Because this attribute is only deemed as a helper, its value can be modified
    by administrators (via the \texttt{updateNode} GraphQL query) to fit a more
    appropriate name that better represents the node in the cluster.

  \item \texttt{roles}
    \newline
    Node's roles.
    \newline
    This attribute is nearly similar to that of the user entity, but it is
    unique to nodes rather than users. Roles are used to logically categorize nodes,
    with different roles indicating various degrees of privilege.
    \newline
    Because a node may have multiple/different responsibilities at the same time,
    the \texttt{roles} attribute is of the form array (\texttt{[]}) of \texttt{node\_role}
    enum.
    \newline
    The \texttt{node\_role} enum has the following possible values/constants:\

    \begin{itemize}
      \item \texttt{RECLUSTER\_CONTROLLER}
        \newline
        The corresponding node has an instance of a reCluster component that
        controls (monitors) the cluster and thus must be operational at all
        times.
        \newline
        The Server component is currently the only reCluster component that can be
        deployed outside of the K8s environment and needs to be always active.

      \item \texttt{K8S\_CONTROLLER}
        \newline
        The node is a Kubernetes Controller that must be maintained functioning
        at all times.
        \newline
        It must not be autoscaled by the Cluster Autoscaler component and the only
        entities authorized to deactivate it are administrators via a specific protected
        GraphQL query.

      \item \texttt{K8S\_WORKER}
        \newline
        The node is a Kubernetes Worker that can be autoscaled by the Server
        component in combination with the Cluster Autoscaler component.
        \newline
        It is important to note that if a node is also a Controller, it is not
        autoscaled.
    \end{itemize}
    A node can have multiple roles assigned to it, but the \texttt{roles}
    attribute must always have at least one value. A node can be a Kubernetes Controller
    (\texttt{K8S\_CONTROLLER}) and Worker (\texttt{K8S\_WORKER}) that monitors the
    K8s cluster but also accepts workload, as well as a reCluster Controller (\texttt{RECLUSTER\_CONTROLLER})
    that enables physical node autoscaling and provides the GraphQL API.
    \newline
    The generic Controller component in reCluster is both a Kubernetes
    Controller and a reCluster Controller, but not a Worker.

  \item \texttt{permissions}
    \newline
    Node's permissions.
    \newline
    This attribute is comparable to that of the user entity, but it is specific
    to nodes rather than users. The \texttt{roles} attribute and the context in
    which a query is performed are both directly related to the \texttt{permissions}
    attribute. Furthermore, and this is where the \texttt{permissions} in the
    node entity differ, it can be used within the node during some predefined actions
    such as before termination or after bootstrap.
    \newline
    Because a node may have multiple/different \texttt{permissions} depending on
    the role(s), context, and action, the attribute's type is an array of \texttt{node\_permission}
    enum.
    \newline
    Even though the cluster implementation supports it out of the box, there are
    currently no available permissions for nodes (hence the empty array as the default
    value). The latter enables future implementations to easily add and modify the
    \texttt{node\_permission} enum and \texttt{permissions} attribute without changing
    the implementation.

  \item \texttt{address}
    \newline
    Node's IP address.
    \newline
    The IP address of the node in the Internal Network. There must be no nodes
    with the same IP address in the Internal Network. The latter is reflected in
    the database schema, which specifies that the attribute value must be
    \texttt{UNIQUE}, and thus two nodes cannot have the same IP address. As a result,
    this attribute can also be used to uniquely distinguish a node record in the
    nodes entity.
    \newline
    Because each database implementation has its own specific/custom datatype for
    encoding an IP address, the attribute is of type text.
    \newline
    During the downscaling procedure (see section
    \ref{subsec:implementation_server_downscaling}), the node's IP address is
    used to establish an SSH connection (from Server to Node) and remotely
    terminate the node. Furthermore, the IP address is available on multiple
    GraphQL queries and can be used by users or scripts. The latter is especially
    helpful when using Prometheus because it requires the IP addresses of the nodes
    where the metrics are published to monitor them.
    \newline
    The attribute's value is constantly updated using the K8s informer provided
    by the Kubernetes API (see section \ref{subsec:implementation_server_monitoring}).
    Because of this, the Database and thus the Server always have the most
    recent available IP address of the node, even though (technically) it should
    never change.

  \item \texttt{memory}
    \newline
    Node's memory size in bytes.
    \newline
    Because a large amount of memory does not fit into the smaller \texttt{integer}
    type, the attribute is of type \texttt{bigint}\footnote{\url{https://www.postgresql.org/docs/current/datatype-numeric.html}}.
    \newline
    The unit is in \texttt{byte} because it is the smallest on which memory can
    be represented (\texttt{bit} unit is almost useless and the conversion is
    straightforward). The latter is especially useful when dealing with
    conversions (which are directly supported by the GraphQL API) and
    comparisons because the unit is always the same and thus no further
    adjustments are needed.
    \newline
    Every attribute in the database that defines a quantity is represented in its
    smallest unit. The installation script or the server automatically converts
    a larger value to its smallest unit.

  \item \texttt{node\_pool\_assigned}
    \newline
    A Boolean flag that indicates whether the node is allocated to the
    corresponding node pool.
    \newline
    The attribute is of type \texttt{boolean}, with \texttt{true} indicating
    that the node is allocated to the node pool (visible to Kubernetes) and
    \texttt{false} indicating that the node is not allocated to the node pool (not
    visible to Kubernetes).
    \newline
    As previously stated, a node is always related to a node pool upon registration.
    This attribute is used in combination with the status entity (see section \ref{subsubsec:implementation_server_database_status})
    to decide whether the node is appropriate for autoscaling. A node in the \texttt{inactive}
    state that is not allocated to a node pool can be selected for automatic
    bootstrap. A node that is not allocated to the node pool but is in a state other
    than \texttt{inactive} indicates that the node is bootstrapping, terminating,
    or experiencing a problem.
    \newline
    The default value (\texttt{DEFAULT}) is \texttt{false} because the node has not
    yet initiated any Kubernetes-related processes and thus cannot be allocated to
    the pool when it is registered for the first time in the cluster. Only when
    the installation procedure has been completed successfully and the node
    sends a request to a specific and protected GraphQL API along with the auth
    token the value can be set to \texttt{true}.

  \item \texttt{min\_power\_consumption}
    \newline
    Minimum power consumption in \texttt{Watt}\footnote{\url{https://wikipedia.org/wiki/Watt}}
    (\texttt{W}).
    \newline
    The node's minimum power consumption when there is no workload and the running
    processes are the bare minimum.
    \newline
    The unit is in \texttt{Watt} rather than a lower one because the device used
    to measure the power consumption of the different nodes has a minimum sensitivity
    of \texttt{Watt} (see section \ref{subsec:implementation_installer_node_power_consumption}).
    Furthermore, because the read values are always integers (i.e., no decimal point/float),
    the attribute's type can be securely set to \texttt{integer}. To represent that
    the device can read units well below the \texttt{Watt}, the type can be set to
    \texttt{bigint} in different implementations to prevent overflow. Nonetheless,
    because the maximum value of an \texttt{integer} is approximately \texttt{2}
    or \texttt{4} billions (depending on whether the sign can be ignored or not
    depending on the database implementation) and there are no systems that utilize
    this much, the \texttt{integer} type is well above the needs for the implementation.
    \newline
    The latter affects every attribute in the database that denotes a value for
    power consumption.

  \item \texttt{max\_efficiency\_power\_consumption}
    \newline
    Maximum power consumption in \texttt{Watt} (\texttt{W}) when in a power-efficient
    state.
    \newline
    Modern systems are beginning to include hybrid components that, when under
    low workload conditions, are more power efficient, consuming less energy at
    the expense of performance, while when under high workload conditions, are
    more energy-hungry (less power efficient), but with higher performance and reactiveness.
    With efficiency and performance cores, the latter is becoming more prevalent
    in the CPU ecosystem (see section
    \ref{subsubsec:implementation_server_database_cpu}).
    \newline
    Because hybrid systems are still in their early stages and the
    implementation does not yet recognize when and how the threshold is reached,
    causing the system to transition from an efficiency to a performance state,
    the attribute is \texttt{NULLABLE}. Furthermore, a \texttt{NULL} value can
    determine whether or not the node is hybrid.

  \item \texttt{min\_performance\_power\_consumption}
    \newline
    Minimum power consumption in \texttt{Watt} (\texttt{W}) when in a
    performance state.
    \newline
    It is closely related to the previous attribute \texttt{max\_efficiency\_power\_consumption},
    but in this case, the attribute reflects the minimum power consumption when
    the hybrid system is in the performance state and has the lowest workload. Simply
    put, it is the minimum power consumption when switching from a power-efficient
    to a performance state.
    \newline
    This attribute, like the prior one, is \texttt{NULLABLE} due to the present
    lack of tools and the effort needed to monitor hybrid systems, and a \texttt{NULL}
    value can identify whether or not the node is hybrid.
    \newline
    It is important to note that whenever the node is registered or updated,
    both the attributes \texttt{max\_efficiency\_power\_consumption} and \texttt{min\_performance\_power\_consumption}
    must be provided; otherwise, an error is generated and the operation is
    terminated.

  \item \texttt{max\_power\_consumption}
    \newline
    Maximum power consumption in \texttt{Watt} (\texttt{W}).
    \newline
    The node's maximum power consumption when under full workload, and thus its
    hardware resources are under intense stress.
    \newline
    It is the opposite of the \texttt{min\_power\_consumption} attribute, and its
    value is read when the node is under extreme stress.

  \item \texttt{created\_at}
    \newline
    Date and time of creation.
    \newline
    The attribute indicates when the node record was created.
    \newline
    The attribute is of type \texttt{timestamptz}.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the node record was updated.
    \newline
    The attribute is of type \texttt{timestamptz}.
\end{itemize}

\subsubsection{Cpu}
\label{subsubsec:implementation_server_database_cpu}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_cpu.pdf}
\end{figure}

The cpu entity contains information about the CPU (Central Processing Unit).
\newline
The entity includes the most important information about a particular CPU model obtained
from the Linux Kernel data available at the virtual path \texttt{/proc/cpuinfo}\footnote{\url{https://www.kernel.org/doc/html/latest/filesystems/proc.html\#kernel-data}}.
\newline
Because a particular CPU model can be present in numerous nodes (as is the case with
worker nodes in reCluster), it is checked before registering a node that the CPU
is already listed in the database. If not, it is registered using the provided information;
otherwise, the information is combined with certain attributes to minimize potential
deviation errors (i.e. \texttt{benchmarks}) or to update the attribute with the most
recent known information (i.e. \texttt{vulnerabilities}).
\newline
As stated in the node entity, modern Processors, such as Arm Big.LITTLE\footnote{\url{https://www.arm.com/technologies/big-little}}
or Intel Performance Hybrid Architecture\footnote{\url{https://www.intel.com/content/www/us/en/gaming/resources/how-hybrid-design-works.html}},
are beginning to include hybrid technology (heterogeneous processing
architecture) which employs two kinds of processor. The first type of processor
is intended for maximum power efficiency, which reduces overall performance,
whereas the second type of processor is designed for maximum compute performance,
which increases power consumption. This form of Processor can instantly adapt to
the variable workload by switching from one type to the other\cite{cpu_arm}\cite{cpu_intel}.
Even though the latter technology is new and difficult to monitor (understanding
when the CPU decides to transition from one type to the other and vice versa),
it is represented in the entity with two dedicated attributes (\texttt{efficiency\_threshold}
and \texttt{performance\_threshold}).
\newline
The \texttt{cpu} entity attributes are as follows:

\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) a CPU record.
    \newline
    The attribute is of the \texttt{UUID} type.
    \newline
    Because multiple nodes can share the same CPU type, the id attribute is associated
    with the node entity. As a result, the relation is one (\texttt{1}) to many
    (\texttt{*}).

  \item \texttt{name}
    \newline
    CPU name.
    \newline
    The Processor's common name, including its project name.
    \newline
    The \texttt{name} attribute, like some of the previous attributes, can be
    used by users and administrators to easily and uniquely (\texttt{UNIQUE}) identify
    the processor without relying on one or more complicated alphanumerical attribute(s).
    \newline
    This attribute, along with the \texttt{vendor}, \texttt{family}, and \texttt{model}
    attributes, specifically defines a CPU record (\texttt{UNIQUE(name,vendor,family,model)}).
    The quadruplet is used during the node registration procedure to uniquely identify
    if the same CPU has already been registered. It should be noted that if a
    CPU has the same \texttt{name} attribute value but a different value for the
    other attributes, registration fails because something in the received data
    is incorrect.
    \newline
    An example \texttt{name} attribute value is \texttt{Intel(R) Core(TM) i7-6700HQ
    CPU @ 2.60GHz}, which identifies the model \texttt{i7-6700HQ} with a basic
    frequency of \texttt{2.60GHz} manufactured by \texttt{Intel Corporation}\footnote{\url{www.intel.com}}.

  \item \texttt{vendor}
    \newline
    CPU vendor identifier.
    \newline
    Identifies the Processor manufacturer identifier.
    \newline
    The attribute is of type \texttt{cpu\_vendor} enum. It was decided to use a
    preset set of values because almost every desktop CPU processor is
    manufactured by a very limited number of vendors, and knowing the possible values
    ahead of time aids in the development of a more powerful and robust GraphQL API.
    Nonetheless, if a CPU has a manufacturer that is not available, an \texttt{UNKNOWN}
    value exists, enabling future implementations to easily add and support more
    manufacturers.
    \newline
    The \texttt{cpu\_vendor} enum has the following possible values/constants:
    \begin{itemize}
      \item \texttt{AMD}
        \newline
        Advanced Micro Devices\footnote{\url{https://www.amd.com}} (AMD).

      \item \texttt{INTEL}
        \newline
        Intel Corporation.

      \item \texttt{UNKNOWN}
        \newline
        Identifies an unknown manufacturer.
    \end{itemize}
    Due to historical reasons, the original manufacturer identifier string value
    read from \texttt{/proc/cpuinfo} is slightly different\footnote{\url{https://wikipedia.org/wiki/CPUID}}:
    \texttt{INTEL} is \texttt{GenuineIntel} and \texttt{AMD} is \texttt{AuthenticAMD}.
    They have been changed/standardized to be more easily identified, with a shorter
    name as well.

  \item \texttt{family}
    \newline
    CPU family.
    \newline
    Identifies the Processor's microarchitectural lineage.
    \newline
    Following the example CPU in the \texttt{name} attribute, it has a family
    value of \texttt{6}\footnote{\url{https://wikipedia.org/wiki/P6_(microarchitecture)}},
    indicating that it is a descendant of the Pentium Pro\footnote{\url{https://wikipedia.org/wiki/Pentium_Pro}}
    and thus uses the same microarchitecture.

  \item \texttt{model}
    \newline
    CPU model.
    \newline
    Identifies the model of the Processor as specified by the manufacturer.
    \newline
    It should be noted that the unique quadruplet (\texttt{UNIQUE(name,vendor,family,model)})
    must include the \texttt{name} attribute because the other three attributes (\texttt{vendor},
    \texttt{family}, \texttt{model}) distinguish only a collection of processors
    rather than a single specific one.
    \newline
    Following the example CPU in the \texttt{name} attribute, it has a model value
    of \texttt{94} indicating that it is a \texttt{Skylake}\footnote{\url{https://www.intel.com/content/www/us/en/products/platforms/details/skylake-s.html}}
    generation (model) Processor.

  \item \texttt{architecture}
    \newline
    CPU architecture.
    \newline
    Identifies the Processor Instruction Set Architecture implementation\footnote{\url{https://wikipedia.org/wiki/Instruction_set_architecture}}.
    \newline
    The attribute is of type \texttt{cpu\_architecture} enum. The possible
    values reflect the CPU architectures supported by the cluster. The latter is
    critical because some components only support specific architectures, and
    downloaded dependencies ignore files that are not present in the
    configuration (commonly unsupported architectures). It should be noted that the
    installation script detects if it is operating on an unsupported architecture.
    Nonetheless, the verification occurs also in the GraphQL API during the registration
    process.
    \newline
    The \texttt{cpu\_architecture} enum has the following possible values/constants:
    \begin{itemize}
      \item \texttt{AMD64}\footnote{\url{https://wikipedia.org/wiki/X86-64}}
        \newline
        \texttt{64-bit} version of the \texttt{x86}\footnote{\url{https://wikipedia.org/wiki/X86}}
        instruction set.
        \newline
        This architecture is also known as \texttt{x64}, \texttt{x86\_64}, \texttt{x86-64},
        and \texttt{Intel 64}.

      \item \texttt{ARM64}\footnote{\url{https://wikipedia.org/wiki/AArch64}}
        \newline
        \texttt{64-bit} extension of the ARM architecture family\footnote{\url{https://wikipedia.org/wiki/ARM_architecture_family}}.
        \newline
        This architecture is also known as \texttt{AArch64}.
    \end{itemize}
    It should be noted that the names of the enum values are subjective, and thus
    they can be (easily) changed to better fit the organization administering the
    cluster's favored choice.

  \item \texttt{flags}
    \newline
    CPU feature flags.
    \newline
    Determines the features that the Processor implements (identified by a
    distinct flag name). One example is the \texttt{fpu}\footnote{\url{https://wikipedia.org/wiki/Floating-point_unit}}
    (floating-point unit) flag, which indicates that the CPU has a dedicated
    coprocessor for working with floating-point numbers.
    \newline
    The attribute is of the type array (\texttt{[]}) of text, and it contains
    all of the feature flags. The latter enables a robust and complex GraphQL API
    for filtering and searching particular CPUs with a specified set of flags.
    \newline
    An application can be built to support a particular set of feature flags (normally
    improving overall performance thanks to compiler optimizations) and then
    scheduled only on worker nodes whose Processor implements these flags. The selection
    of nodes is based solely on the node(s) identification number (\texttt{id}
    attribute) that is available in the Kubernetes environment under the label \texttt{recluster.io/id=<ID>}.

  \item \texttt{cores}
    \newline
    CPU core count.
    \newline
    Almost every modern CPU is a multi-core processor\footnote{\url{https://wikipedia.org/wiki/Multi-core_processor}},
    which combines multiple processing units (two or more) to enable parallel
    work. This attribute indicates how many core units the CPU has.
    \newline
    It should be noted that this value takes into consideration the CPU's
    multithreading\footnote{\url{https://wikipedia.org/wiki/Multithreading_(computer_architecture)}}
    capability, so it is common for this value to be twice the real physical number
    of cores.
    \newline
    Following the example CPU in the \texttt{name} attribute, the attribute has
    a value of \texttt{8}, even though the actual physical cores are \texttt{4}.

  \item \texttt{cache\_l1d}
    \newline
    CPU \texttt{L1d} (data) cache\footnote{\url{https://wikipedia.org/wiki/CPU_cache}}
    size in bytes (\texttt{B}).
    \newline
    Because of their small sizes, all cache attributes are of the type \texttt{integer}
    rather than \texttt{bigint}.

  \item \texttt{cache\_l1i}
    \newline
    CPU \texttt{L1i} (instructions) cache size in bytes (\texttt{B}).

  \item \texttt{cache\_l2}
    \newline
    CPU \texttt{L2} cache size in bytes (\texttt{B}).

  \item \texttt{cache\_l3}
    \newline
    CPU \texttt{L3} cache size in bytes (\texttt{B}).

  \item \texttt{vulnerabilities}
    \newline
    Known CPU vulnerabilities.
    \newline
    A collection of known vulnerabilities that affect the Processor.
    \newline
    Because the Processor is vulnerable to numerous vulnerabilities, the attribute's
    type is an array (\texttt{[]}) of \texttt{text}. If the CPU is already registered,
    the previous value of the \texttt{vulnerabilities} attribute is combined with
    the new value obtained during the registration procedure, with the result
    that if newer vulnerabilities are discovered, the database and server components
    are immediately updated with the information. Furthermore, because it is an array
    type, the GraphQL API allows advanced filtering and searching.
    \newline
    The vulnerabilities attribute can be used to evaluate applications on known vulnerable
    dedicated nodes. The latter is particularly helpful in an offensive security
    class, where students can experiment with various vulnerabilities to take control
    of the node or execute arbitrary code.

  \item \texttt{single\_thread\_score}
    \newline
    CPU single-thread score.
    \newline
    This attribute indicates the performance score of a single CPU core.
    \newline
    It should be noted that the evaluation, as well as the final score for this attribute
    and \texttt{multi\_thread\_score} attribute, are carried out using the sysbench\footnote{\url{https://github.com/akopytov/sysbench}}
    application (see section
    \ref{subsec:implementation_installer_node_benchmarks})

  \item \texttt{multi\_thread\_score}
    \newline
    CPU multi-thread score.
    \newline
    This attribute indicates the performance score of all CPU cores combined.
    \newline
    It should be noted that hyperthreading capability is taken into
    consideration, and the tests are performed on highly parallelizable algorithms.

  \item \texttt{efficiency\_threshold}
    \newline
    CPU power-efficiency threshold (inclusive).
    \newline
    The workload threshold value (upper) beyond which it is no longer certain that
    the Processor will stay in a power-efficient condition.
    \newline
    The threshold value is a number between \texttt{1} (inclusive) and \texttt{99}
    (inclusive), where \texttt{1} indicates that the CPU is executing at \texttt{1\%}
    of its total capacity (almost idle) and \texttt{99} indicates that the CPU is
    operating at \texttt{99\%} of its total capacity (full load).
    \newline
    Due to the difficulty of monitoring the threshold value, the attribute allows
    a \texttt{NULL} value (\texttt{NULLABLE}).

  \item \texttt{performance\_threshold}
    \newline
    CPU performance threshold (inclusive).
    \newline
    The workload threshold value (lower) beyond which it is no longer certain that
    the Processor will stay in a performance condition.
    \newline
    By combining the attributes \texttt{efficiency\_threshold} and \texttt{performance\_threshold},
    a workload range is obtained that indicates an area of uncertainty where the
    Processor can transition from one state to the other or remain in the current
    one. For example, if a CPU has the attribute \texttt{efficiency\_threshold}
    set to \texttt{45} and the attribute \texttt{performance\_threshold} set to
    \texttt{55}, then the workload range between \texttt{45} and \texttt{55} (\texttt{10}
    workload range) is uncertain, and the Processor can transition or not.
    \newline
    Due to the difficulty of monitoring the threshold value, the attribute
    allows a \texttt{NULL} value (\texttt{NULLABLE}).

  \item \texttt{created\_at}
    \newline
    Date and time of creation.
    \newline
    The attribute indicates when the CPU record was created.
    \newline
    The attribute is of the \texttt{timestamptz} type.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the CPU record was updated.
    \newline
    The attribute is of the \texttt{timestamptz} type.
\end{itemize}

\subsubsection{Storage}
\label{subsubsec:implementation_server_database_storage}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_storage.pdf}
\end{figure}

The storage entity represents the persistence storage(s) accessible on the
associated node.
\newline
The information is obtained by reading the block devices identified by the Linux
kernel and made available via the virtual path \texttt{/sys/block}\footnote{\url{https://www.kernel.org/doc/html/next/filesystems/sysfs.html}}.
Block devices, such as a Hard Disk, SSD, CD-ROM, and RAM disk, are distinguished
by random access to data organized in fixed-size blocks\cite{block_device}. Because
the implementation only requires persistent storage(s), the collection of block devices
is filtered to retrieve only Disk(s)/SSD(s) block devices that are not removable
devices\footnote{\url{https://wikipedia.org/wiki/Removable_media}} (i.e. USB flash
drive). The latter is done automatically by the installation script while reading
the node information.
\newline
A node can have zero or multiple storage devices. Nonetheless, it is usual for a
consumer system to include at least one storage device, and modern (consumer) systems
may include a combination of a Hard Disk (cheap with large capacity but slow) and
an SSD (expensive but faster than a Hard Disk). Using a multi-SSD configuration
can significantly improve cluster performance. Furthermore, if certain nodes
have three or more storage devices, Longhorn (see section
\ref{subsubsec:implementation_dependencies_docker_registry_image_storage}) can utilize
them to create a distributed block storage system.
\newline
Even though \texttt{Sysbench} (see section
\ref{subsec:implementation_installer_node_benchmarks}) supports it, the
application currently does not support a scoring system for a storage device
that can help determine its performance under various conditions. The \texttt{storage}
entity attributes are as follows:

\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) a storage record.
    \newline
    The attribute is of type \texttt{UUID}.

  \item \texttt{node\_id}
    \newline
    Node identifier.
    \newline
    Uniquely identify a node record (\texttt{FK}).
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    A node can have multiple storage devices, but each storage device is
    associated with one unique node. As a result, the relation is many (\texttt{*})
    to one (\texttt{1}).

  \item \texttt{name}
    \newline
    Storage name.
    \newline
    The name of the storage device on the associated node. In GNU/Linux, typical
    storage device names are \texttt{sda}, \texttt{sdb}, \texttt{sdc}, and so on
    for Hard Disks/SSDs and \texttt{nvme0}, \texttt{nvme1}, \texttt{nvme2}, and so
    on for NVMEs\footnote{\url{https://wikipedia.org/wiki/NVM_Express}}.
    Nonetheless, the organization in charge of the cluster can choose to modify the
    default name by using arbitrary names to better identify the storage device.
    \newline
    Despite its appearance as a helper for users or administrators, this attribute
    is also used in conjunction with the \texttt{node\_id} attribute to uniquely
    (\texttt{UNIQUE(node\_id,name)}) identify the storage device on the node and
    to prevent the registration of two devices with the same name on the same
    node.

  \item \texttt{size}
    \newline
    Storage memory size in bytes (\texttt{B}).
    \newline
    The total quantity of memory that the storage device can store.
    \newline
    Because the overall memory of common storage devices is quite large and their
    counterpart in bytes is quite massive, the attribute is of type \texttt{bigint}.

  \item \texttt{created\_at}
    \newline
    Date and time of creation.
    \newline
    The attribute indicates when the storage record was created.
    \newline
    The attribute is of type \texttt{timestamptz}.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the storage record was updated.
    \newline
    The attribute is of type \texttt{timestamptz}.
\end{itemize}

\subsubsection{Interface}
\label{subsubsec:implementation_server_database_interface}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_interface.pdf}
\end{figure}

The interface entity depicts the network interface(s) accessible within the
cluster's Internal Network that are associated with the node.
\newline
The \texttt{ip}\footnote{\url{https://linux.die.net/man/8/ip}} command/application
is used to obtain a node's available network interfaces (\texttt{ip -details
link show}). However, the returned list contains both logical interfaces (such as
\texttt{lo}\footnote{\url{https://tldp.org/LDP/nag/node66.html}}) and physical interfaces
(such as \texttt{eth0} or \texttt{wlan0}\footnote{\url{https://wiki.debian.org/NetworkInterfaceNames}}),
and must be filtered to retrieve only the physical one. The latter action is performed
automatically by the installation script that discards all network interfaces that
have the \texttt{info\_kind} information object and is of type \texttt{loopback}.
\newline
A physical interface is a Network Interface Card (NIC) that is installed on the
node and can thus be used to acquire information about the NIC as well as be
configured to modify some key parameters (i.e. Wake-on-Lan flags) to function properly
in the cluster environment. The information obtained is critical for
understanding which nodes are available for automated bootstrapping by the
Server (see section \ref{subsec:implementation_server_upscaling}) and to which
specific physical address (MAC address) the magic wake-up message must be sent.
\newline
A node must have at least one NIC to communicate with/from it. The latter
verification is done by both the installation script and the Server component
when the registration query is executed. Nonetheless, (almost) every system is
equipped with at least one NIC and, if possible/needed, can have also extra NICs.
It is standard practice in data centers and almost every enterprise environment to
have a system with multiple NICs.
\newline
The \texttt{interface} entity attributes are as follows:

\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) an interface record.
    \newline
    The attribute is of type \texttt{UUID}.

  \item \texttt{node\_id}
    \newline
    Node identifier.
    \newline
    Uniquely identify a node record (\texttt{FK}).
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    A node can have multiple network interfaces, but each network interface is
    associated with a single and unique node. As a result, the relation is many
    (\texttt{*}) to one (\texttt{1}).

  \item \texttt{name}
    \newline
    Interface name.
    \newline
    The network interface name on the associated node. In GNU/Linux, common network
    interface names for wired network interfaces are \texttt{eth0}, \texttt{eth1},
    and so on, and for wireless network interfaces are \texttt{wlan0}, \texttt{wlan1},
    and so on. The network interface name, like block devices, can be changed by
    the organization in control of the cluster from the one automatically assigned
    by the Kernel. The latter procedure, while supported, is strongly
    discouraged to prevent potential confusion. Furthermore, modifying it
    necessitates reconfiguring and restarting all applications/programs that are
    configured to interact with the particular interface (i.e. \texttt{SSH}).
    \newline
    The interface \texttt{name} attribute is used in conjunction with the
    \texttt{node\_id} attribute to uniquely identify (\texttt{UNIQUE(node\_id,name)})
    an interface record and to avoid two interfaces with the same name from being
    registered for the same node. Moreover, identifying a NIC by its name for a related
    node is also helpful to users and administrators.

  \item \texttt{address}
    \newline
    Interface MAC address.
    \newline
    The MAC address assigned to the NIC and used for Internal Network communication
    (see section \ref{subsec:architecture_network_internal_network}). There must
    be no interfaces in the Internal Network with the same MAC address. The latter
    is reflected in the database schema, which states that the attribute value
    must be \texttt{UNIQUE}, implying that no two interfaces can have the same
    MAC address. This attribute can be used to uniquely identify an interface record
    as a result of the latter, and it is known to be particularly helpful when
    dealing with networking problems in understanding from which node certain
    packets are sent/lost in the cluster.
    \newline
    Each database implementation, like the \texttt{address} attribute in the node
    entity, has its own specific/custom datatype for encoding a MAC address, therefore
    the attribute type is defined as plain text.
    \newline
    Each database implementation, like the address attribute in the node entity,
    has its own specific/custom datatype for encoding a MAC address, therefore the
    attribute type is defined to plain text.
    \newline
    It should be noted that the NIC's MAC address is assigned by the
    corresponding manufacturer. Nonetheless, there are software tools, such as \texttt{macchanger}\footnote{\url{https://www.kali.org/tools/macchanger}},
    that enable the MAC address to be arbitrarily modified with any (valid) value.

  \item \texttt{speed}
    \newline
    Interface speed in bit-per-second (\texttt{b/s}).
    \newline
    The NIC's speed/performance in bit-per-second. The value is read with the \texttt{ethtool}
    (see section \ref{subsec:implementation_distributions_packages}) program by
    specifying the network interface name and is thus used in conjunction with
    the \texttt{ip} program output. Because the obtained value/unit is not in
    \texttt{b/s} but in a base 10 multiple unit, such as \texttt{100 Mb/s} or\texttt{
    1 Gb/s} (common speed values/units for consumer NICs), the installation script
    converts to \texttt{b/s} using the \texttt{numfmt}\footnote{\url{https://www.gnu.org/software/coreutils/manual/html_node/numfmt-invocation.html}}
    application, which understands the unit and applies the corresponding
    multiplication value.
    \newline
    Although some speed values of consumer NICs can fit in an \texttt{integer}
    type, this is not always the case (for example, enterprise NICs, which can
    easily reach \texttt{10 Gb/s} and beyond), therefore the attribute's type has
    been set to \texttt{bigint}.

  \item \texttt{wol}
    \newline
    Interface Wake-on-Lan (\texttt{WoL}) flags supported.
    \newline
    An interface can be triggered to bootstrap the node by multiple distinct
    \texttt{WoL} activities and this attribute determines which \texttt{WoL}
    flags (each flag mapping to a particular trigger) are supported by the
    interface.
    \newline
    The \texttt{wol} attribute is an array (\texttt{[]}) of \texttt{wol\_flag} enum
    because an interface can accept different \texttt{WoL} triggers/flags. The array
    is empty if an interface does not support \texttt{WoL}. The latter is reflected
    in the schema by the default value (\texttt{DEFAULT([])}) provided by the
    database to the attribute.
    \newline
    The \texttt{wol} attribute is intrinsically tied to the \texttt{address} attribute
    because both are used during the Server's (up)scaling procedure (see section
    \ref{subsec:implementation_server_upscaling}). To begin, the \texttt{wol} attribute
    is used to filter inactive nodes to obtain only a list of nodes that can be automatically
    bootstrapped using \texttt{WoL} (search where \texttt{wol} is not an empty
    array). Then, for each node that needs to be bootstrapped (select \texttt{n}
    nodes from the list), the associated MAC address of the interface is used as
    the destination address of the \texttt{WoL} special message. It should be noted
    that the current implementation only allows basic \texttt{WoL}, even though some
    nodes in the cluster may support advanced \texttt{WoL} configurations. Nevertheless,
    future implementations may support more sophisticated logic and
    deterministically apply specific configurations based on the interface's
    supported \texttt{WoL} flags.
    \newline
    The node \texttt{wol\_flag} enum has the following possible values/constants:
    \begin{itemize}
      \item \texttt{a}
        \newline
        Wake on \texttt{ARP} activity.

      \item \texttt{b}
        \newline
        Wake on \texttt{Broadcast} activity.

      \item \texttt{g}
        \newline
        Wake on \texttt{MagicPacket} activity.

      \item \texttt{m}
        \newline
        Wake on \texttt{Multicast} activity.

      \item \texttt{p}
        \newline
        Wake on \texttt{PHY} (Physical) activity.

      \item \texttt{s}
        \newline
        Enable \texttt{SecureOn} password for \texttt{MagicPacket}.

      \item \texttt{u}
        \newline
        Wake on \texttt{Unicast} activity.
    \end{itemize}
    It is worth noting that there is an additional possible value for the
    \texttt{WoL} flags, which is the value \texttt{d}, which indicates that
    \texttt{WoL} is disabled on the interface. Because the installation script
    checks to determine whether or not all \texttt{WoL} capable NICs have \texttt{WoL}
    enabled (if not, an error is generated), the \texttt{d} value cannot be accepted
    by the GraphQL API and is thus omitted.
    \newline
    The \texttt{g} flag is the basic configuration for a \texttt{WoL} interface,
    allowing the node to be automatically bootstrapped with \texttt{WoL}. As previously
    stated, the latter is the current (only) supported configuration.

  \item \texttt{created\_at}
    \newline
    Date and time of creation.
    \newline
    The attribute indicates when the interface record was created.
    \newline
    The attribute is of type \texttt{timestamptz}.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the interface record was updated.
    \newline
    The attribute is of type \texttt{timestamptz}.
\end{itemize}

\subsubsection{Status}
\label{subsubsec:implementation_server_database_status}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_status.pdf}
\end{figure}

The status entity, as the name implies, represents the node's current status, as
updated with the most recently available information.
\newline
The status entity's information is critical for gaining a better understanding
of the node, its condition, and if any potential problems, such as a crash or an
anomaly/unprogrammed shutdown, arise. Furthermore, it is widely used in
autoscaling procedures (see section \ref{subsec:implementation_server_upscaling}
and section \ref{subsec:implementation_server_downscaling}). When upscaling, the
Server determines what are the list of nodes that are in an inactive status and,
vice versa, when downscaling, the Server determines what are the list of nodes that
are in an active status. It should be emphasized that the status entity is
composed of multiple attributes, not just one, each of which specifies a
particular knowledge that can also be correlated to other attributes.
\newline
A node must have only one (unique) status record, and a status record must
identify only one (unique) node record. When a node successfully registers, the
status record is automatically created, and it is continuously updated while the
node is operational. Because there are no status information updates sent by the
node when it is inactive, all of the attribute's values reflect the most recent status
information before the node is shut down.
\newline
The status information is automatically updated by leveraging the Kubernetes API\footnote{\url{https://kubernetes.io/docs/concepts/architecture/nodes/\#node-status}},
which provides access to the node's status information, which is constantly
updated by periodic Heartbeats\footnote{\url{https://kubernetes.io/docs/concepts/architecture/nodes/\#heartbeats}}
sent by the Kubernetes node to the API Server (Controllers). The information (attributes)
of the status entity is only a subset of the multitude made available by the Kubernetes
API, as most of them are unnecessary for low-level cluster operation and
administration. Furthermore, some status updates are performed without
leveraging the Kubernetes API, either because the latter is unavailable or because
the node autonomously sends a request (i.e. after successfully bootstrapping and
before starting all cluster services) to specific protected GraphQL queries, generating
intrinsic knowledge that the node is somewhat active and performing a status update.
\newline
It should be noted that this entity lacks the \texttt{created\_at} attribute because
it is created automatically when the associated node is created, and thus the
two timestamps will have the same value.
\newline
The \texttt{status} entity attributes are as follows:

\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) a status record.
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    A node must have only one status, and a status must be assigned to only one/unique
    node. As a result, the relation is one (\texttt{1}) to one (\texttt{1}).
    \newline
    The \texttt{id} attribute is also the Node identifier, which allows a node
    record to be uniquely identified (\texttt{FK}). The \texttt{id} and node identifier
    (\texttt{node\_id}) attributes are separated in the previous entities to
    represent the fact that the corresponding node can be associated with
    multiple records of the entity. In the status entity, however, the \texttt{id}
    attribute serves as both the primary key (\texttt{PK}) and the foreign key (\texttt{FK})
    to a distinct node record. The latter ensures that a status record is unique,
    easy to identify, and related to a single/unique node record automatically at
    the database schema level (without the need for Server checks). When a
    status record is created (during the installation process), its \texttt{id}
    is not derived from a default value (in fact, the \texttt{DEFAULT(UUID)}
    metadata is missing), but it is the same as the associated node. The latter is
    also very useful for preventing complex and expensive joins between the two entities
    because identifying a node's status record requires only its id because they
    are the same value.

  \item \texttt{status}
    \newline
    Node status.
    \newline
    Identify the node's high-level status.
    \newline
    It is one of the most essential data that correlates to a node, and it is continuously
    updated when the node is powered on. It can be used by users/administrators
    to gain a better understanding of the node's current state, such as determining
    whether or not the node is properly working/healthy. It is widely used in
    autoscaling procedures in conjunction with other attributes to identify
    which nodes are inactive (upscaling) or active (downscaling).
    \newline
    The attribute is of type \texttt{node\_status} enum. Understanding each status
    is critical, and the general perspective resembles a finite-state machine\footnote{\url{https://wikipedia.org/wiki/Finite-state_machine}}
    machine. Some statuses are only used to transition from one to the next,
    whereas others designate a permanent status that varies only in response to
    specific actions.
    \newline
    The \texttt{node\_status} enum has the following possible values/constants:
    \begin{itemize}
      \item \texttt{ACTIVE}
        \newline
        Node is active and healthy, with no Kubernetes orchestrator operating.
        \newline
        It is a transitory status that is assigned in one of the two situations
        listed below:
        \begin{enumerate}
          \item \texttt{BOOTING} \faicon{arrow-circle-o-right} \texttt{ACTIVE}
            \faicon{arrow-circle-o-right} \texttt{ACTIVE\_NOT\_READY}
            \newline
            The node has successfully booted (\texttt{BOOTING}) and has sent an update
            status request to the Server's protected GraphQL API (\texttt{ACTIVE}).
            After receiving a successful response from the Server, the Node
            begins to initialize all necessary cluster services, such as Node Exporter
            and K3s, by leveraging the corresponding installed Init System (\texttt{ACTIVE\_NOT\_READY}).

          \item \texttt{ACTIVE\_DELETING} \faicon{arrow-circle-o-right} \texttt{ACTIVE}
            \faicon{arrow-circle-o-right} \texttt{INACTIVE}
            \newline
            The node has been successfully removed (\texttt{ACTIVE\_DELETING}) from
            the Kubernetes cluster (\texttt{ACTIVE}), and the Server begins the shutdown
            procedure (\texttt{INACTIVE}).
        \end{enumerate}

      \item \texttt{ACTIVE\_READY}
        \newline
        Node is active, healthy and the Kubernetes orchestrator is operating and
        accepting pods.
        \newline
        It is a permanent status that is constantly monitored by the Heartbeat
        system.
        \newline
        The status is assigned in the following situation:
        \begin{enumerate}
          \item \texttt{ACTIVE\_NOT\_READY} \faicon{arrow-circle-o-right} \texttt{ACTIVE\_READY}
            \newline
            The Kubernetes orchestrator on the node (\texttt{ACTIVE\_NOT\_ READY})
            has finished the initialization and is ready to accept workloads (\texttt{ACTIVE\_READY}).
        \end{enumerate}
        The status is unassigned in the following situations:
        \begin{enumerate}
          \item \texttt{ACTIVE\_READY} \faicon{arrow-circle-o-right} \texttt{ACTIVE\_DELETING}
            \newline
            The node (\texttt{ACTIVE\_READY}) should be turned off (\texttt{ACTIVE\_DELETING})
            because the Cluster Autoscaler has monitored it and determined that
            its overall workload is less than the predefined threshold, or because
            an administrator has sent a request to the GraphQL API.

          \item \texttt{ACTIVE\_READY} \faicon{arrow-circle-o-right} \texttt{ACTIVE\_NOT\_READY}
            \newline
            The node has not sent any Heartbeat messages for more than the
            predefined duration threshold value. When the threshold time value is
            reached, the node status changes from \texttt{ACTIVE\_READY} to \texttt{ACTIVE\_NOT\_READY},
            and Kubernetes stops scheduling workloads (pods) on the node.
        \end{enumerate}

      \item \texttt{ACTIVE\_NOT\_READY}
        \newline
        Node is active, healthy and the Kubernetes orchestrator is initializing (it
        cannot accept pods).
        \newline
        It is a transitory status that is assigned in the following situations:
        \begin{enumerate}
          \item \texttt{ACTIVE} \faicon{arrow-circle-o-right} \texttt{ACTIVE\_NOT\_READY}
            \faicon{arrow-circle-o-right} \texttt{ACTIVE\_READY}
            \newline
            The node has received a successful response from the Server (\texttt{ACTIVE})
            and is starting all cluster-related services via the node's corresponding
            Init System. When most services start, they are still in a transitory
            phase known as initialization, in which they are not immediately
            ready to accomplish their primary purposes (\texttt{ACTIVE\_NOT\_
            READY}). After initialization, the services are available to be
            utilized and fulfill their purposes (\texttt{ACTIVE\_READY}).
            \newline
            For example, the Kubernetes orchestrator takes nearly \texttt{15} to
            \texttt{30} seconds to initialize, and during this time it is unable
            to receive any workload from the Controller nodes because all of its
            components are also initializing.

          \item \texttt{ACTIVE\_READY} \faicon{arrow-circle-o-right} \texttt{ACTIVE\_NOT\_READY}
            \faicon{arrow-circle-o-right} \texttt{ACTIVE\_READY}
            \newline
            Because the node has not sent any Heartbeat messages for more than
            the threshold time value, its status is changed from \texttt{ACTIVE\_READY},
            indicating that the node is working and accepting pods, to \texttt{ACTIVE\_NOT\_READY},
            indicating that the node is not working/unknown, and Controller nodes
            cease scheduling pods to the node. If the node resumes sending
            Heartbeat messages periodically, the status returns to \texttt{ACTIVE\_READY}.
            Otherwise, if the node's most recent Heartbeat message has been
            received for more than a second (longer) threshold, the node status changes
            to \texttt{UNKNOWN} with the attributes \texttt{reason} and \texttt{message}
            properly set, indicating that the node is unreachable and thus in error.
        \end{enumerate}

      \item \texttt{ACTIVE\_DELETING}
        \newline
        Node is active and healthy and it is being removed from the Kubernetes
        cluster.
        \newline
        It is a transitory status that is assigned in the following situation:
        \begin{enumerate}
          \item \texttt{ACTIVE\_READY} \faicon{arrow-circle-o-right} \texttt{ACTIVE\_DELETING}
            \faicon{arrow-circle-o-right} \texttt{ACTIVE}
            \newline
            After receiving the request to power off the node, the Server instructs
            the Kubernetes cluster to drain\footnote{\url{https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node}}
            the node (safely remove/evict all pods from the node) so that it can
            be successfully removed from the Kubernetes cluster (\texttt{ACTIVE\_DELETING}).
            After successfully removing the node from the Kubernetes cluster,
            the associated Kubernetes operator on the deleted node is terminated
            (\texttt{ACTIVE}).
            \newline
            Since this status is executed because the node needs to be turned
            off, the Server automatically shuts down the node after the node status
            is changed to \texttt{ACTIVE}. The latter is to indicate that if the
            \texttt{ACTIVE} status is assigned after this status, it will only be
            effective for a very short time before the node becomes \texttt{INACTIVE}.
        \end{enumerate}

      \item \texttt{BOOTING}
        \newline
        Node is booting.
        \newline
        It is a transitory status that is assigned in the following situation:
        \begin{enumerate}
          \item \texttt{INACTIVE} \faicon{arrow-circle-o-right} \texttt{BOOTING}
            \faicon{arrow-circle-o-right} \texttt{ACTIVE}
            \newline
            The Server has received a GraphQL request from the Cluster
            Autoscaler or an administrator to turn on a server in the cluster (\texttt{INACTIVE}).
            The Server then sends the associated \texttt{Wake-on-Lan} magic message
            to the corresponding \texttt{MAC} address of the node's interface to
            automatically boot it up (\texttt{BOOTING}). After the booting
            period, which differs depending on the hardware of the node, the
            node sends the previously mentioned update status message, which changes
            its status to \texttt{ACTIVE}.
            \newline
            It should be noted that if a node is powered on manually by a user or
            an administrator using the power button, the \texttt{BOOTING} status
            is omitted in the transition because the Server has no awareness of the
            action until the node sends the update status request message transitioning
            from an \texttt{INACTIVE} to \texttt{ACTIVE} status.
        \end{enumerate}

      \item \texttt{INACTIVE}
        \newline
        Node is inactive (powered off).
        \newline
        It is a permanent status.
        \newline
        The status is assigned in the following situation:
        \begin{enumerate}
          \item \texttt{ACTIVE} \faicon{arrow-circle-o-right} \texttt{INACTIVE}
            \newline
            The node is in \texttt{ACTIVE} status, and the Server terminated it
            remotely via SSH (\texttt{INACTIVE}).
            \newline
            It should be noted that the latter can also be executed manually by
            a user/administrator using the power button, but the Server does not
            know anything about the current status because there is no predefined
            transition between the various statuses, and thus, after a period of
            time is set to an error status (\texttt{UNKNOWN}).
        \end{enumerate}
        The status is unassigned in the following situation:
        \begin{enumerate}
          \item \texttt{INACTIVE} \faicon{arrow-circle-o-right} \texttt{BOOTING}
            \newline
            The node is in the \texttt{INACTIVE} status until the Server (via
            \texttt{Wake-on-Lan}) or a user/administrator (via the power button)
            turns it on (\texttt{BOOTING}).
            \newline
            As previously stated, if the node is started using the power button,
            the status transitions directly from \texttt{INACTIVE} to \texttt{ACTIVE},
            skipping the \texttt{BOOTING} status because the Server does not
            know anything about the manual operation.
        \end{enumerate}

      \item \texttt{UNKNOWN}
        \newline
        Unknown/Error status.
        \newline
        This status is special in that it identifies a node that is in an error condition
        due to an issue that must be resolved.
        \newline
        All statuses can transition to (error discovered) and from (error resolved)
        this status.
        \newline
        For example, when this status is set for a node, it means that the Kubernetes
        orchestrator has crashed and no Heartbeats have been received by the Controllers
        for a specified amount of time.
    \end{itemize}
    Figure \ref{fig:status_machine} depicts all of the possible statuses and the
    transitions between them. Take note of the manual transition from \texttt{INACTIVE}
    to \texttt{ACTIVE}, as well as the bidirectional transition between \texttt{ACTIVE\_NOT\_READY}
    and \texttt{ACTIVE\_READY}. \par
    \begin{minipage}{\linewidth}
      \centering
      \includegraphics
      [width=.75\textwidth]{images/implementation/status_machine.pdf}
      \label{fig:status_machine} \captionof{figure}{Statuses and their transitions}
    \end{minipage}

  \item \texttt{reason}
    \newline
    Status' reason.
    \newline
    Determine briefly why the \texttt{status} attribute has the corresponding value.
    The \texttt{reason} attribute value is typically one word or a sequence of very
    few words that briefly describe the cause for the node's \texttt{status}
    attribute value. In transitional statuses (i.e. no persistent) that
    transition from one status to another, the value of this attribute can be assigned
    with various and distinct values, defining the logical path to the final
    status. The latter is particularly obvious during the Kubernetes initialization
    (from \texttt{ACTIVE} to \texttt{ACTIVE\_READY}), which continues to change
    the attribute's value to represent the fact that the various Kubernetes
    components are starting.
    \newline
    For example, if the node's status value is \texttt{ACTIVE\_READY} and the Kubernetes
    process is running smoothly, the value of this attribute is set to \texttt{KubeletReady},
    indicating that the Kubernetes \texttt{kubelet}\footnote{\url{https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet}}
    component is functioning properly.
    \newline
    Because the reason why the status has the corresponding value is not always
    known, this attribute can take a \texttt{NULL} value (\texttt{NULLABLE}),
    indicating that the reason is undetermined. This is prevalent if the status value
    is \texttt{UNKNOWN} and the cause of what happened is obscure.

  \item \texttt{message}
    \newline
    Status' message.
    \newline
    Describes why the \texttt{status}/\texttt{reason} attributes have the
    corresponding value using a longer string. This attribute is intrinsically tied
    to the other two attributes; if the value of one of them changes, this attribute
    will probably be updated as well. It can be considered an extended
    description message for the \texttt{reason} value, and it is particularly
    useful for users and administrators who want to understand the current node's
    status. If the node status is set to \texttt{UNKNOWN} and the cause is known,
    the latter can be extremely beneficial: the \texttt{reason} attribute describes
    the issue in brief with a high-level perspective, whereas the \texttt{message}
    attribute is more detailed in explaining the cause of the problem with a low-level
    perspective. As for the \texttt{reason} attribute, the value of the \texttt{message}
    attribute during transitional statuses can vary arbitrarily depending on the
    logical path between the two statuses, whereas it remains nearly constant
    during persistent statuses.
    \newline
    For example, if the node's status is \texttt{ACTIVE\_READY} and the Kubernetes
    process is functioning properly, this attribute's value is set to \texttt{kubelet
    is posting ready status}, indicating that the Kubernetes \texttt{kubelet} is
    accepting pods (working) and sending periodic Heartbeat messages to the corresponding
    Controller node. The combination of the \texttt{reason} and \texttt{message}
    attributes indicate that the Kubernetes node is operating properly and why.
    \newline
    Because the content of the \texttt{message} attribute is not always known, it
    can accept \texttt{NULL} values (\texttt{NULLABLE}). If the value of the
    \texttt{reason} attribute is \texttt{NULL}, it is almost certain that the value
    of the \texttt{message} attribute is also \texttt{NULL}.

  \item \texttt{last\_heartbeat}
    \newline
    Date and time of the most recent Heartbeat message.
    \newline
    The attribute indicates the most recent timestamp at which a Controller node
    received a HeartBeat message sent by the associated node (Kubernetes \texttt{kubelet}).
    The attribute is of type \texttt{timestamptz}.
    \newline
    A HeartBeat message is sent whenever the node's Kubernetes status changes or
    there is no update in the defined HeartBeat interval\cite{k8s_heartbeat}. During
    the initialization phase, Kubernetes bootstraps numerous components, and the
    \texttt{reason} and \texttt{message} attributes are constantly changed,
    resulting in frequent updates to the \texttt{last\_heartbeat} attribute. Far
    from it, when the node is finally ready/working and accepting pods (\texttt{ACTIVE\_READY}
    status), the frequency of Heartbeat messages is much slower, but constant, and
    follows the specified interval, defined as \texttt{node-status-update-frequency}.
    If the node's Heartbeat messages are not received after a second time
    threshold, defined as \texttt{node-monitor-grace-period} and higher than the
    \texttt{node-status-update-frequency}, the node status is changed from
    \texttt{ACTIVE\_READY} to \texttt{ACTIVE\_NOT\_READY}, and the Controller cease
    assigning workload (pods) to the node. If the node becomes reachable again
    and resumes transmitting periodic Heartbeat messages, the node status is immediately
    revert to \texttt{ACTIVE\_READY}, and the Controller node resume scheduling pods
    to the node. If no Heartbeat messages are received after a third time
    threshold, defined as \texttt{pod-eviction-timeout} and higher than \texttt{node-monitor-grace-period},
    the Controller begins evicting\footnote{\url{https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction}}
    all deployments (pods) that are scheduled on the node and the node's status
    is changed to \texttt{UNKNOWN}. Note that the pods scheduled on the node may
    still operate, and because the Controller is unable to communicate with the disconnected
    node, the pods status is also set to \texttt{Terminating} or \texttt{Unknown}\cite{k8s_heartbeat}.
    \newline
    To summarize, the previously mentioned threshold time values are outlined below
    (along with their respective default value):
    \begin{enumerate}
      \item \texttt{node-status-update-frequency}\footnote{\url{https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/\#node-status-update-frequency}}
        \newline
        The frequency at which the kubelet sends node status (heartbeat message)
        to the Controller.
        \newline
        The default value is \texttt{10} seconds (\texttt{10s}).

      \item \texttt{node-monitor-grace-period}\footnote{\url{https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/\#node-monitor-grace-period}}
        \newline
        Amount of time after which an operating node is set to be unresponsive, indicating
        that it is unhealthy. Must be \texttt{N} times greater than the \texttt{node-status-update-frequency}
        of the node's \texttt{kubelet}, where \texttt{N} is the number of retries.
        \newline
        The default value is \texttt{40} seconds (\texttt{40s}).

      \item \texttt{pod-eviction-timeout}\footnote{\url{https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/\#pod-eviction-timeout}}
        \newline
        The duration after which the pods scheduled on the unresponsive node
        begin to be removed.
        \newline
        The default value is \texttt{300} seconds (\texttt{5m}).
    \end{enumerate}
    It should be noted that the latter threshold values can be tailored to
    cluster-specific use-case scenarios, yielding approximately three configurations:
    \texttt{Fast Update/Fast Reaction}, \texttt{Medium Update/Average Reaction} and
    \texttt{Low Update/Slow reaction}\cite{k8s_reliability}.
    \newline
    Because the reception of the most recent Heartbeat message is not always
    known, this attribute can be \texttt{NULL} (\texttt{NULLABLE}). Furthermore,
    the value is \texttt{NULL} when the \texttt{status} is \texttt{INACTIVE}.

  \item \texttt{last\_transition}
    \newline
    Date and time of the most recent status transition.
    \newline
    This attribute indicates the last time the \texttt{status} attribute has
    been changed. It is very helpful for users and administrators to understand how
    long the node has been in the current status. The attribute is of type
    \texttt{timestamptz}.
    \newline
    The value cannot be \texttt{NULL} because the status attribute cannot be \texttt{NULL}
    (and thus it is known when the most recent transition occurs) and also
    because the Server is in charge of updating this attribute. Therefore, the
    Server is the only component with both low-level knowledge (GraphQL API requests
    sent by the node itself) and high-level knowledge (Kubernetes Heartbeats).
    \newline
    For example, if the node's status attribute is set to \texttt{ACTIVE\_READY}
    and the difference between the current timestamp and the value of the last transition
    attribute is \texttt{48} hours, which implies that the node has been participating
    in cluster operations for two consecutive days. The same logic can be
    applied to determine how long the node is powered off (\texttt{INACTIVE}
    status) and thus unused.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the status record was updated.
    \newline
    The attribute is of type \texttt{timestamptz}.
\end{itemize}

\subsubsection{Node Pool}
\label{subsubsec:implementation_server_database_node_pool}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/erm_node_pool.pdf}
\end{figure}

The node pool entity represents a collection of nodes with similar hardware characteristics.
\newline
This entity is critical for autoscaling, and the Cluster Autoscaler component makes
extensive use of the available registered node groups, their respective
information, and the corresponding nodes. Indeed, the Cluster Autoscaler relies on
both high-level knowledge obtained from the Kubernetes API to continuously monitor
the nodes and all scheduled workloads across the cluster, and low-level
knowledge obtained from this entity (via the GraphQL API) to determine whether
there are node pools with inactive nodes that can be booted up (upscaling) or
nodes that can be turned off (downscaling) if some nodes are underutilized.
\newline
The current implementation combines nodes based on their respective number of CPU
cores and memory (RAM) amount, which is also reflected in the \texttt{name} attribute.
It should be noted that in future implementations, the GPU model may also be used.
Because the number of CPU cores is relatively small, dealing with comparisons is
relatively simple. However, for two primary causes, the latter is far from true
for memory quantity. Firstly, the memory quantity unit is in byte (\texttt{B}),
the data type is \texttt{bigint}, and therefore comparisons are performed on
extremely large integers. Secondly, even if two nodes are from the same
manufacturer and have the same hardware components, it is not always guaranteed that
the memory quantity in bytes is identical. The latter has happened in reCluster,
where the four Worker nodes are the same model from the same manufacturer and equipped
with the same hardware, but their memory quantities in bytes vary slightly. One potential
explanation is that some memory cells are faulty (this is more likely with older
hardware, on which reCluster is designed), and the memory controller hides them from
the Operating System, causing the total to be slightly different. To address the
latter issues, the memory amount is first converted to GigaBytes (\texttt{GiB}) and
then rounded to the closest half, yielding \texttt{0.5} or a multiple of it. It
is now simple to organize the nodes in groups. It should be noted that the \texttt{512
MiB} (\texttt{0.5 GiB}) value is not random, but rather reflects the minimal
hardware requirements of the Kubernetes Orchestrator (K3s). As an example,
consider the following four nodes: the first has a \texttt{4} core CPU and
\texttt{4 GiB} of memory, the second has an \texttt{8} core CPU and \texttt{7.5 GiB}
of memory, the third has a \texttt{4} core CPU and \texttt{4 GiB} of memory, and
the fourth has an \texttt{8} core CPU and \texttt{8 GiB} of memory. The first and
third nodes are assigned to the same node pool, whereas the second and fourth
nodes are assigned to separate node pools.
\newline
The System has two logical node pools: one for Controller nodes and one for
Worker nodes. Worker node pools function as previously explained, and they're
employed for autoscaling by default. While all Controller nodes are merged into a
single node pool called \texttt{controllers}, no hardware components are examined,
and auto-scaling is disabled. The latter is done because, by design, the cluster
cannot auto-scale Controller nodes, leaving hardware comparisons meaningless. If
a controller node needs to be converted to a Worker node, it must first be
deleted by the Kubernetes cluster, then removed by the Server/Database (via a
single GraphQL API call made by an administrator), and finally re-registered in
the cluster as a Worker node.
\newline
The node must be assigned to a node pool during the registration process. If the
node pool does not exist, it is created automatically; otherwise, the node is assigned
to the appropriate node pool with the same amount of CPU cores and transformed memory
quantity.
\newline
The node pool entity has the attributes \texttt{min\_nodes} and \texttt{auto\_scale},
which can be modified by an administrator via a GraphQL API node pool update request
to change the associated node pool's autoscaling behavior.
\newline
As previously stated, a node is assigned to a single and unique node pool, but a
node pool can be assigned to zero or multiple nodes.
\newline
The \texttt{node\_pool} entity attributes are as follows:

\begin{itemize}
  \item \texttt{id}
    \newline
    Uniquely identify (\texttt{PK}) a node pool record.
    \newline
    The attribute is of type \texttt{UUID}.
    \newline
    A node must be assigned to a single and unique node pool, and a node pool can
    have zero or more nodes assigned to it. As a result, the relation is one(\texttt{1})
    to many(\texttt{*}).

  \item \texttt{name}
    \newline
    Node pool's name.
    \newline
    A string of characters that uniquely (UNIQUE) identifies a node pool record.
    \newline
    The \texttt{name} and \texttt{id} attributes are conceptually
    interchangeable because they both uniquely identify the same node pool
    record. Furthermore, identical to the user entity's \texttt{username} attribute,
    the \texttt{name} attribute is mostly used for users and administrators to easily
    and quickly identify a node pool record without having to remember a 128-bit
    alphanumerical string (\texttt{UUID}).
    \newline
    When the Server needs to create an additional node pool, it automatically
    assigns a value to the \texttt{name} attribute based on the node type, the
    number of CPU cores, and the transformed memory quantity value. If the node
    pool is dedicated only to Controller nodes, the name is set to \texttt{controllers}
    without any further information (this can be changed in the Server configuration).
    However, the name of a node pool dedicated to Worker node(s) is a combination
    of the two hardware values separated by a dot (\texttt{.}). It should be noted
    that the latter can be easily modified in the Server implementation with different
    values/logic.
    \newline
    For example, \texttt{cpu8.memory7.5} is the name of a Worker nodes pool that
    combines nodes with \texttt{8} CPU cores and \texttt{7.5 GiB} of memory.

  \item \texttt{min\_nodes}
    \newline
    Minimum number of active (\texttt{ACTIVE\_READY} status) nodes in the node pool.
    \newline
    The attribute specifies a minimal number (inclusive) of active nodes with
    the \texttt{ACTIVE\_READY} status in the node pool.
    \newline
    The Cluster Autoscaler uses this information to determine the minimal number
    of nodes in the node pool that must always be available in the Kubernetes
    cluster. Until the number of active nodes in the node pool equals the value of
    the \texttt{min\_nodes} attribute, the Cluster Autoscaler is allowed to downscale
    the nodes in the node pool.
    \newline
    The architecture is particularly focused on minimizing resource loss and
    energy usage. Therefore, to enable the Cluster Autoscaler to completely
    downscale all Worker nodes (or particular node pools), all node pools that are
    dedicated to Worker nodes by default have the value of this attribute set to
    \texttt{0}. Contrarily, the Cluster Autoscaler automatically disables
    autoscaling procedures in the node pool reserved for controller nodes (\texttt{controllers}),
    where the number of the \texttt{min\_nodes} attribute is set to the total number
    of nodes assigned to the node pool. Since there are no active Worker nodes
    when the cluster's overall workload is extremely low or nonexistent, with just
    one or a very small number of Controller Nodes requiring minimal resources,
    this enables the least amount of power consumption and resource loss.
    \newline
    Administrators can modify this attribute based on the cluster's requirements
    and/or increase the cluster's minimal capacity in preparation for a high volume
    of requests. However, whenever the attribute is changed, it is verified that
    the new value is not less than \texttt{0} or greater than the total number of
    nodes assigned to the node pool. The two latter checks prevent a node pool
    from having the minimum number of nodes below \texttt{0} (impossible) and also
    ensure that it does not exceed the node pool's maximum allowed capacity, as
    any autoscaling capabilities or procedures are automatically disabled when a
    value is greater than the number of nodes assigned to the node pool.
    \newline
    It should be noted that the additional knowledge of the number of active nodes
    (\texttt{count}) and the maximum number of nodes (\texttt{max\_nodes}) available
    in the node pool are calculable values that reflect a higher degree of knowledge
    concerning the Database schema and are, as a result, calculated by the Server
    and made available via internal services and the GraphQL API. The two values
    are also used by the Cluster Autoscaler to determine the number of active
    nodes currently present in the node pool as well as the highest number of
    nodes that can be provided for upscaling.

  \item \texttt{auto\_scale}
    \newline
    A Boolean flag that indicates whether or not the node pool can be employed
    for autoscaling.
    \newline
    The attribute is of type \texttt{boolean}, where \texttt{true} indicates
    that the node pool can be used for autoscaling and \texttt{false} indicates that
    it cannot be handled for autoscaling and all nodes assigned to the node pool
    must be ignored.
    \newline
    For all node pools that identify Worker nodes, the default value (\texttt{DEFAULT})
    is \texttt{true}, so the Server omits the attribute whenever it needs to
    create a new node pool for Worker nodes. The flag is set to \texttt{false}
    by the Server during the initial creation of the node pool designated to
    Controller nodes since any autoscaling procedures must ignore it.
    \newline
    When combined with the \texttt{min\_nodes} attribute, this attribute prevents
    the node pool designated for Controller nodes from autoscaling. Additionally,
    administrators have the same flexibility in changing the flag temporarily as
    with the prior attribute to meet specific cluster requirements.

  \item \texttt{created\_at}
    \newline
    Date and time of creation.
    \newline
    The attribute indicates when the node pool record was created.
    \newline
    The attribute is of type \texttt{timestamptz}.

  \item \texttt{updated\_at}
    \newline
    Date and time of the most recent update.
    \newline
    The attribute specifies the most recent timestamp the node pool record was updated.
    \newline
    The attribute is of type \texttt{timestamptz}.
\end{itemize}

\subsection{GraphQL API}
\label{subsec:implementation_server_graphql_api}

This description shows the GraphQL API made available by the Server, which enables
users and nodes to interact with both the high-level cluster, which represents
the Kubernetes cluster and its overall workload, and the low-level cluster,
which represents the bare-metal knowledge stored inside the Database. As previously
mentioned, the API can provide more advanced capabilities and calculated data/attributes
that are not present in the pure Database schema, such as the \texttt{count} and
\texttt{max\_nodes} attributes of the Node Pool entity.

\begin{wrapfigure}
  {r}{.25\textwidth} %
  \centering
  \def\stackalignment{r}\stackunder{ \includegraphics[width=\linewidth]{images/logos/graphql.png} } %
  {\scriptsize \parbox[t]{\linewidth}{ Source: \url{https://graphql.org/brand}} }
  \caption{GraphQL logo}
\end{wrapfigure}

GraphQL is a query language and server-side runtime for running queries using a
type system specified in a schema. A GraphQL service is built by specifying types
and fields on those types, then providing methods (resolvers) for each field on
each type. Before any operation is made, it is first verified that it only
pertains to the types and variables specified in the schema. Declarative data fetching
with GraphQL allows clients to define precisely which data are needed,
eliminating over- and under-fetching\footnote{\url{https://www.oreilly.com/library/view/hands-on-restful-web/9781838643577/becffd48-bca9-4c3a-9d89-0a490c5dce49.xhtml}}.
Additionally, a GraphQL server only provides a single API endpoint rather than multiple
endpoints that yield fixed data structures, as is the case with \texttt{REST}\footnote{\url{https://en.wikipedia.org/wiki/Representational_state_transfer}}
(REpresentational State Transfer)\cite{graphql}. The GraphQL endpoint in the Server
implementation is accessible at \texttt{/graphql}, but it can be modified to
better suit the requirements of the cluster's organization. It requires considerable
effort/time to understand how GraphQL operates and how its overall structure/architecture
is composed of all the various types, fields, queries, mutations, etc. The
latter, however, is outside the scope of this document, so it is strongly advised
to visit the official website at \texttt{https://graphql.org} for a better
comprehension of how GraphQL works. \\ %
It is nearly unfeasible to provide a detailed explanation of the \textit{generated}
GraphQL schema for the Server implementation because it consists of almost \texttt{2000}
lines of code. Therefore, the explanations in this section (hence the titles of the
two subsections) are limited to the queries and mutations that are exposed by
the GraphQL API and the majority of them can be considered as an extension of
the knowledge that is contained in the database schema that was previously
discussed. Additionally, the word \textit{generated} at the beginning of this paragraph
was not written at random but rather to highlight the fact that the GraphQL API is
implemented using a code-first approach rather than a schema-first approach,
establishing a single source of truth by defining the schema using classes and
decorators. \\ %
The majority of GraphQL queries and mutations provided by the Server
implementation do not require any authentication or authorization, allowing anyone
to utilize them. Far from it; the remaining queries and mutations are protected by
a customized GraphQL Auth Directive (\texttt{@auth}) that prevents
unauthenticated and/or unauthorized accesses. Since the GraphQL Auth Directive is
a standard feature that can be used and shared by other GraphQL APIs and projects,
it is not restricted to cluster implementation and enables protection for all
possible GraphQL APIs. To make the GraphQL Auth Directive accessible to the public
as a library that can be easily integrated with other projects, it was developed
as a side project alongside the cluster implementation. Attachment \ref{sec:corollary_projects_graphql_auth_diretive}
includes an in-depth overview correlated with examples of the GraphQL Auth Directive.
Compared to the implementation of the latter directive, the directive used in the
cluster implementation only differs by one small detail: \texttt{type}, an extra
argument used to identify the type of the entity making the request (\texttt{USER}
or \texttt{NODE}). Requests are rejected if the entity sending them does not match
the one listed in the directive. The value \texttt{USER} is assigned by default if
the \texttt{type} parameter is missing. \\ %
A personal security token that is generated and provided by the Server for
either a Node or a User is required from the entity performing the request to
access a protected GraphQL query or mutation. When a Node registers to the cluster
for the first time, it automatically obtains the token and is unable to obtain a
new one ever again. A User is more flexible and can obtain a new token whenever it
is successfully authenticated using the appropriate GraphQL mutation (\texttt{signIn}).
The token implementation and management rely entirely on JSON Web Tokens\footnote{\url{https://jwt.io/introduction}}
(JWT). JWT is an open standard, which specifies a compact and self-contained method
for safely transferring data between parties as a JSON (JavaScript Object
Notation) object. Because it is digitally signed with a secret (using the HMAC algorithm)
or a public/private key pair (using RSA or ECDSA), this information can be verified
and trusted\cite{rfc_7519}. The JWT token is digitally signed by \texttt{RS256}
in the Server implementation, but this can be easily modified in the configuration
settings with the appropriate public/private key pair files and corresponding encryption
algorithm. The content of the token comprises 4 pieces of encoded information in
addition to the JWT's data. The first of these is the \texttt{type}, which specifies
the type of the entity (\texttt{USER} or \texttt{NODE}), the second is the
\texttt{id}, which is used to uniquely identify the entity in queries and
mutations implementation, and the final two are the \texttt{roles} and \texttt{permissions}
of the entity, which are used in combination with the \texttt{type} to ensure protection.
The token must be provided in the request's \texttt{Authorization} header using the
\texttt{Bearer} authentication scheme\footnote{\url{https://www.rfc-editor.org/rfc/rfc6750}}:
\texttt{Authorization: Bearer <TOKEN>}. An error response is generated if the
entity attempts to access a protected API but the token is either absent, in an invalid
format, or the verification procedure fails. \\ %
It should be noted that in the implementation, all GraphQL queries/mutations that
return a list (\texttt{[]}) support pagination, which includes both offset
pagination and cursor-based pagination, via the arguments \texttt{cursor}, \texttt{skip}
(default value set to \texttt{0}), and \texttt{take} (default value set to
\texttt{8}). This allows the entities to divide the returned resources into manageable
chunks improving the overall search experience and the overall Server performance.
Additionally, the latter queries/mutations support advanced sorting, via the
\texttt{orderBy} argument, and filtering, via the \texttt{where} argument. As previously
stated, the GraphQL API is built on top of the database and its schema, so the API
can be considered an extension of the database in terms of both the parameters that
are accepted and the resources/objects that are returned.

\subsubsection{Queries}
\label{subsubsec:implementation_server_graphql_api_queries}

GraphQL queries (\texttt{Query}\footnote{\url{http://spec.graphql.org/draft/\#sec-Query}})
are used to query/fetch data without any server-side modification. \texttt{Query}
fields can be executed in parallel by the GraphQL engine. \\ %
A GraphQL \texttt{Query} is the logical equivalent of a \texttt{GET} request in \texttt{REST}.
\\ %
All GraphQL \texttt{Query}(s) supported by the Server implementation are listed
below:

\begin{itemize}
  \item \lstinline[language=graphql, morekeywords={[2]{Cpu}}, morekeywords={[4]{id}},
    morekeywords={[5]{cpu}}]{cpu(id: ID!): Cpu}
    \newline
    \texttt{cpu} query returns the \texttt{Cpu}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/Cpu.ts}}
    object that matches the specified identifier (\texttt{id} argument).
    \newline
    Because of the \texttt{!} symbol, the \texttt{id} argument cannot be \texttt{null}
    and is thus required. Whereas the returned \texttt{Cpu} object is nullable (there
    is no \texttt{!} symbol) implying that the query returns \texttt{null} if no
    CPU matches the provided identifier.

  \item \lstinline[language=graphql, morekeywords={[2]{Cpu, OrderByCpuInput, NonNegativeInt, WhereCpuInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{cpus}}]{cpus(cursor: ID, orderBy: OrderByCpuInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \newline
    \hphantom{---------}\lstinline[language=graphql, morekeywords={[2]{Cpu, OrderByCpuInput, NonNegativeInt, WhereCpuInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{cpus}}]{where: WhereCpuInput): [Cpu!]!}
    \newline
    \texttt{cpus} query returns a non-nullable list (\texttt{[]!}) of non-nullable
    \texttt{Cpu} objects.
    \newline
    The query allows for pagination (\texttt{cursor}, \texttt{skip}, and \texttt{take}
    arguments), filtering (\texttt{where} argument of type \texttt{WhereCpuInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereCpuInput.ts}}),
    and sorting (\texttt{orderBy} argument of type \texttt{OrderByCpuInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByCpuInput.ts}}).
    It is worth noting that the argument \texttt{skip} is of the type \texttt{NonNegativeInt},
    indicating that it can accept values of \texttt{0} (inclusive) or greater
    and that it is required (\texttt{!} symbol), but because it has a default
    value of \texttt{0}, it can be omitted in the request. The \texttt{take}
    argument is similar, but it can accept values less than, equal to, or greater
    than \texttt{0} and has a default value of \texttt{8}. The latter arguments
    and requirements are present in all queries that return a list, with the only
    variation being the type of the associated \texttt{where} and \texttt{orderBy}
    arguments.

  \item \lstinline[language=graphql, morekeywords={[2]{MAC, Interface}}, morekeywords={[4]{address, id}},
    morekeywords={[5]{interface}}]{interface(address: MAC, id: ID): Interface}
    \newline
    \texttt{interface} query returns the \texttt{Interface}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/Interface.ts}}
    object that matches the specified identifier (\texttt{id} argument) or MAC address
    (\texttt{mac} argument).
    \newline
    Because they both uniquely designate an \texttt{Interface} object, both arguments
    are nullable. If the query is executed with both arguments set to \texttt{null},
    an error is returned because the corresponding resolver function is unable
    to uniquely identify the record.

  \item \lstinline[language=graphql, morekeywords={[2]{Interface, OrderByInterfaceInput, NonNegativeInt, WhereInterfaceInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{interfaces}}]{interfaces(cursor: ID, orderBy: OrderByInterfaceInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \hphantom{----------------}\lstinline[language=graphql, morekeywords={[2]{Interface, OrderByInterfaceInput, NonNegativeInt, WhereInterfaceInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{interfaces}}]{where: WhereInterfaceInput): [Interface!]!}
    \newline
    \texttt{interfaces} query returns a non-nullable list of non-nullable \texttt{Interface}
    objects.
    \newline
    The \texttt{where} argument is of type \texttt{WhereInterfaceInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereInterfaceInput.ts}},
    and the \texttt{orderBy} argument is of type \texttt{OrderByInterfaceInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByInterfaceInput.ts}}.

  \item \lstinline[language=graphql, morekeywords={[2]{Node, IP}}, morekeywords={[4]{address, id, name}},
    morekeywords={[5]{node}}]{node(id: ID, address: IP, name: String): Node}
    \newline
    \texttt{node} query returns the \texttt{Node}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/Node.ts}}
    object that matches the specified identifier (\texttt{id} argument), IP
    address (\texttt{ip} argument) or name (\texttt{name} argument).

  \item \lstinline[language=graphql, morekeywords={[2]{Node, OrderByNodeInput, NonNegativeInt, WhereNodeInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{nodes}}]{nodes(cursor: ID, orderBy: OrderByNodeInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \newline
    \hphantom{-----------}\lstinline[language=graphql, morekeywords={[2]{Node, OrderByNodeInput, NonNegativeInt, WhereNodeInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{nodes}}]{where: WhereNodeInput): [Node!]!}
    \newline
    \texttt{nodes} query returns a non-nullable list of non-nullable \texttt{Node}
    objects.
    \newline
    The \texttt{where} argument is of type \texttt{WhereNodeInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereNodeInput.ts}},
    and the \texttt{orderBy} argument is of type \texttt{OrderByNodeInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByNodeInput.ts}}.

  \item \lstinline[language=graphql, morekeywords={[2]{NodePool}}, morekeywords={[4]{id, name}},
    morekeywords={[5]{nodePool}}]{nodePool(id: ID, name: String): NodePool}
    \newline
    \texttt{nodePool} query returns the \texttt{NodePool}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/NodePool.ts}}
    object that matches the specified identifier (\texttt{id} argument) or name (\texttt{name}
    argument).
    \newline
    As mentioned in section \ref{subsubsec:implementation_server_database_node_pool},
    the \texttt{Node Pool} object in the GraphQL API adds two additional attributes/fields
    to the basic knowledge provided by the Database schema. The first field, \texttt{max\_nodes},
    defines the total number of nodes that the node pool can handle. The field
    has the same value as the number of nodes that are associated with the
    corresponding node group. The second field, \texttt{count}, defines the
    amount of nodes that are associated with the respective node pool, have the \texttt{node\_pool\_assigned}
    attribute set to \texttt{true}, and are operational (active status). The latter
    field is used to determine the amount of Kubernetes nodes of the corresponding
    node pool that are operational and actively participating in the cluster.
    Both of these attributes are critical for autoscaling purposes and are
    extensively used by the Cluster Autoscaler component.

  \item \lstinline[language=graphql, morekeywords={[2]{NodePool, OrderByNodePoolInput, NonNegativeInt, WhereNodePoolInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{nodePools}}]{nodePools(cursor: ID, orderBy: OrderByNodePoolInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \newline
    \hphantom{----------------}\lstinline[language=graphql, morekeywords={[2]{NodePool, OrderByNodePoolInput, NonNegativeInt, WhereNodePoolInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{nodePools}}]{where: WhereNodePoolInput)): [NodePool!]!}
    \newline
    \texttt{nodePools} query returns a non-nullable list of non-nullable NodePool
    objects.
    \newline
    The \texttt{where} argument is of type \texttt{WhereNodePoolInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereNodePoolInput.ts}},
    and the \texttt{orderBy} argument is of type \texttt{OrderByNodePoolInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByNodePoolInput.ts}}.

  \item \lstinline[language=graphql, morekeywords={[2]{Status}}, morekeywords={[4]{id}},
    morekeywords={[5]{status}}]{status(id: ID!): Status}
    \newline
    \texttt{status} query returns the \texttt{Status}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/Status.ts}}
    object that matches the specified identifier (\texttt{id} argument).

  \item \lstinline[language=graphql, morekeywords={[2]{Status, OrderByStatusInput, NonNegativeInt, WhereStatusInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{statuses}}]{statuses(cursor: ID, orderBy: OrderByStatusInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \newline
    \hphantom{---------------}\lstinline[language=graphql, morekeywords={[2]{Status, OrderByStatusInput, NonNegativeInt, WhereStatusInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{statuses}}]{where: WhereStatusInput): [Status!]!}
    \newline
    \texttt{statuses} query returns a non-nullable list of non-nullable \texttt{Status}
    objects.
    \newline
    The \texttt{where} argument is of type \texttt{WhereStatusInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereStatusInput.ts}},
    and the \texttt{orderBy} argument is of type \texttt{OrderByStatusInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByStatusInput.ts}}.

  \item \lstinline[language=graphql, morekeywords={[2]{Storage}}, morekeywords={[4]{id}},
    morekeywords={[5]{storage}}]{storage(id: ID!): Storage}
    \newline
    \texttt{storage} query returns the \texttt{Storage}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/Storage.ts}}
    object that matches the specified identifier (\texttt{id} argument).

  \item \lstinline[language=graphql, morekeywords={[2]{Storage, OrderByStorageInput, NonNegativeInt, WhereStorageInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{storages}}]{storages(cursor: ID, orderBy: OrderByStorageInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \newline
    \hphantom{---------------}\lstinline[language=graphql, morekeywords={[2]{Storage, OrderByStorageInput, NonNegativeInt, WhereStorageInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{storages}}]{where: WhereStorageInput): [Storage!]!}
    \newline
    \texttt{storages} query returns a non-nullable list of non-nullable \texttt{Storage}
    objects.
    \newline
    The \texttt{where} argument is of type \texttt{WhereStorageInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereStorageInput.ts}},
    and the \texttt{orderBy} argument is of type \texttt{OrderByStorageInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByStorageInput.ts}}.

  \item \lstinline[language=graphql, morekeywords={[2]{User, NonEmptyString}}, morekeywords={[4]{id, username}},
    morekeywords={[5]{user}}]{user(id: ID, username: NonEmptyString): User}
    \newline
    \texttt{user} query returns the \texttt{User}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/entities/User.ts}}
    object that matches the specified identifier (\texttt{id} argument) or username
    (\texttt{username} argument).

  \item \lstinline[language=graphql, morekeywords={[2]{User, OrderByUserInput, NonNegativeInt, WhereUserInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{users}}]{users(cursor: ID, orderBy: OrderByUserInput, skip: NonNegativeInt! = 0, take: Int! = 8,}
    \newline
    \hphantom{-----------}\lstinline[language=graphql, morekeywords={[2]{User, OrderByUserInput, NonNegativeInt, WhereUserInput}},
    morekeywords={[4]{cursor, orderBy, skip, take, where}}, morekeywords={[5]{users}}]{where: WhereUserInput): [User!]!}
    \newline
    \texttt{users} query returns a non-nullable list of non-nullable \texttt{User}
    objects.
    \newline
    The \texttt{where} argument is of type \texttt{WhereUserInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/where/WhereUserInput.ts}},
    and the \texttt{orderBy} argument is of type \texttt{OrderByUserInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/orderby/OrderByUserInput.ts}}.
\end{itemize}

\subsubsection{Mutations}
\label{subsubsec:implementation_server_graphql_api_mutations}

GraphQL mutations (\texttt{Mutation}\footnote{\url{http://spec.graphql.org/draft/\#sec-Mutation}})
are used for operations that modify any server-side data. \texttt{Mutation} top-level
fields are executed in serial by the GraphQL engine. \\ %
A GraphQL \texttt{Mutation} is the logical equivalent of a \texttt{POST},
\texttt{PUT}, \texttt{PATCH}, or \texttt{DELETE} request in \texttt{REST}. \\ %
All GraphQL \texttt{Mutation}(s) supported by the Server implementation are
listed below:

\begin{itemize}
  \item \lstinline[language=graphql, morekeywords={[2]{CreateNodeInput, JWT}}, morekeywords={[4]{data}},
    morekeywords={[5]{createNode}}]{createNode(data: CreateNodeInput!): JWT!}
    \newline
    \texttt{createNode} resolver registers a new \texttt{Node} object in the
    Database using the data provided in the \texttt{data} input argument (of
    type \texttt{CreateNodeInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/create/CreateNodeInput.ts}})
    and returns a \texttt{JWT} security token if successful.
    \newline
    The associated Node will use the returned \texttt{JWT} token for any future operations
    involving any of the protected GraphQL APIs.
    \newline
    It should be noted that if the registration fails for any reason, no \texttt{JWT}
    token is returned and an error is generated.
    \newline
    Because neither the \texttt{data} argument nor the returned \texttt{JWT} token
    can be \texttt{null}, a value must/is always supplied.

  \item \lstinline[language=graphql, morekeywords={[2]{CreateUserInput, User}}, morekeywords={[4]{data}},
    morekeywords={[5]{createUser}}]{createUser(data: CreateUserInput!): User!}
    \newline
    \texttt{createUser} resolver registers a new \texttt{User} object in the
    Database using the data provided in the \texttt{data} input argument (of
    type \texttt{CreateUserInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/create/CreateUserInput.ts}})
    and returns the created \texttt{User} object if successful.
    \newline
    The returned \texttt{User} object includes extra fields related to the given
    data, such as the generated \texttt{id} and designated \texttt{roles}/\texttt{permissions}
    fields.
    \newline
    The plain text \texttt{password} is hashed using the \texttt{bcrypt}
    password-hashing function before being saved in the Database. Furthermore,
    there is no \texttt{password} field mapped in the GraphQL schema that is assigned
    to the \texttt{User} object, so it cannot be retrieved via any GraphQL queries
    or mutations. The latter is done as a security precaution to prevent
    sensitive information from leaking outside of the Server.
    \newline
    The difference between this mutation and the previous one is that no \texttt{JWT}
    security token is returned because the user must use the specific mutation
    \texttt{signIn} to successfully authenticate and acquire the \texttt{JWT}
    security token.
    \newline
    Because neither the \texttt{data} argument nor the returned \texttt{User} object
    can be \texttt{null}, a value must/is always supplied.

  \item \lstinline[language=graphql, morekeywords={[2]{NonEmptyString, JWT}}, morekeywords={[4]{password, username}},
    morekeywords={[5]{signIn}}]{signIn(username: NonEmptyString!, password: NonEmptyString!): JWT!}
    \newline
    \texttt{signIn} resolver requires a user's \texttt{username} and \texttt{password}
    to authenticate it. A \texttt{JWT} security token identifying the user entity
    is returned if the authentication procedure succeeded.
    \newline
    The \texttt{username} argument is used to uniquely identify a \texttt{User}
    record to acquire the associated \texttt{password} hashed using the \texttt{bcrypt}
    password-hashing algorithm during the registration procedure. A \texttt{bcrypt}
    function compares the provided plain text \texttt{password} argument with the
    saved hashed \texttt{password} and returns \texttt{true} if the password matches
    and \texttt{false} otherwise.
    \newline
    If the authenticated procedure fails due to an invalid \texttt{username} and/or
    \texttt{password}, the mutation's error provides the generic message \texttt{"Username
    or password is incorrect"} to prevent possible information leakage that an attacker
    could leverage.
    \newline
    It should be noted that the current implementation lacks an advanced token
    system for withdrawing tokens or verifying the current available active
    sessions. The latter may be employed in future implementations that depend on
    a more advanced architecture.

  \item \lstinline[language=graphql, deletekeywords={type}, morekeywords={[2]{Node, @auth}},
    morekeywords={[3]{USER, ADMIN}}, morekeywords={[4]{id, type, roles}},
    morekeywords={[5]{unassignNode}}]{unassignNode(id: ID!): Node! @auth(type: USER, roles: [ADMIN])}
    \newline
    \texttt{unassignNode} mutation unassign and deactivate the node identified by
    the \texttt{id} attribute.
    \newline
    The mutation is widely used in downscaling procedures to downscale the cluster's
    specific node, which is identified by the \texttt{id} argument. The
    procedure drains the Kubernetes node first, then deletes it from Kubernetes,
    and when the Server monitoring receives confirmation of the deletion, it remotely
    turns off the node via \texttt{SSH}. Section \ref{subsec:implementation_server_downscaling}
    explains the latter procedure in better detail.
    \newline
    An equivalent mutation for upscaling procedures does not exist because the
    Cluster Autoscaler only has high-level knowledge of the cluster and thus
    only knows about the current Kubernetes cluster and its corresponding active
    nodes, rather than all available nodes and their respective low-level information
    (i.e. identifiers, statuses and more).
    \newline
    The Auth directive (\texttt{@auth}) is used to protect the mutation from unauthenticated
    and unauthorized access. It specifies that only an entity of type \texttt{USER}
    with the \texttt{ADMIN} role is authorized to unassign and turn off a node (as
    indicated by the directive arguments).

  \item \lstinline[language=graphql, deletekeywords={type}, morekeywords={[2]{NodePool, UpdateNodePoolInput, @auth}},
    morekeywords={[3]{USER, ADMIN}}, morekeywords={[4]{id, data, type, roles}},
    morekeywords={[5]{updateNodePool}}]{updateNodePool(id: ID!, data: UpdateNodePoolInput!): NodePool! @auth(type: USER, roles: [ADMIN])}
    \newline
    \texttt{updateNodePool} mutation is used to update the node pool with the \texttt{data}
    argument (of type \texttt{UpdateNodePoolInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/update/UpdateNodePoolInput.ts}})
    identified by the \texttt{id} argument.
    \newline
    It should be noted that the \texttt{data} argument contains the \texttt{count}
    field that is not available in the Database and is thus considered marker information.
    The cluster is autoscaled whenever the \texttt{count} field is not \texttt{null}
    and the difference between it and the current count is not zero. If the specified
    \texttt{count} is less than the current \texttt{count}, the cluster must be
    downscaled, and the difference indicates how many nodes in the corresponding
    node pool must be downscaled (deleted from Kubernetes and powered off
    remotely). The downscale procedure is the same as the one outlined in the
    previous mutation, but the nodes that must be downscaled are selected with the
    strict policy of minimizing total cluster power consumption. As a result, the
    selected nodes are those that consume more energy than the others that are currently
    operational in the Kubernetes cluster. If the specified \texttt{count} is greater
    than the current \texttt{count}, the cluster must be upscaled, and the
    difference indicates how many nodes in the corresponding node pool must be
    upscaled (remotely powered on). The list of nodes that need to be upscaled is
    generated by searching for nodes that are currently in the \texttt{INACTIVE}
    status and have the appropriate interface with \texttt{Wake-on-Lan} support (\texttt{wol}
    attribute array non-empty) to allow remote bootstrapping. Then, the energy-saving
    policy is applied again, and the list is sorted in ascending order of power consumption:
    nodes at the beginning of the list are more power-efficient, while nodes at the
    end are less power-efficient. Starting from the beginning of the list, the Server
    selects the same number of nodes as the difference between the two count values
    (is always positive since it is an upscale procedure), and for each node, the
    Server sends a Wake-on-Lan message on the associated interfaces' MAC address,
    remotely bootstrapping it. Section
    \ref{subsec:implementation_server_upscaling} explains the latter procedure
    in better detail.
    \newline
    If the specified \texttt{count} field is less than zero or higher than the
    total number of nodes associated with the corresponding node pool (\texttt{max\_nodes}),
    an error is returned because the value provided is incorrect. Furthermore,
    if the identified node pool has disabled the autoscaling feature by having
    the \texttt{auto\_scale} flag attribute set to \texttt{false}, an error is returned
    indicating that the operation is not allowed for the given node pool.
    \newline
    Identical to the previous one, this mutation is protected by the Auth directive
    (\texttt{@auth}), which allows only a \texttt{USER} entity with the \texttt{ADMIN}
    role to perform the action.

  \item \lstinline[language=graphql, deletekeywords={type}, morekeywords={[2]{Status, UpdateStatusInput, @auth}},
    morekeywords={[3]{NODE}}, morekeywords={[4]{data, type}}, morekeywords={[5]{updateStatus}}]{updateStatus(data: UpdateStatusInput!): Status! @auth(type: NODE)}
    \newline
    \texttt{updateStatus} mutation updates the Status with the \texttt{data}
    argument (of type \texttt{UpdateStatusInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/update/UpdateStatusInput.ts}})
    of the corresponding Node identified in the \texttt{JWT} security token
    provided. If the update procedure is successful, the updated \texttt{Node} object
    is returned.
    \newline
    Because the mutation is protected by the Auth directive (\texttt{@auth}),
    and a \texttt{JWT} token (of a \texttt{NODE} entity type) must always be
    provided to perform the mutation, there is intrinsic knowledge of who is requesting
    the update. The \texttt{JWT} token encodes the Node identifier (\texttt{id} field),
    which is used to uniquely identify the status record that necessitates the update
    with the provided \texttt{data} argument. As stated in section \ref{subsubsec:implementation_server_database_status},
    the Status record identifier and the Node identifier are the same value
    because the Status entity's \texttt{id} attribute is both a Primary Key (\texttt{PK})
    and a Foreign Key (\texttt{PK}) that is uniquely related to the corresponding
    Node identifier (\texttt{id} field). If the two IDs are different, a join
    operation between the Node entity and the Status entity is required, necessitating
    of more resources and increasing the total execution time of the mutation.

  \item \lstinline[language=graphql, deletekeywords={type}, morekeywords={[2]{User, UpdateUserInput, @auth}},
    morekeywords={[3]{USER}}, morekeywords={[4]{data, type}}, morekeywords={[5]{updateUser}}]{updateUser(data: UpdateUserInput!): User! @auth(type: USER)}
    \newline
    \texttt{updateUser} mutation updates the User with the \texttt{data}
    argument (of type \texttt{UpdateUserInput}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/graphql/inputs/update/UpdateUserInput.ts}})
    of the corresponding User identified in the \texttt{JWT} security token
    provided. If the update procedure is successful, the updated \texttt{User} object
    is returned.
    \newline
    This mutation, like the previous one, relies on the identifier encoded in
    the provided \texttt{JWT} token (of a \texttt{USER} entity type) to uniquely
    identify the User record and update it with the \texttt{data} argument provided.
    \newline
    It should be noted that if the \texttt{password} field in the \texttt{data}
    argument is not \texttt{null}, it must be verified that it matches the
    minimal password criteria outlined in the section
    \ref{subsubsec:implementation_server_database_user}. If the \texttt{password}
    meets the minimal criteria, it is hashed using the \texttt{bcrypt} password-hashing
    function and then saved in the Database.
\end{itemize}

\subsection{Upscaling}
\label{subsec:implementation_server_upscaling}

\begin{wrapfigure}
  {l}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/implementation/upscaling.pdf}
\end{wrapfigure}

This section illustrates how the Server implementation performs the Upscaling procedure
to enable \texttt{INACTIVE} nodes to be remotely bootstrapped employing \texttt{Wake-on-Lan}.
\\ %
\texttt{Wake-on-Lan} (\texttt{WoL}) is an Ethernet standard protocol used to power
on a machine using a specifically crafted network message known as a Magic Packet\cite{wake_on_lan}.
It should be noted that to be awakened, the system's NIC must support and enable
\texttt{WoL}. The Magic Packet is a frame sent in broadcast (destination \texttt{MAC}
address \texttt{FF:FF:FF:FF:FF:FF}) containing the NIC's \texttt{MAC} address of
the target machine that needs to be awakened. When the Magic Packet is received
by the corresponding NIC and \texttt{WoL} is supported and enabled, the system
bootstraps as if it were turned on via the power button. Because the Magic Packet
is broadcasted, every NIC in the cluster receives it, but it is simply ignored because
the target \texttt{MAC} address does not match. \texttt{WoL} configuration and
flags have already been thoroughly discussed in section
\ref{subsubsec:implementation_server_database_interface}. \\ %
When the \texttt{data} argument of the \texttt{updateNodePool} GraphQL mutation
has the \texttt{count} field set to a value larger than the current one, the
Upscaling process is executed. It should be noted that if the \texttt{count}'s
value is less than the current \texttt{count}, the process is reversed and a
Downscale procedure is performed. The difference between the \texttt{count}
argument and the current \texttt{count} is the number of \texttt{INACTIVE} nodes
in the associated node pool that must be bootstrapped. The Server implementation
examines invalid or unfeasible values (i.e. the \texttt{count} is higher than
the total number of nodes assigned to the associated node pool) and potential errors
during the overall procedure. \\ %
Because the Server only has a low-level knowledge of the cluster and does not continuously
monitor the entire Kubernetes cluster, its overall workload, and resource consumption,
it must be instructed to execute the action by another entity. The entity that initiates
the Upscaling procedure is typically the Cluster Autoscaler, which, unlike the Server,
has a high-level knowledge of the cluster and can thus use the previously
missing information to determine when and for how many nodes to perform the
Upscaling procedure. A human Administrator is the second entity that physically forces
the cluster to be Upscaled. The latter is less common, but it can be the consequence
of different motivations and or requirements, such as preparing the cluster for
an upcoming high volume of traffic that cannot be sustained with the previous
number of active nodes. \\ %
The decision about which \texttt{INACTIVE} nodes should be bootstrapped adheres to
the strict policy of power consumption reduction. The collection of \texttt{INACTIVE}
nodes is ordered in ascending order of power consumption, first by the \texttt{max\_power\_consumption}
attribute, then by the \texttt{min\_power\_consumption} attribute if the previous
values are the same. If two nodes have both attributes equal, they are compared against
their performance in descending order, with the most performant nodes appearing
first. The final resulting list begins with all of the most power-efficient nodes
and ends with the most energy-hungry nodes. Starting at the top of the list, the
number of nodes that must be bootstrapped is determined by the difference between
the count data argument and the current count. It should be noted that the
Cluster Autoscaler's Automatic Upscaling request is typically not aggressive and
only involves one node. As a result, only the first node at the top of the list
is typically selected and involved in the bootstrapping process. \\ %
The preceding information and strategies for cluster Upscaling are condensed in
the following steps and visually depicted in Figure \ref{fig:upscaling}:
\begin{enumerate}
  \item A Cluster Autoscaler or Administrator entity submits a request to the GraphQL
    mutation \texttt{updateNodePool} with the node pool's identifier (\texttt{id}
    argument) and a \texttt{count} value higher than the current \texttt{count}.
    To allow the entity to perform the action, a valid \texttt{JWT} token must
    be supplied in the \texttt{Authorization} header.
    \newline
    As an example, the identified node pool is \texttt{460d17d5-96ff-4e56-815c-e3367c60ae0d},
    the \texttt{count} value is \texttt{2}, and the current count is \texttt{1}.

  \item The \texttt{count} value is compared to the current one to determine
    whether the number of nodes in the corresponding node group should be
    \texttt{Increased} (\texttt{Upscaled}) or \texttt{Decreased} (\texttt{Downscaled}).
    The requested operation is to \texttt{Increment} the number of nodes in the
    node pool because the \texttt{count} value is higher. Moreover, it is also calculated
    the difference between the two values to determine the number of nodes to bootstrap.
    \newline
    Because the provided \texttt{count} is higher than the existing count, the
    \texttt{Increase} operation is selected. The difference (\texttt{N}) number is
    \texttt{1}.

  \item The Database is queried to obtain the list of \texttt{INACTIVE} nodes in
    the associated node pool that also supports \texttt{WoL}.

  \item By comparing the \texttt{max\_power\_consumption} and/or \texttt{min\_power\_consumption}
    attributes, the list is ordered in ascending order of power consumption. If the
    latter two attributes for some nodes are identical, they are sorted by
    performance score in descending order.

  \item \texttt{N} nodes are selected starting at the top of the list, where the
    most power-efficient nodes are located. A Magic Packet in broadcast is sent
    for each of these \texttt{N} nodes, targeting the \texttt{MAC} address of the
    associated node's NIC. After that, the status of each node is updated to \texttt{BOOTING}.
    \newline
    The single most power-efficient node is selected (\texttt{NODE 1}) and the
    Magic Packet targeting its NIC \texttt{MAC} address is sent. The status is
    then updated to \texttt{BOOTING}.

  \item The requested \texttt{N} nodes are successfully bootstrapped, and following
    the initialization procedure, they join the Kubernetes cluster.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{images/implementation/upscaling_schema.pdf}
  \caption{Upscaling scheme}
  \label{fig:upscaling}
\end{figure}

\subsection{Downscaling}
\label{subsec:implementation_server_downscaling}

\begin{wrapfigure}
  {l}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/implementation/downscaling.pdf}
\end{wrapfigure}

This section illustrates how the Server implementation performs the Downscaling procedure
to enable \texttt{ACTIVE} nodes to be remotely powered off employing \texttt{SSH}.
\\ %
\texttt{SSH} (Secure Shell) is a protocol for secure remote login from one
machine to another. \texttt{SSH} is used in the cluster implementation to
remotely issue the \texttt{poweroff}\footnote{\url{https://linux.die.net/man/8/poweroff}}
command to the corresponding node that needs to be Downscaled. The \texttt{SSH} remote
connection is immediately closed (automatic disconnection) by the Server as soon
as the command is successfully issued to prevent potential errors because the Downscaled
node begins the shutdown procedure, which includes killing all processes, including
the \texttt{SSH} Server process itself. Every node in the cluster has an \texttt{SSH}
Server that accepts remote connections. The \texttt{SSH} Server process is
continuously monitored by the corresponding Init System deployed on the node, and
if it fails for any reason, it is resumed automatically. To issue the \texttt{poweroff}
command, the Server implementation involves an \texttt{SSH} Client that
establishes a secure and remote connection with the associated \texttt{SSH} Server
by specifying the node's \texttt{IP} address. Both the \texttt{SSH} Server on
the nodes and the \texttt{SSH} Client on the Server component must be properly configured
with correct parameters, certificates, and keys, or the request for a connection
from Client to Server will be promptly denied, resulting in the failure of the Downscaling
procedure. During the installation procedure, the installer script configures the
\texttt{SSH} Server, replacing any previous configuration with the one provided.
The \texttt{SSH} Client configuration, on the other hand, is part of the Server and
is thus performed with the same approach (see section
\ref{subsec:implementation_server_configuration}). The current cluster
implementation configures \texttt{SSH} in accordance with the best practices outlined
in the article "\texttt{Secure Secure Shell}"\footnote{\url{https://stribika.github.io/2015/01/04/secure-secure-shell.html}}:
Only the \texttt{Ed25519}\footnote{\url{https://ed25519.cr.yp.to/ed25519-20110926.pdf}}
high-speed, high-security digital signature scheme is allowed, and only the most
secure \texttt{Cipher}s and \texttt{MAC}s are enabled. Because the latter are only
configuration files, the organization can easily change and customize them to
reflect different requirements. Having all cluster nodes pre-configured with \texttt{SSH}
allows extra and diverse capabilities: Administrators can use an \texttt{SSH}
Client to remotely connect to a node and perform various operations such as
interactive and/or automated file transfers, whereas the use of automation tools
such as Ansible\footnote{\url{https://www.ansible.com}} or Terraform\footnote{\url{https://www.terraform.io}}
allow performing operations in parallel and in the same programmatic way,
enabling another solution of configuring/managing the cluster via an
Infrastructure as a Code. Finally, if the cluster is deployed in an Air-Gap
environment and/or relying on \texttt{SSH} for remotely issuing the \texttt{poweroff}
command is deemed unnecessary or too verbose for an organization, other non-protected
and unsecure login protocols, such as \texttt{telnet} or \texttt{rlogin}, exist and
can be used to achieve the same result. The cluster implementation currently
only supports \texttt{SSH}, but extending support for other alternatives and/or
unsecure protocols should be relatively simple. \\ %
Two GraphQL mutations support the Downscaling procedure:
\begin{enumerate}
  \item \texttt{updateNodePool}
    \newline
    The first mutation is very similar to the process outlined for Upscaling. The
    Downscaling procedure is triggered when the \texttt{count} field in the
    \texttt{data} argument is set to a value smaller than the current one. The
    number of \texttt{ACTIVE} nodes in the associated node pool that must be terminated
    is the difference between the current \texttt{count} and the \texttt{count} argument.
    The collection of current \texttt{ACTIVE} nodes is arranged in the same way as
    the Upscaling procedure, using the same attributes and the same priority
    order. The sorting order, however, is inverted, so the \texttt{max\_power\_consumption}
    and \texttt{min\_power\_consumption} attributes are ordered in descending
    order while the \texttt{performance} is ordered in ascending order. The
    final list starts with the most energy-hungry (least power-efficient) nodes
    and ends with the most power-efficient (least energy-hungry) nodes. The number
    of nodes that must be terminated, starting at the top of the list, is equivalent
    to the previously stated difference between the current \texttt{count} and
    the \texttt{count} field.
    \newline
    Before completely terminating a node, it must be drained (automatically
    performed by the Cluster Autoscaler before calling the GraphQL mutation) and
    deleted from the Kubernetes cluster (status \texttt{ACTIVE\_DELETING}), resulting
    in all of its deployments being safely migrated without outages and the node
    being completely removed by the Kubernetes cluster, stopping the K8s Agent
    and its monitoring. The latter action is asynchronous, and once completed, the
    Server retrieves the \texttt{IP} address of the associated node, initiates an
    \texttt{SSH} remote connection to the \texttt{SSH} Server, and issues the
    \texttt{poweroff} command, effectively terminating the node. When the latter
    is finished, the node's status is updated to \texttt{INACTIVE}. The node's
    \texttt{ACTIVE} status lasts only a few seconds, from when the Server is
    notified of the Kubernetes removal to when the command is properly issued
    via \texttt{SSH}.
    \newline
    It should be noted that the Server is notified of the node's successful
    removal from Kubernetes by utilizing the same monitoring system discussed in
    section \ref{subsec:implementation_server_monitoring}, which is also used to
    keep the node's status updated. As a result, the implementation meets two requirements
    with a single efficient solution.

  \item \texttt{unassignNode}
    \newline
    The second mutation is much simpler than the first because it does not
    require the entire section of Increase/Decrease, filtering, and sorting
    because the node to be terminated is already provided as an argument (\texttt{id}).
    \newline
    The complete termination procedure, which includes draining and deleting the
    node from Kubernetes and then remotely terminating via \texttt{SSH}, is the same
    as previously described.
\end{enumerate}
The important distinction between the two mutations is that \texttt{updateNodePool}
is generic in that it is most concerned about how many and which nodes need to
be terminated, whereas \texttt{unassignNode} is specific in that it allows to
specifically identify which node needs to be terminated. Because most Downscale operations
are performed on a single and known a priori node, the mutation \texttt{unassignNode}
is the obvious and preferred option. The latter is especially noticeable in the
Cluster Autoscaler, which knows which nodes are underutilized and for how long thanks
to continuous monitoring. If one of the latter nodes exceeds a predefined time
threshold, the Cluster Autoscaler makes a request to the \texttt{unassignNode}
mutation, specifying the identifier of the underutilized node. As previously
mentioned, when the Cluster Autoscaler sends the Downscaling request, the Drain
operation is much quicker because it has almost already been performed by the
Cluster Autoscaler itself and/or the residual number of deployments are
extremely low. \\ %
The preceding information and strategies for cluster Downscaling are condensed
in the following steps and visually depicted in Figure \ref{fig:downscaling}:
\begin{enumerate}
  \item A Cluster Autoscaler or Administrator entity submits a request to one of
    the two GraphQL mutations with a valid \texttt{JWT} token supplied in the
    \texttt{Authorization} header:
    \begin{itemize}
      \item \texttt{updateNodePool}
        \begin{enumerate}
          \item The request includes the node pool's identifier (\texttt{id} argument)
            and a \texttt{count} value lower than the current \texttt{count}.
            \newline
            As an example, the identified node pool is \texttt{460d17d5-96ff-4e56-815c-e3367c60ae0d},
            the \texttt{count} value is \texttt{1}, and the current count is \texttt{2}.

          \item The Database is queried to obtain the list of \texttt{ACTIVE\_READY}
            nodes in the associated node pool.

          \item By comparing the \texttt{max\_power\_consumption} and/or \texttt{min\_power\_consumption}
            attributes, the list is ordered in descending order of power
            consumption. If the latter two attributes for some nodes are identical,
            they are sorted by performance score in ascending order.

          \item \texttt{N} nodes are selected starting at the top of the list, where
            the less power-efficient nodes are located.

          \item For each selected node: Continue to parent step \texttt{2}.
        \end{enumerate}

      \item \texttt{unassignNode}
        \begin{enumerate}
          \item The request includes the node identifier (\texttt{id} argument).
            \newline
            As an example, the identified node is \texttt{e64fd01e-03c4-435b-9b02-32dfed936507}.

          \item The database is queried to obtain information about the corresponding
            node.

          \item Continue to parent step \texttt{2}.
        \end{enumerate}
    \end{itemize}

  \item The corresponding Kubernetes node is drained and deleted.
    \newline
    The node's status is updated to \texttt{ACTIVE\_DELETING}.

  \item The previous operation is completed and the Server is notified that the node
    was successfully deleted from Kubernetes.
    \newline
    The node's status is updated to \texttt{ACTIVE}.

  \item The Server retrieves the corresponding node's \texttt{IP} address, establishes
    an \texttt{SSH} remote connection to the \texttt{SSH} Server, and issues the
    \texttt{poweroff} command.
    \newline
    The node's status is updated to \texttt{INACTIVE}.

  \item The node has been successfully terminated, and the Kubernetes cluster
    now has one fewer node.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics
  [width=\textwidth]{images/implementation/downscaling_schema.pdf}
  \caption{Downscaling scheme}
  \label{fig:downscaling}
\end{figure}

\subsection{Monitoring}
\label{subsec:implementation_server_monitoring}

The Server component must continuously monitor the currently active nodes to
grasp their \texttt{status} and determine whether a node has been successfully \texttt{Added},
\texttt{Updated}, or \texttt{Deleted}. \\ %
Implementing a monitoring system from scratch that is based on periodic updates
via Heartbeat messages and is also reliable, scalable, performant, and simple to
use is a challenging task. \\ %
Because the cluster is built around Kubernetes, a monitoring system with
periodic Heartbeat messages is already in place, satisfying all of the preceding
requirements. As a result, the Server implementation takes advantage of the latter,
thanks to the use of a Kubernetes \texttt{Informer}\footnote{\url{https://github.com/kubernetes-client/javascript/blob/master/src/informer.ts}}.
\\ %
A Kubernetes \texttt{Informer} employs the Kubernetes API to \texttt{List} and \texttt{Watch}
a specific resource or a group of resources. To prevent performance degradation and
potentially useless requests (the resource has not been changed since the last
request), the \texttt{Informer} does not constantly query (polling) the Kubernetes
API. Instead, the \texttt{Informer} queries the resource(s) and stores the result
in a local/internal cache, and an event (\texttt{Create}, \texttt{Update}, or \texttt{Delete})
is only triggered/generated when the cached resource(s) differs from the one
available through the Kubernetes API. The latter outlined Kubernetes \texttt{Informer}
is only an oversimplification of how it works and how it is internally
implemented, but it is critical to understand that it provides a powerful, yet
simple-to-use and configure, mechanism for monitoring cluster nodes. \\ %
The Kubernetes \texttt{Informer} can be considered as a subscription mechanism
in which the Server component subscribes to the Node resource and whenever the
resource changes, an event callback (\texttt{Add}, \texttt{Update}, or \texttt{Delete})
is triggered. \\ %
The monitoring system is implemented by the Server through a specialized class
called \texttt{NodeInformer}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/k8s/NodeInformer.ts}},
which encapsulates the Kubernetes Informer and is subscribed to all possible
events concerning the Node resource. When the Kubernetes \texttt{Informer}
triggers an event, the \texttt{NodeInformer} class's associated callback method
is called. A callback function is only a frontend interface to a Kubernetes event,
with the actual business logic delegated to one of the many Services based on
the event that triggered the callback. The \texttt{NodeInformer} class, and thus
the Kubernetes \texttt{Informer}, are only instantiated once when the Server bootstraps
and terminated when the Server itself terminates. \\ %
Figure \ref{fig:informer} depicts the monitoring scheme and the interaction of the
previously mentioned components.

% TODO Check
\pagebreak


\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/implementation/informer.pdf}
  \caption{Monitoring scheme}
  \label{fig:informer}
\end{figure}

The following list depicts the various NodeInformer callbacks and their purposes:
\begin{itemize}
  \item \texttt{onAdd(node)}
    \newline
    A new \texttt{node} resource has been added.
    \newline
    Update the associated \texttt{node} record with the acquired \texttt{IP} address
    and \texttt{status}.

  \item \texttt{onUpdate(node)}
    \newline
    A \texttt{node} resource has been updated.
    \newline
    Update the associated \texttt{node} record with the acquired \texttt{IP} address
    and \texttt{status}.

  \item \texttt{onDelete(node)}
    \newline
    A \texttt{node} resource has been deleted.
    \newline
    Terminate (\texttt{poweroff}) the associated \texttt{node}. The latter
    procedure also updates its \texttt{status}.

  \item \texttt{onConnect()}
    \newline
    \texttt{Informer} has successfully connected and is started monitoring the associated
    resource (\texttt{Node}).

  \item \texttt{onError(error)}
    \newline
    Informer has encountered an \texttt{error}.
    \newline
    After \texttt{3} seconds, it automatically restarts. The latter value is
    configurable.
\end{itemize}

\subsection{Configuration}
\label{subsec:implementation_server_configuration}

The Server implementation is fully customizable, with two methods available. \\ %
The first method is to use a \texttt{TypeScript} configuration file, named
\texttt{config.ts}\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/config/config.ts}},
that contains all of the setting parameters and structures used by the Server. Because
it is a pure \texttt{TypeScript} file, it can be imported and used directly by other
\texttt{TypeScript} source files with no additional steps. \\ %
The second method, which is more commonly used for application configuration,
employs the use of environment variables\footnote{\url{https://github.com/carlocorradini/reCluster/blob/main/server/src/config/env.ts}}.
Because polluting the environment with a multitude of variables is not a good practice,
only the most significant and/or required configuration parameters are available.
If an environment variable is provided, it overrides the default value assigned
in the first method's \texttt{TypeScript} configuration file (\texttt{config.ts}).
As a result, even though it only supports a subset of all possible configuration
parameters, the second approach has a higher precedence than the first. \\ %
It would take too much space to explain all of the available configuration parameters
that the Server implementation can be customized with, and some are self-explanatory,
such as the \texttt{name} parameter, which specifies the application name, while
others are almost immutable and rarely change. As a result, the following table
only describes all of the configuration parameters and thus the environment
variables that can be customized using the second method.

\begin{xltabular}
  {\textwidth} { >{\ttfamily}l | c | X | >{\ttfamily}c }

  \multicolumn{1}{ c |}{\large{\textbf{Name}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Required}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Description}}} &
  \multicolumn{1}{ c }{\large{\textbf{Default Value}}} \\ \hhline{====}

  NODE\_ENV & \textcolor{bulmaRed}{\faicon{times}} & \texttt{Node.js}
  environment.
  \newline
  Different environments enable or disable certain Server components or features.
  For example, in \texttt{production}, the GraphQL Studio Explorer\footnote{\url{https://www.apollographql.com/docs/graphos/explorer/explorer}}
  is disabled, whereas it is enabled in all other environments.
  \newline
  The following environments are supported:
  \begin{itemize}
    \item \texttt{development}

    \item \texttt{production}

    \item \texttt{test}
  \end{itemize}
  & production \\ \hline

  HOST & \textcolor{bulmaRed}{\faicon{times}} & Listening address(es). & 0.0.0.0
  \\ \hline

  PORT & \textcolor{bulmaRed}{\faicon{times}} & Listening port. & 80 \\ \hline

  LOGGER\_LEVEL & \textcolor{bulmaRed}{\faicon{times}} & Logging level.
  \newline
  Attachment \ref{cha:logging} provides additional information regarding logging
  and logging levels.
  \newline
  The following logging levels are supported (listed in descending order of
  importance):
  \begin{itemize}[noitemsep]
    \item[\protect\icircled{\texttt{7}}] \texttt{silent}

    \item[\protect\icircled{\texttt{6}}] \texttt{fatal}

    \item[\protect\icircled{\texttt{5}}] \texttt{error}

    \item[\protect\icircled{\texttt{4}}] \texttt{warn}

    \item[\protect\icircled{\texttt{3}}] \texttt{info}

    \item[\protect\icircled{\texttt{2}}] \texttt{debug}

    \item[\protect\icircled{\texttt{1}}] \texttt{trace}
  \end{itemize}
  & info \\ \hline

  DATABASE\_URL & \textcolor{bulmaGreen}{\faicon{check}} & Database URL.
  \newline
  The current implementation is dependent on \texttt{PostgreSQL} and therefore
  the \texttt{URL} must be a valid \texttt{PostgreSQL} connection string\footnote{\url{https://www.postgresql.org/docs/current/libpq-connect.html\#LIBPQ-CONNSTRING}}.
  & \\ \hline

  SSH\_USERNAME & \textcolor{bulmaRed}{\faicon{times}} & \texttt{SSH} username. &
  root \\ \hline

  SSH\_PRIVATE\_KEY & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{SSH}
  private key (identity file). & \\ \hline

  SSH\_PASSPHRASE & \textcolor{bulmaRed}{\faicon{times}} & \texttt{SSH}
  passphrase.
  \newline
  The passphrase used to encrypt the \texttt{SSH} private key's sensitive
  section.
  \newline
  Because the default cluster implementation does not involve any passphrase, it
  is not required. & \\ \hline

  TOKEN\_PRIVATE\_KEY & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{JWT}
  private key.
  \newline
  The private key used to sign the \texttt{JWT}. & \\ \hline

  TOKEN\_PUBLIC\_KEY & \textcolor{bulmaGreen}{\faicon{check}} & \texttt{JWT}
  public key.
  \newline
  The public key used to verify the \texttt{JWT}. & \\ \hline

  TOKEN\_PASSPHRASE & \textcolor{bulmaRed}{\faicon{times}} & \texttt{JWT}
  passphrase.
  \newline
  The passphrase used to encrypt the \texttt{JWT} private key's sensitive
  section.
  \newline
  Because the default cluster implementation does not involve any passphrase, it
  is not required. & \\ \hline

  \caption{Server configuration environment variables}
\end{xltabular}

\section{Autoscaling}
\label{sec:implementation_autoscaling}

Autoscaling dynamically increases (upscale) or decreases (downscale) the number
of cluster resources (limits, replicas, and nodes) that are allocated. \\ %

The cluster must be able to automatically perform autoscaling procedures to independently
and autonomously change the resource limits of pods, number of replicas (pods), and
number of nodes, requiring no or minimal human/manual intervention. \\ %
Kubernetes includes three Autoscaler components that perform automatically and
autonomously the previous autoscaling procedures: \texttt{Vertical Pod Autoscaler}
(see section \ref{subsec:implementation_autoscaling_vertical_pod_autoscaler})
modifies the resource limits of the pods, \texttt{Horizontal Pod Autoscaler} (see
section \ref{subsec:implementation_autoscaling_horizontal_pod_autoscaler})
modifies the number of replicas (pods), and \texttt{Cluster Autoscaler} (see section
\ref{subsec:implementation_autoscaling_cluster_autoscaler}) modifies the number
of nodes in the cluster. \\ %

Because autoscaling (\texttt{Vertical}, \texttt{Horizontal}, and \texttt{Cluster})
is not the primary emphasis of the document, this section is not dedicated to explaining
it in depth and how it is implemented. Rather, it is intended to provide a high-level
overview of the three autoscaling mechanisms and their Kubernetes implementations.
The only exception is the \texttt{Cluster Autoscaler}, because it necessitates a
custom implementation built on top of the existing one to be compatible with the
specific cluster environment (cloud provider).

\subsection{Vertical Pod Autoscaler}
\label{subsec:implementation_autoscaling_vertical_pod_autoscaler}

\begin{wrapfigure}
  {l}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{
    images/implementation/vertical_pod_autoscaler.pdf
  }
\end{wrapfigure}

Vertical Pod Autoscaler\footnote{\url{https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler}}
(VPA) adjusts a Pod's resource attributes (requests and limits) automatically. \\ %

Pods are executed with unbounded resource constraints by default and can thus utilize
as much as the amount of available resources on the Node. To address this and
other potential issues, a global policy can be defined via a Limit Range\footnote{\url{https://kubernetes.io/docs/concepts/policy/limit-range}},
which allows particular types of objects (such as \texttt{Pod} or \texttt{PersistentVolumeClaims})
to be limited in the quantity of allocable resources. The latter only partially solves
the problem because it does not allow for a finer-grained resource definition for
each possible scheduled object deployment/instance, because some Pods may
require fewer resources, resulting in resource waste, while others may require/need
more resources, resulting in performance degradation and/or service outages. \\ %
Kubernetes enables the fine-grained definition of the amount of resources that a
Pod necessitates, commonly \texttt{CPU} and \texttt{Memory}. There are two kinds
of resource definitions available: \texttt{request} and \texttt{limit}\footnote{\url{https://kubernetes.io/docs/concepts/configuration/manage-resources-containers}}.
\\ %
The resources \texttt{request} is the bare minimum of resources that the Pod necessitates.
The \texttt{kube-scheduler}\footnote{\url{https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler}}
component, which monitors for newly created Pods with no Node designated, uses this
information to determine which Node is the best choice for the Pod to be scheduled
on. When a Pod is deployed to a Node, the \texttt{kubelet}\footnote{\url{https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet}}
component reserves the Node's desired resources \texttt{request} exclusively for
the Pod. \\ %
The resources \texttt{limit} is the highest amount of resources that the Pod can
use. The \texttt{kubelet} component enforces these constraints on a Pod, preventing
it from utilizing more than what has been defined. These limits can be applied
either \texttt{reactively} (when a violation is detected) or by \texttt{enforcement}
(prevents the container from ever exceeding the limits). \\ %
If the Node where the Pod is deployed has sufficient free resources, the Pod may
use more resources than the defined \texttt{request} amount while not exceeding
the defined \texttt{limit}. \\ %
Manually defining a Pod's resources \texttt{request} and resources \texttt{limit}
is a challenging task because it can result in resource waste if the values are set
too high, as well as potential service outages and/or performance degradation if
the values are set too low. The latter issues are the same as when the \texttt{Limit
Range} approach is used. \\ %
Vertical Pod Autoscaler (VPA) relieves users from the burden of manually, accurately,
and consistently updating the resources \texttt{limit} and \texttt{request} of a
Pod. The VPA automatically sets the values of resources \texttt{request} and
resources \texttt{limit} based on Pod utilization over time, providing for an
improved scheduling mechanism that ensures the proper resource amount is
available for each Pod scheduled on a Node. Moreover, the VPA can maintain the
same resource ratio between \texttt{request} and \texttt{limit} that the user
initially defined. As a result, the VPA downscales Pods that over-request
resources and upscales Pods that under-request resources.

\subsection{Horizontal Pod Autoscaler}
\label{subsec:implementation_autoscaling_horizontal_pod_autoscaler}

\begin{wrapfigure}
  {l}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{
    images/implementation/horizontal_pod_autoscaler.pdf
  }
\end{wrapfigure}

Horizontal Pod Autoscaler\footnote{\url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale}}
(HPA) adjusts the number of Pods (replicas) automatically. \\ %

Manually defining the number of Pods for a deployment is a difficult task that,
like in the VPA, if not properly configured can result in resource waste and/or service
disruptions. \\ %
The previously described resource \texttt{request} and \texttt{limit} must be specified
when defining the Pod configuration. They are employed to establish the single
resource or average resource utilization measurements of all Pods and whether an
upscale (add replicas) or downscale (remove replicas) action is required. \\ %
The HPA configuration must target a specific application deployment and must
include a minimal number of replicas and a maximum number of replicas. The minimal
number of replicas is used to determine a lower bound (inclusive) level below which
the HPA terminates any downscale operation. The maximum number of replicas is
used to determine an upper bound (inclusive) level that cannot be exceeded, and the
HPA stops any upscale operation. The final required parameter for the HPA is to
specify a collection of metrics that indicate when to perform any autoscaling
operation based on what resource types and their corresponding values to monitor.
The \texttt{CPU} and \texttt{Memory} resources, as well as their average value, are
the most common metric types. As an example, suppose the HPA is set to monitor the
CPU metric with an average resource utilization of 50\%. The HPA adds or removes
Pods until the deployment's average CPU utilization is near 50\%. If the average
utilization is greater than 50\%, the HPA would then upscale by adding more Pods,
whereas if the average utilization is less than 50\%, the HPA would then
downscale by removing Pods. \\ %
Although the resource attributes used are similar to those employed by the VPA, the
HPA behavior, execution, and algorithm are significantly different because the VPA
scales the resources of a Pod while the HPA scales the number of Pods. It should
be noted that HPA cannot be used alongside VPA on the same resources, and horizontal
scaling is only suitable for stateless applications. \\ %
Horizontal Pod Autoscaler (HPA) relieves users from the burden of manually,
accurately and consistently updating the number of Pods for a specific
deployment by leveraging metrics monitoring and utilization.

\subsection{Cluster Autoscaler}
\label{subsec:implementation_autoscaling_cluster_autoscaler}

\begin{wrapfigure}
  {l}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{
    images/implementation/cluster_autoscaler.pdf
  }
\end{wrapfigure}

Cluster Autoscaler\footnote{\url{https://github.com/carlocorradini/autoscaler/tree/master/cluster-autoscaler}}
(CA) adjusts the number of Nodes automatically. \\ %

Because the CA has already been extensively described in section \ref{subsec:architecture_components_cluster_autoscaler},
this section is devoted to showing how it is intended to interface with the
Server component (and, more broadly, with the cluster implementation) for
upscaling (see section \ref{subsec:implementation_server_upscaling} and section \ref{subsubsec:implementation_autoscaling_cluster_autoscaler_upscaling})
or downscaling (see section \ref{subsec:implementation_server_downscaling} and section
\ref{subsubsec:implementation_autoscaling_cluster_autoscaler_downscaling})
operations. Finally, are mentioned some of the most important configuration
parameters that can be modified to adjust the CA and its general autoscaling
responsiveness/behavior to better fit the use-case situation of the organization
maintaining the cluster. \\ %
Unlike the prior autoscalers, where only VPA or HPA could be used to monitor the
same resources, CA can be used alongside both of them because it depends on
different metric objects and has a different end goal. The combination of HPA and
CA in the cluster is especially recommended. If the HPA attempts to schedule a
number of Pods that exceed the current cluster capacity, the CA responds by increasing
the total number of active nodes in the cluster, allowing un-schedulable/waiting
Pods (Pods waiting to be assigned to a Node with enough free resources) to be
scheduled on the newly booted node. If the HPA decreases the number of Pods,
reducing the overall cluster workload, and a Node remains underutilized, the CA
responds by migrating the Pods scheduled on the Node to another Node, decreasing
the total number of active nodes in the cluster\cite{hpa_ca}.

\subsubsection{Cloud Provider}
\label{subsubsec:implementation_autoscaling_cluster_autoscaler_cloud_provider}

The CA implementation differs from the previous two autoscalers in that it is not
restricted to the Kubernetes cluster boundaries but also involves the external cluster
managed by the so-called Cloud Provider. As a result, the CA must have a dedicated
implementation built on top of the core CA implementation to support the specific
Cloud Provider that manages the underlying cluster in order to automatically upscale
(increase the number of nodes) and/or downscale the cluster (decrease the number
of nodes). \\ %
The distinction between the two boundaries has already been extensively discussed
using the word knowledge. The CA has a high-level knowledge of the cluster (Kubernetes)
and information about the active Nodes and Pods, so it can determine when to perform
an autoscaling operation, but it lacks a low-level understanding of how to do so.
As a result, it must rely on the Cloud Provider's Server implementation, which has
complete low-level knowledge of how to bootstrap or terminate the Nodes in the cluster
but lacks the high-level knowledge of when to do so. As a result, the CA and the
Server cooperate to accomplish the common autoscaling objective by combining their
knowledge. \\ %
The official CA implementation is only compatible with big private and enterprise-oriented
Cloud Providers like AWS, Azure, and GCP. As a result, a custom implementation of
a Cloud Provider, known as \texttt{recluster}\footnote{\url{https://github.com/carlocorradini/autoscaler/tree/master/cluster-autoscaler/cloudprovider/recluster}},
has been developed that is compatible with the architecture outlined in this
document. Furthermore, the \texttt{recluster} implementation is smaller in size than
the official CA distribution because the official one needs to be compatible with
every Cloud Provider (hence more code), whereas the custom implementation is
only compatible with the \texttt{recluster} architecture and excludes the other Cloud
Providers (hence reducing the code). \\ %
The Cloud Configuration (\texttt{cloud-config}) file is required by the CA to
configure the custom Cloud Provider. The recluster Cloud Configuration is a \texttt{JSON}
file that contains the two following attributes:
\begin{enumerate}
  \item \texttt{token}
    \newline
    \texttt{JWT} security token.
    \newline
    It is included in the Authorization header of every request to the Server's
    GraphQL API.

  \item \texttt{URL}
    \newline
    Server \texttt{URL}.
    \newline
    The complete Server \texttt{URL}, including potential routes such as \texttt{/graphql}.
\end{enumerate}

\subsubsection{Upscaling}
\label{subsubsec:implementation_autoscaling_cluster_autoscaler_upscaling}

\subsubsection{Downscaling}
\label{subsubsec:implementation_autoscaling_cluster_autoscaler_downscaling}

\subsubsection{Configuration}
\label{subsubsec:implementation_autoscaling_cluster_autoscaler_configuration}

\section{Installer}
\label{sec:implementation_installer}

\url{https://github.com/carlocorradini/reCluster/blob/main/install.sh}

\subsection{Configuration Parameters}
\label{subsec:implementation_installer_configuration_parameters}

\begin{xltabular}
  {\textwidth} { >{\ttfamily}l | X | >{\ttfamily}c }

  \multicolumn{1}{ c |}{\large{\textbf{Name}}} &
  \multicolumn{1}{ c |}{\large{\textbf{Description}}} &
  \multicolumn{1}{ c }{\large{\textbf{Default Value}}} \\ \hhline{===}

  --admin-username <USERNAME> & & admin \\ \hline

  --admin-password <PASSWORD> & & Password\$0 \\ \hline

  --airgap & & false \\ \hline

  --autoscaler-username <USERNAME> & & autoscaler \\ \hline

  --autoscaler-password <PASSWORD> & & Password\$0 \\ \hline

  --autoscaler-version <VERSION> & & latest \\ \hline

  --bench-time <TIME> & & 30 \\ \hline

  --certs-dir <DIR> & & configs/certs \\ \hline

  --config-file <FILE> & & configs/recluster/config.yaml \\ \hline

  --disable-color & & false \\ \hline

  --disable-spinner & & false \\ \hline

  --help & Display a help message and terminate (successfully). & \\ \hline

  --init-cluster & & false \\ \hline

  --k3s-config-file <FILE> & & configs/k3s/config.yaml \\ \hline

  --k3s-registry-config-file <FILE> & & configs/k3s/registries.yaml \\ \hline

  --k3s-version <VERSION> & & latest \\ \hline

  --log-level <LEVEL> & Logging level (\texttt{<LEVEL>}).
  \newline
  Attachment \ref{cha:logging} provides additional information regarding logging
  and logging levels.
  \newline
  The following logging levels are supported (listed in descending order of importance):
  \begin{itemize}[noitemsep]
    \item[\protect\icircled{\texttt{5}}] \texttt{fatal}

    \item[\protect\icircled{\texttt{4}}] \texttt{error}

    \item[\protect\icircled{\texttt{3}}] \texttt{warn}

    \item[\protect\icircled{\texttt{2}}] \texttt{info}

    \item[\protect\icircled{\texttt{1}}] \texttt{debug}
  \end{itemize}
  & info \\ \hline

  --node-exporter-config-file <FILE> & & configs/node\_exporter/config.yaml \\
  \hline

  --node-exporter-version <VERSION> & & latest \\ \hline

  --pc-device-api <URL> & & http://pc.recluster.local/cm?cmnd=status\%2010 \\
  \hline

  --pc-interval <TIME> & & 1 \\ \hline

  --pc-time <TIME> & & 30 \\ \hline

  --pc-warmup <TIME> & & 10 \\ \hline

  --server-env-file <FILE> & & configs/recluster/server.env \\ \hline

  --spinner &
  \begin{enumerate}
    \item \texttt{dots}
      \newline

    \item \texttt{grayscale}
      \newline

    \item \texttt{propeller}
      \newline
  \end{enumerate}
  & \\ \hline

  --ssh-authorized-keys-file <FILE> & & configs/ssh/authorized\_keys \\ \hline

  --ssh-config-file <FILE> & & configs/ssh/ssh\_config \\ \hline

  --sshd-config-file <FILE> & & configs/ssh/sshd\_config \\ \hline

  --user <USER> & & root \\ \hline

  \caption{Installer script parameters}
\end{xltabular}

\subsection{Configuration Files}
\label{subsec:implementation_installer_configuration_files}

% TODO # ... means skipped
% TODO config script
% TODO # replaced value from configs.sh

\subsubsection{Certificates}
\label{subsubsec:implementation_installer_configuration_files_certificates}

\subsubsection{K3s}
\label{subsubsec:implementation_installer_configuration_files_k3s}

\begin{lstlisting}[language=yaml, alsoletter={._}, escapeinside={(*@}{@*)}, morekeywords={[2]{mirrors, endpoint, configs, tls, ca_file, key_file}}, xleftmargin=\parindent, label={lst:k3s_registries_config}, caption=TODO]
  mirrors:
    ${{ k8s.registry.mirror.host }}(*@\ttfamily\textcolor{yamlBlue}{:}@*)${{ k8s.registry.mirror.port }}:
    # registry.recluster.local:5000:
      endpoint:
        - "https://${{ k8s.registry.endpoint.host }}:${{ k8s.registry.endpoint.port }}"
          # "https://10.0.0.200:5000"

  configs:
    ${{ k8s.registry.mirror.host }}:
    # registry.recluster.local:
      tls:
        ca_file: '/usr/local/share/ca-certificates/registry.crt'
        key_file: '/usr/local/share/ca-certificates/registry.key'
\end{lstlisting}

\begin{lstlisting}[language=yaml, alsoletter={.-}, morekeywords={[2]{token, cluster-init, cluster-domain, flannel-backend, disable, node-taint}}, xleftmargin=\parindent, label={lst:k3s_controller_config}, caption=TODO]
  token: "${{ k3s.token }}"
  # token: "4646f99bc4cbae3d5eceed856b337c9d3284be0d4056a3909f780c0c385fbf93"
  cluster-init: true
  # server: "https://${{ k3s.server.host }}:${{ k3s.server.port }}"
  # server: "https://10.0.0.10:6443"
  cluster-domain: 'recluster.local'
  flannel-backend: 'host-gw'
  disable:
    - 'servicelb'
  node-taint:
    # - 'CriticalAddonsOnly=true:NoExecute'
\end{lstlisting}

\begin{lstlisting}[language=yaml, alsoletter={.}, morekeywords={[2]{token, server}}, xleftmargin=\parindent, label={lst:k3s_worker_config}, caption=TODO]
  token: "${{ k3s.token }}"
  # token: "4646f99bc4cbae3d5eceed856b337c9d3284be0d4056a3909f780c0c385fbf93"
  server: "https://${{ k3s.server.host }}:${{ k3s.server.port }}"
  # server: "https://10.0.0.10:6443"
\end{lstlisting}

\subsubsection{K8s}
\label{subsubsec:implementation_installer_configuration_filesn_k8s}

\begin{lstlisting}[language=yaml, alsoletter={.-1}, morekeywords={1}, morekeywords={[2]{apiVersion, kind, metadata, name, namespace, stringData, cloud-config, labels, app, spec, replicas, selector, matchLabels, template, serviceAccountName, tolerations, containers, image, command, volumeMounts, mountPath, readOnly, volumes, secret, secretName}}, xleftmargin=\parindent, label={lst:k8s_autoscaler_deployment}, caption=TODO]
  # ...

  ---
  apiVersion: 'v1'
  kind: 'Secret'
  metadata:
    name: 'cluster-autoscaler-secret'
    # ...
  stringData:
    cloud-config: |-
      {
        "url": "http://${{ recluster.server.host }}:${{ recluster.server.port }}                                /${{ recluster.server.path }}",
        # "url": "http://10.0.0.10:8080/graphql",
        "token": "${{ __.token }}"
        # "token": "${{ __.token }}"
      }

  ---
  apiVersion: 'apps/v1'
  kind: 'Deployment'
  metadata:
    name: 'cluster-autoscaler'
    # ...
  spec:
    # ...
    replicas: 1
    template:
      # ...
      spec:
        # ...
        serviceAccountName: 'cluster-autoscaler'
        containers:
          - name: 'cluster-autoscaler'
            image: "${{ k8s.registry.mirror.host }}:${{ k8s.registry.mirror.port }}/recluster                       /cluster-autoscaler"
            # image: "registry.recluster.local:5000/recluster/cluster-autoscaler"
            command:
              - './cluster-autoscaler'
              - '--v=4'
              - '--stderrthreshold=info'
              - '--cloud-provider=recluster'
              - '--cloud-config=/config/cloud-config'
              - '--cluster-name=recluster'
              - '--scale-down-unneeded-time=5m'
              - '--skip-nodes-with-local-storage=false'
              - '--skip-nodes-with-system-pods=false'
            # ...
        volumes:
          - name: 'cloud-config'
            secret:
              secretName: 'cluster-autoscaler-secret'
\end{lstlisting}

\begin{lstlisting}[language=yaml, alsoletter={.}, morekeywords={[2]{apiVersion, kind, metadata, name, namespace, spec, addresses, ipAddressPools}}, xleftmargin=\parindent, label={lst:k8s_metallb_config}, caption=TODO]
  ---
  apiVersion: 'metallb.io/v1beta1'
  kind: 'IPAddressPool'
  metadata:
    name: 'default'
    namespace: 'metallb-system'
  spec:
    addresses:
      - "${{ k8s.loadbalancer.ip.from }}-${{ k8s.loadbalancer.ip.to }}"
        # "10.0.0.200-10.0.0.250"

  ---
  apiVersion: 'metallb.io/v1beta1'
  kind: 'L2Advertisement'
  metadata:
    name: 'default'
    namespace: 'metallb-system'
  spec:
    ipAddressPools:
      - 'default'
\end{lstlisting}

\begin{lstlisting}[language=yaml, alsoletter={.015}, morekeywords={1, 5000}, morekeywords={[2]{apiVersion, kind, type, metadata, name, namespace, data, tls.crt, tls.key, accessModes, storageClassName, resources, requests, storage, spec, selector, app, ports, protocol, port, targetPort, loadBalancerIP, replicas, template, containers, image, containerPort, volumes, secret, secretName, persistentVolumeClaim, claimName}}, xleftmargin=\parindent, label={lst:k8s_registry_config}, caption=TODO]
  # ...

  ---
  apiVersion: 'v1'
  kind: 'Secret'
  type: 'kubernetes.io/tls'
  metadata:
    name: 'registry-secret'
    # ...
  data:
    tls.crt: "${{ k8s.registry.tls.crt }}"
    # tls.crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZaekNDQTArZ0F3SUJBZ0lVVWg5T01vV0k1SEFG..."
    tls.key: "${{ k8s.registry.tls.key }}"
    # tls.key: "LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUpRd0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVND..."

  ---
  apiVersion: 'v1'
  kind: 'PersistentVolumeClaim'
  metadata:
    name: 'registry-data-pvc'
    # ...
  spec:
    accessModes:
      - 'ReadWriteOnce'
    storageClassName: 'local-path'
    # storageClassName: 'longhorn'
    resources:
      requests:
        storage: '32Gi'

  ---
  apiVersion: 'apps/v1'
  kind: 'Deployment'
  metadata:
    name: 'registry'
    # ...
  spec:
    # ...
    replicas: 1
    template:
      # ...
      spec:
        # ...
        containers:
          - name: 'registry'
            image: 'registry:2'
            ports:
              - containerPort: 5000
            # ...
        volumes:
          - name: 'registry-certs'
            secret:
              secretName: 'registry-secret'
          - name: 'registry-data'
            persistentVolumeClaim:
              claimName:' registry-data-pvc'

  ---
  apiVersion: 'v1'
  kind: 'Service'
  metadata:
    name: 'registry-service'
    # ...
  spec:
    type: 'LoadBalancer'
    selector:
      app: 'registry'
    ports:
      - name: 'registry-port'
        protocol: 'TCP'
        port: ${{ k8s.registry.endpoint.port }}
        # port: 5000
        targetPort: 5000
    loadBalancerIP: "${{ k8s.registry.endpoint.host }}"
    # loadBalancerIP: "10.0.0.200"
\end{lstlisting}

\subsubsection{Node exporter}
\label{subsubsec:implementation_installer_configuration_files_node_exporter}

% TODO Write node exporter no config file but install.sh transform

\begin{lstlisting}[language=yaml, alsoletter={.-}, morekeywords={[2]{collector, disable-defaults, cpu, meminfo}}, xleftmargin=\parindent, label={lst:node_exporter_config}, caption=TODO]
collector:
  disable-defaults: true
  cpu: true
  meminfo: true
\end{lstlisting}

\subsubsection{reCluster}
\label{subsubsec:implementation_installer_configuration_files_recluster}

\begin{lstlisting}[language=yaml, alsoletter={.}, morekeywords={[2]{kind, recluster, server}}, xleftmargin=\parindent, label={lst:recluster_controller_config}, caption=TODO]
  kind: 'controller'

  recluster:
    server: "http://${{ recluster.server.host }}:${{ recluster.server.port }}"
    # server: "10.0.0.10:8080"
\end{lstlisting}

\begin{lstlisting}[language=yaml, alsoletter={.}, morekeywords={[2]{kind, recluster, server}}, xleftmargin=\parindent, label={lst:recluster_worker_config}, caption=TODO]
  kind: 'worker'

  recluster:
    server: "http://${{ recluster.server.host }}:${{ recluster.server.port }}"
    # server: "10.0.0.10:8080"
\end{lstlisting}

\begin{lstlisting}[language=shell, alsoletter={_}, morestring={*[b]"}, morekeywords={[4]{NODE_ENV, HOST, PORT, LOGGER_LEVEL, DATABASE_URL, SSH_USERNAME, SSH_PRIVATE_KEY, SSH_PASSPHRASE, TOKEN_PRIVATE_KEY, TOKEN_PUBLIC_KEY, TOKEN_PASSPHRASE}}, xleftmargin=\parindent, label={lst:recluster_server_config}, caption=TODO]
  NODE_ENV='production'
  HOST='0.0.0.0'
  PORT=${{ recluster.server.port }}
  # PORT=8080
  LOGGER_LEVEL='info'
  DATABASE_URL="postgresql://${{ recluster.database.user }}:${{ recluster.database.password }}                  @${{ recluster.database.host }}:${{ recluster.database.port }}                                /${{ recluster.database.db }}"
  # DATABASE_URL="postgresql://recluster:password@10.0.0.10:5432/recluster"
  SSH_USERNAME='root'
  SSH_PRIVATE_KEY='/etc/recluster/certs/ssh.key'
  SSH_PASSPHRASE=
  TOKEN_PRIVATE_KEY='/etc/recluster/certs/token.key'
  TOKEN_PUBLIC_KEY='/etc/recluster/certs/token.crt'
  TOKEN_PASSPHRASE=
\end{lstlisting}

\subsubsection{SSH}
\label{subsubsec:implementation_installer_configuration_files_ssh}

\begin{lstlisting}[language=shell, xleftmargin=\parindent, label={lst:ssh_authorized_keys}, caption=TODO]
  ${{ ssh.authorized_key }}
  # ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK4mrabO3qgmkmWKSP+WcBDEWGGqJUBoiQdK3yAbKry9
\end{lstlisting}

\begin{lstlisting}[language=shell, alsoletter={*.-@}, morekeywords={[4]{Host, AddKeysToAgent, IdentitiesOnly, PasswordAuthentication, ChallengeResponseAuthentication, PubkeyAuthentication, HostKeyAlgorithms, Ciphers, MACs, UseRoaming}}, keywordstyle={[5]\color{shellGreen}}, morekeywords={[5]{*, yes, no, ssh-ed25519-cert-v01@openssh.com, ssh-ed25519, chacha20-poly1305@openssh.com, aes256-gcm@openssh.com, aes128-gcm@openssh.com, aes256-ctr, aes192-ctr, aes128-ctr, hmac-sha2-512-etm@openssh.com, hmac-sha2-256-etm@openssh.com, umac-128-etm@openssh.com, hmac-sha2-512, hmac-sha2-256, umac-128@openssh.com}}, xleftmargin=\parindent, label={lst:ssh_config}, caption=TODO]
  Host *
    AddKeysToAgent yes
    IdentitiesOnly yes
    PasswordAuthentication no
    ChallengeResponseAuthentication no
    PubkeyAuthentication yes
    HostKeyAlgorithms ssh-ed25519-cert-v01@openssh.com,ssh-ed25519
    Ciphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,                                           aes128-gcm@openssh.com,aes256-ctr,aes192-ctr,aes128-ctr
    MACs hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com,                                    umac-128-etm@openssh.com,hmac-sha2-512,hmac-sha2-256,umac-128@openssh.com
    UseRoaming no
    # ...
\end{lstlisting}

\begin{lstlisting}[language=shell, xleftmargin=\parindent, alsoletter={*.-@/}, morekeywords={[4]{Ciphers, MACs, PubkeyAuthentication, PasswordAuthentication, ChallengeResponseAuthentication, AllowTcpForwarding, GatewayPorts, X11Forwarding, Subsystem}}, keywordstyle={[5]\color{shellGreen}}, morekeywords={[5]{no, yes, sftp, /usr/lib/ssh/sftp-server, chacha20-poly1305@openssh.com, aes256-gcm@openssh.com, aes128-gcm@openssh.com, aes256-ctr, aes192-ctr, aes128-ctr, hmac-sha2-512-etm@openssh.com, hmac-sha2-256-etm@openssh.com, umac-128-etm@openssh.com, hmac-sha2-512, hmac-sha2-256, umac-128@openssh.com}}, label={lst:sshd_config}, caption=TODO]
  Ciphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,                                           aes128-gcm@openssh.com,aes256-ctr,aes192-ctr,aes128-ctr
  MACs hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com,umac-128-etm@openssh.com,           hmac-sha2-512,hmac-sha2-256,umac-128@openssh.com
  PubkeyAuthentication yes
  PasswordAuthentication
  ChallengeResponseAuthentication no
  AllowTcpForwarding no
  GatewayPorts no
  X11Forwarding no
  Subsystem sftp /usr/lib/ssh/sftp-server
  # ...
\end{lstlisting}

\subsection{Benchmarks}
\label{subsec:implementation_installer_node_benchmarks}

% TODO sysbench

\subsection{Power Consumption}
\label{subsec:implementation_installer_node_power_consumption}

% CloudFree Smart Plug

\subsection{Cluster Initialization}
\label{subsec:implementation_installer_cluster_initialization}

\subsubsection{Kubeconfig}
\label{subsubsec:implementation_installer_cluster_initialization_kubeconfig}

% https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig

\subsubsection{Database}
\label{subsubsec:implementation_installer_cluster_initialization_database}

\subsubsection{Server}
\label{subsubsec:implementation_installer_cluster_initialization_server}

\subsubsection{Admin And Autoscaler Users}
\label{subsubsec:implementation_installer_cluster_initialization_admin_and_autoscaler_users}

\subsubsection{K8s}
\label{subsubsec:implementation_installer_cluster_initialization_k8s}